[{"content":"Introduction Few months back, my wife has started blogging on worpress and I setup a Wordpress instances with docker compose on Oracle Cloud alwayfree tier VM. Every thing went smooth, and then ocurred my mind after a month, \u0026ldquo;What about backup?\u0026rdquo;, maybe in time in future, Oracle may remove the alway-free tire, the I have to migrate to new cloud provider and it is also general recomendation to setup a backup.\nGoals I have specific goals in my mind like mentioned above, like it has to be\nFree Automated Preferably offsite backups (More prefered way is 3-2-1 Backup Strategy) And ofcouse the backups should be encrypted Then I found Cloudflare R2 which I get free 10 GB monthly object storage (Read more on R2 Pricing) which is enought for my worpress backup.\nNOTE: This post only covers deployment of Kopia with UI for Wordpress and its database via docker compose\nKopia Deployment Kopia is a fast and secure open-source backup/restore tool that allows you to create encrypted snapshots of your data and save the snapshots to remote or cloud storage of your choice, to network-attached storage or server, or locally on your machine. Kopia does not ‘image’ your whole machine. Rather, Kopia allows you to backup/restore any and all files/directories that you deem are important or critical.\nSource: https://kopia.io/docs/\nI use Traefik for reverse proxy which gives flexibility to add routes in docker compose labels itself\nHere is my Wordpress website with traefik docker-compose.yml Github Gist for sake of completeness\nLet\u0026rsquo;s look at my Kopia docker compose\n--- networks: traefik_public: external: true volumes: cache: config: logs: services: kopia: image: kopia/kopia:latest container_name: kopia hostname: kopia restart: unless-stopped command: - server - start - --insecure - --no-persistent-logs - --log-level=info - --address=0.0.0.0:51515 labels: - com.centurylinklabs.watchtower.enable=true - traefik.enable=true - traefik.docker.network=traefik_public - traefik.http.routers.kopia.tls=true - traefik.http.routers.kopia.entrypoints=websecure - traefik.http.routers.kopia.tls.certresolver=letsencrypt - traefik.http.routers.kopia.rule=Host(`kopia.${MY_DOMAIN}`) - traefik.http.services.kopia.loadbalancer.server.port=51515 privileged: true cap_add: - SYS_ADMIN security_opt: - apparmor:unconfined devices: - /dev/fuse:/dev/fuse:rwm dns: - 8.8.8.8 environment: TZ: Europe/Berlin KOPIA_SERVER_USERNAME: ${KOPIA_SERVER_USERNAME} KOPIA_SERVER_PASSWORD: ${KOPIA_SERVER_PASSWORD} networks: - traefik_public volumes: - config:/app/config - cache:/app/cache - logs:/app/logs - /opt/docker/volumes:/data:ro kopia.${MY_DOMAIN} - Sub-domain for the kopia UI to view and configure backups. For example; if your domain is example.com, then it would be kopia.example.com /opt/docker/volumes:/data:ro - Mount /opt/docker/volumes into /data so that Kopia can access source location to backup. If you look at my wordpress docker-compose.yaml at L95 and L119, the data is stored in /opt/docker/volumes. Kopia can access both mariadb and wordpress volume data KOPIA_SERVER_USERNAME and KOPIA_SERVER_PASSWORD - This is the kopia server login username and password to view Kopia UI and configure backups Deploy Kopia\ndocker-compose up -d Backup Configuration Create Cloudflare R2 Bucket ","permalink":"https://veerendra2.github.io/worpress-backup-kopia/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eFew months back, my wife has started blogging on worpress and I setup a Wordpress instances with docker compose on Oracle Cloud alwayfree tier VM. Every thing went smooth, and then ocurred my mind after a month, \u0026ldquo;What about backup?\u0026rdquo;, maybe in time in future, Oracle may remove the alway-free tire, the I have to migrate to new cloud provider and it is also general recomendation to setup a backup.\u003c/p\u003e","title":"How I setup my self-host Wordpress and automated backup with Kopia and Cloudflare R2 for FREE"},{"content":"This is a how-to guide inspired by \u0026ldquo;Quick and Easy SSL Certificates for Your Homelab!\u0026rdquo;\u0026quot; focus on Traefik reverse proxy server. Although this guide focuses on DuckDNS, a similar configuration can apply to any DNS provider.\nThe trick is by setting private IP for the domain name you own, in my case DuckDNS.\nGet DuckDNS Sub-Domain Sign-up an account in duckdns.org, choose a sub-domain(As I choose a dummy sub-domain lser.duckdns.org) and add your home server IP address(In my case 192.168.0.120)\nCopy token which will be used later for DNS challenge\nConfigure Traefik Reference docs https://doc.traefik.io/traefik/https/acme/\nDefine certificatesResolvers Below is an example toml config snippet. You can find the complete configuration in my Github repo veerendra2/raspberrypi-homeserver/services/traefik/config/traefik.toml\n... [certificatesResolvers.whatever-resolver-name-here.acme] email = \u0026#34;veerendra2@github.com\u0026#34; storage = \u0026#34;/etc/traefik/acme.json\u0026#34; [certificatesResolvers.whatever-resolver-name-here.acme.dnsChallenge] provider = \u0026#34;duckdns\u0026#34; resolvers = [\u0026#34;1.1.1.1:53\u0026#34;, \u0026#34;8.8.8.8:53\u0026#34;] ... I defined certificatesResolvers with name whatever-resolver-name-here with dnsChallenge(DNS challenge)\nemail - A manditory option to identify user storage - Storage file location to store private key and other certificate details provider - duckdns (List of other dns provider supported by Traefik here) SSL Certificate for the Domain As Traefik docs says\nDefining a certificate resolver does not result in all routers automatically using it. Each router that is supposed to use the resolver must reference it.\nUse defined certificatesResolvers(In my case, it is whatever-resolver-name-here) by adding it in labels like below docker-stack.yml\n\u0026#x1f449; You can also find below docker swarm docker-stack.yml in my Github repo. https://github.com/veerendra2/raspberrypi-homeserver/tree/main/services/traefik\n... labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=network_public\u0026#34; - \u0026#34;traefik.http.routers.api.rule=Host(`lser.duckdns.org`) \u0026amp;\u0026amp; PathPrefix(`/dashboard`) || Host(`lser.duckdns.org`) \u0026amp;\u0026amp; PathPrefix(`/api`)\u0026#34; - \u0026#34;traefik.http.routers.api.tls.certresolver=whatever-resolver-name-here\u0026#34; - \u0026#34;traefik.http.routers.api.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.api.tls=true\u0026#34; - \u0026#34;traefik.http.routers.api.service=api@internal\u0026#34; - \u0026#34;traefik.http.services.dummy.loadbalancer.server.port=9999\u0026#34; env_file: - .env_traefik secrets: - duckdns ... As you see in the above docker-stack.yml, I mentioned duckdns secret. To complete the DNS challenge, you must pass the DuckDNS token you copied earlier as environmental variables to Traefik\n\u0026#x1f449; Reference docs https://go-acme.github.io/lego/dns/duckdns/\nHere is a directory tree for a better understanding\nveerendra at atom in ~/raspberrypi-homeserver/services/traefik on main $ tree . . ├── config │ ├── acme.json │ └── traefik.toml ├── docker-stack.yml ├── network.yml └── secrets └── duckdns.txt 3 directories, 7 files $ cat .env_traefik DUCKDNS_TOKEN_FILE=/run/secrets/duckdns $ cat secrets/duckdns.txt [REDACTED] Once everything is placed, deploy the stack by running below command\n$ docker stack deploy -c docker-stack.yml traefik ... That\u0026rsquo;s it!, Traefik completes the DNS challenge and gets a SSL certificate for your DuckDNS domain. If you want to create a sub-domain for any other service, you can simply add a domain name and resolver in docker labels like below\n... - \u0026#34;traefik.http.routers.api.rule=Host(`my-service.lser.duckdns.org`)\u0026#34; - \u0026#34;traefik.http.routers.api.tls.certresolver=whatever-resolver-name-here\u0026#34; - \u0026#34;traefik.http.routers.api.entrypoints=https\u0026#34; ... ","permalink":"https://veerendra2.github.io/traefik-https/","summary":"\u003cp\u003eThis is a how-to guide inspired by \u003ca href=\"https://www.youtube.com/watch?v=qlcVx-k-02E\"\u003e\u0026ldquo;Quick and Easy SSL Certificates for Your Homelab!\u0026rdquo;\u003c/a\u003e\u0026quot; focus on Traefik reverse proxy server. Although this guide focuses on DuckDNS, a similar configuration can apply to any DNS provider.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static_content/images/blog-18-1.png\" alt=\"blog-14-1\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eThe trick is by setting private IP for the domain name you own, in my case DuckDNS.\u003c/p\u003e\n\u003ch2 id=\"get-duckdns-sub-domain\"\u003eGet DuckDNS Sub-Domain\u003c/h2\u003e\n\u003cp\u003eSign-up an account in \u003ca href=\"https://duckdns.org/\"\u003educkdns.org\u003c/a\u003e, choose a sub-domain(As I choose a dummy sub-domain \u003ccode\u003elser.duckdns.org\u003c/code\u003e) and add your home server IP address(In my case \u003ccode\u003e192.168.0.120\u003c/code\u003e)\u003c/p\u003e","title":"Traefik HTTPS Config with DuckDNS for Local Homeserver"},{"content":"Introduction One of the services I always wanted to have on my raspberry pi home server is VPN and BitTorrent, which basically, route all BitTorrent traffic through a VPN container. Then I saw Wolfgang\u0026rsquo;s \u0026ldquo;Set Up Your Own Wireguard VPN Server with 2FA in 5 Minutes!\u0026rdquo; on Youtube, which inspired me to set up my own VPN server. I quickly created an instance on Oracle Cloud and ran the ansible playbook to deploy the wireguard VPN. After I tested a few months on Mobile and PC, I\u0026rsquo;m comfortable setting up server and client. So, I decided to use the Wireguard in my raspberrypi-homeserver project.\nBut why BitTorrent with VPN? BitTorrent does not, on its own, offer its users anonymity. One can usually see the IP addresses of all peers in a swarm in one\u0026rsquo;s own client or firewall program. This may expose users with insecure systems to attacks.\nhttps://en.wikipedia.org/wiki/BitTorrent#Anonymity\nFor better anonymity, I decided to use VPN for BitTorrent and to download my favourite linux distros\nGoal The goal of this blog is a how-to guide on routing BitTorrent client traffic through Wireguard VPN on the Docker swarm cluster.\nNOTE \u0026#x270b;: The scope of the post is to set up qBittorent and Wireguard VPN clients only, DOES NOT include wireguard server setup. I would recommend checking Set Up Your Own Wireguard VPN Server with 2FA in 5 Minutes! to know how to set up Wireguard VPN server.\nThis blog post is divided into 2 parts\n\u0026#x2712;\u0026#xfe0f; Part 1 covers Intro Challenges on docker swarm Architecture Deployment \u0026#x2712;\u0026#xfe0f; Part 2 covers Testing the stack qBittorrent proxy configuration and testing Kill switch configuration Conclusion By the way, recently I migrated my raspberry pi home server docker-compose setup to docker swarm services, maybe that is for another blog on what were the challenges. But for now, let\u0026rsquo;s see how to route BitTorrent traffic through the Wireguard VPN docker container.\nSource code available in my GitHub repo: https://github.com/veerendra2/raspberrypi-homeserver/tree/main/services/torrent\nChallenges on docker swarm There are some challenges I faced while deploying the BitTorrent client using wireguard.\nContainer linking \u0026#x1f449; https://docs.docker.com/network/links/\nUnlike docker-compose container linking, docker swarm doesn\u0026rsquo;t allow container linking.\nIt would have been a very easy and secure link between qBittorent and wireguard client\n\u0026#x1f7e2; Github open issue: https://github.com/moby/moby/issues/33055\nAlternate options 1. ip routes and iptable rules Create some iptable rules to make Wireguard container a NAT router and set default route to Wireguard in qBittorrent container. In docker-compose, assign ip address to the container like below\n... networks: dhcp-relay-net: ipv4_address: \u0026#39;172.31.0.10\u0026#39; front-tier: {} ... then I can create a shell script to create iptables rules and routes, but in docker swarm, it is not possible to assign ip to a container in the stack\n\u0026#x1f7e2; Github open issue: https://github.com/moby/moby/issues/31860\n2. Go back to docker-compose Since docker-compose has container linking, one option is to go back to the docker-compose setup for this stack. I use traefik for reverse proxy with sub path. After I googled, traefik can\u0026rsquo;t support both docker standalone(docker-composer) and docker swarm at the same time.\n\u0026#x1f7e2; Github open issue: https://github.com/traefik/traefik/issues/6063\n3. Monolithic Docker image Use all-in-one docker image which has qBittorrent and Wireguard. This options looks good, but the down side of the options is typical monolithic approach issues\nIf I want to add or remove services like radarr, jackett, etc I have re-build and have no flexibility Have to rebuild the image if there is a version change 4. SOCKS5 proxy with internal network The final and decent approach is to make sure qBittorrent container does NOT directly connect to the Internet and connect to Wireguard container via the internal network i.e. internal: true to access the Internet with SOCKS5 proxy protocol (As you can see in below diagram)\n┌ │ └ ─ q ─ ─ B ─ ─ i ─ ─ t ─ ─ t ─ ─ o ─ ─ r ─ ─ r ─ ─ e ─ ─ n ─ ─ t ─ ┐ ├ ┘ S ─ O ─ C ─ K ─ S ─ 5 ─ ─ P ─ r ─ o ─ x ─ y ► ┌ │ │ └ ─ W ─ ─ i C ─ ─ r l ─ ─ e i ─ ─ g e ─ ─ u n ─ ─ a t ─ ─ r ─ ─ d ─ ┐ │ ├ ┘ ─ ─ ─ ─ ─ ► ┌ │ └ ─ I ─ ─ N ─ ─ T ─ ─ E ─ ─ R ─ ─ N ─ ─ E ─ ─ T ─ ┐ │ ┘ After googling, I found dante-server \u0026ldquo;A free SOCKS server\u0026rdquo;. I opened github issue, feature request #250 in linuxserver/docker-wireguard to add dante-server in Wireguard image. But, it was rejected, because, it can be done in either customizing containers or as a docker mods.\nSo, I customized the linuxserver\u0026rsquo;s Wireguard container by installing and starting the dante-server while bootstrapping the Wireguard container, as you can next sections\nArchitecture Finally, here is the architecture with traefik reverse proxy\nFrom the above diagram, the traefik_private overlay network is configured with internal: true, which mean there is no external connectivity, but traefik can set up the reverse proxy to qBittorrent, so that I can access. Wireguard is connected to traefik_private and internal overlay network with internal: false i.e. there is external connectivity. So, the idea here is run a SOCKS5 server on Wireguard and configure qBittorrent proxy.\nqBittorent can connect Wireguard, but not directly to the Internet Wireguard is accessible to qBittorent and connected to the Internet too In a nutshell, qBittorent can reach the Internet via Wiregaurd SOCKS5 proxy\u0026hellip; ta-da!\u0026#x1f389;\nDeploy \u0026#x1f4c2; You can browse source code in my GitHub repo: https://github.com/veerendra2/raspberrypi-homeserver\nI created a simple ansible playbook to deploy all services on my raspberry pi. But for the sake of the blog, I will show you how to deploy manually with traefik reverse proxy\nTraefik reverse proxy Since I\u0026rsquo;m using traefik reverse proxy in my raspberry pi. I\u0026rsquo;m showing how to deploy it, if you have a different reverse proxy(like Nginx), skip this step. But, make sure you make it compatible with the above architecture\n$ git clone https://github.com/veerendra2/raspberrypi-homeserver.git $ cd services/traefikv2 # Create traefik_private and traefik_public network $ docker stack deploy -c traefik.yml traefik $ docker network ls NETWORK ID NAME DRIVER SCOPE 9ov3x46i6ci0 traefik_private overlay swarm ydbzvtoc1xvg traefik_public overlay swarm # Deploy traefik reverse proxy $ docker stack deploy -c docker-stack.yml traefikv2 $ $ docker stack ls NAME SERVICES traefik 1 traefikv2 1 Wireguard VPN and qBittorrent \u0026#x1f4c2; https://github.com/veerendra2/raspberrypi-homeserver/tree/main/services/torrent\nMake sure you have Wireguard VPN server up and running and get wg0.conf\nHere is my directory structure(Ignore jackett and radarr for now)\n$ cd services/torrent $ tree . . ├── README.md ├── docker-stack.yml ├── ip-test.sh ├── jackett │ └── ServerConfig.json ├── qbittorrent │ └── qBittorrent.conf ├── radarr │ └── config.xml └── wireguard ├── dante-server │ ├── danted.conf │ ├── install.sh │ └── run.sh └── wg0.conf 5 directories, 10 files The dante-server installation and bootstrapping is straight forward, you just create a scripts like below\n$ cd services/torrent $ tree wireguard/dante-server/ wireguard/dante-server/ ├── danted.conf \u0026lt;---- danted configuration ├── install.sh \u0026lt;----- Install dante-server └── run.sh \u0026lt;--------- Start the danted daemon 0 directories, 3 files And mount these files in wireguard container like below(docker-stack.yml#L38-#L43)\n... volumes: - ./wireguard/dante-server/danted.conf:/etc/danted.conf - ./wireguard/dante-server/install.sh:/custom-cont-init.d/install.sh:ro - ./wireguard/dante-server/run.sh:/custom-services.d/run.sh:ro ... That\u0026rsquo;s it! the scripts will install and bootstrap dante-server in Wireguard container\n$ cd services/torrent # Deploy stack $ docker stack deploy -c docker-stack.yml torrent $ docker stack ls NAME SERVICES torrent 4 traefik 1 traefikv2 1 That\u0026rsquo;s it for now! Next, in part 2, we will see how to test connections and configure clients\n","permalink":"https://veerendra2.github.io/wireguard-qbittorrent-docker-swarm-1/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eOne of the services I always wanted to have on my \u003ca href=\"https://github.com/veerendra2/raspberrypi-homeserver\"\u003eraspberry pi home server\u003c/a\u003e is VPN and BitTorrent, which basically, route all BitTorrent traffic through a VPN container. Then I saw Wolfgang\u0026rsquo;s \u003ca href=\"https://www.youtube.com/watch?v=SMF301vQqJo\"\u003e\u0026ldquo;Set Up Your Own Wireguard VPN Server with 2FA in 5 Minutes!\u0026rdquo;\u003c/a\u003e on Youtube, which inspired me to set up my own VPN server. I quickly created an instance on Oracle Cloud and ran the \u003ca href=\"https://github.com/notthebee/ansible-easy-vpn\"\u003eansible playbook\u003c/a\u003e to deploy the wireguard VPN. After I tested a few months on Mobile and PC, I\u0026rsquo;m comfortable setting up server and client. So, I decided to use the Wireguard in my \u003ca href=\"https://github.com/veerendra2/raspberrypi-homeserver\"\u003eraspberrypi-homeserver\u003c/a\u003e project.\u003c/p\u003e","title":"Wireguard VPN and BitTorrent on Docker Swarm (Part 1)"},{"content":"Previously in part 1, we have deployed the qBittorrent with Wireguard VPN on docker swarm. In this part, we will test the deployment, configure qBittorrent and finish the blog with the kill switch configuration\nTesting Below are the tests to make sure qBittorrent has access to the Internet through Wireguard VPN\n\u0026#x27a1;\u0026#xfe0f;Make sure Wireguard and qBittorrent containers are up and running.\n\u0026#x27a1;\u0026#xfe0f;Check you are able to ping Wireguard container from qBittorrent\n$ docker ps | grep qbittorrent | awk \u0026#39;{print $1, $2}\u0026#39; cd33f2b203c2 linuxserver/qbittorrent:arm64v8-4.5.2 $ docker exec -it cd33f2b203c2 /bin/bash root@qbittorrent:/# ping wireguard PING wireguard (172.16.204.53): 56 data bytes 64 bytes from 172.16.204.53: seq=0 ttl=64 time=0.312 ms 64 bytes from 172.16.204.53: seq=1 ttl=64 time=0.375 ms ^C \u0026#x27a1;\u0026#xfe0f;As mentioned before, qBittorrent should not able to reach the Internet\nroot@qbittorrent:/# ping google.com ping: bad address \u0026#39;google.com\u0026#39; root@qbittorrent:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1): 56 data bytes ping: sendto: Network unreachable \u0026#x27a1;\u0026#xfe0f;But should be able to access the Internet via SOCKS5 proxy\nroot@qbittorrent:/# curl -v -s -x socks5://wireguard:1080 google.com * Trying 172.16.204.54:1080... * Connected to wireguard (172.16.204.54) port 1080 (#0) * Could not resolve host: google.com * Closing connection 0 Hmm, what just happened?! it has connected to the proxy(172.16.204.54:1080), but can\u0026rsquo;t resolve google.com. That\u0026rsquo;s because the Bittorrent container doesn\u0026rsquo;t have DNS config and it can\u0026rsquo;t resolve via curl which is a good thing and making sure the container doesn\u0026rsquo;t have direct access to the Internet. Then, how do we test?\nroot@qbittorrent:/# curl -v -s -x socks5://wireguard:1080 1.1.1.1 * Trying 172.16.204.54:1080... * Connected to wireguard (172.16.204.54) port 1080 (#0) * SOCKS5 connect to IPv4 1.1.1.1:80 (locally resolved) * SOCKS5 request granted. * Connected to wireguard (172.16.204.54) port 1080 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 1.1.1.1 \u0026gt; User-Agent: curl/8.0.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 301 Moved Permanently \u0026lt; Server: cloudflare \u0026lt; Date: Sun, 02 Apr 2023 20:03:31 GMT \u0026lt; Content-Type: text/html \u0026lt; Content-Length: 167 \u0026lt; Connection: keep-alive \u0026lt; Location: https://1.1.1.1/ \u0026lt; CF-RAY: 7b1bacfxxxxx4ae6-xx \u0026lt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;301 Moved Permanently\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;301 Moved Permanently\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;cloudflare\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; * Connection #0 to host wireguard left intact Simply use IP address instead of name. Voila, it worked!!\n\u0026#x27a1;\u0026#xfe0f;Check your public. You should see, the IP address of Wireguard VPN server\nroot@qbittorrent:/# HTTPBIN_ORG_IP=107.22.139.22 root@qbittorrent:/# curl -s -x socks5://wireguard:1080 http://$HTTPBIN_ORG_IP/ip { \u0026#34;origin\u0026#34;: \u0026#34;XX.XX.XX.XX\u0026#34; } Since we don’t have DNS config, can only use IP address. we can use httpbin.org’s IP address which is 107.22.139.22 and run curl like above and my our public IP.\nConfiguration We are not done yet!, we need to configure the qBittorent client to use SOCKS5 proxy.\nGo to qBittorrent UI, in my case https://192.168.0.120/qbittorent as traefik setup the reverse proxy route.\nTools -\u0026gt; Options\nThen click Connection tab and add below\nProxy server type SOCKS5 Host: wireguard (As we tested earlier, we are able to resolve and ping Wiregaurd container. So, we can use the host name here) Port: 1080 (danted daemon listening port) And select rest options you can see below\nTesting We already checked our public IP address using a simple curl request to httpbin.org. Now, this test means to test the visibility of your public IP address in a torrent swarm. For this, we need to find an online service that displays our torrent public IP address just like the httpbin.org\nGoogle \u0026ldquo;torrent ip checker\u0026rdquo;, in my case, I found https://torguard.net/checkmytorrentipaddress.php\nOpen the link, copy the magnet link like below and keep the tab open, we need to come back and check the out public IP address And paste in qBittorrent and click download\nNow go back to torguard.net tab, check your public IP address\nIf you are able to see your Wireguard VPN server\u0026rsquo;s IP address, you are good to go!\nKill switch A kill switch, also known as an emergency brake, emergency stop (E-stop), emergency off (EMO) and as an emergency power off (EPO), is a safety mechanism used to shut off machinery in an emergency, when it cannot be shut down in the usual manner.\nhttps://en.wikipedia.org/wiki/Kill_switch\nIn our case, qBittorent should not be able to access the Internet if the public IP address is not the Wireguard VPN server\u0026rsquo;s IP address. For this I have created to simple shell script and run as docker healthcheck\nHTTPBIN_ORG_IP=107.22.139.22 MY_IP=`curl -s -x socks5://wireguard:1080 http://$HTTPBIN_ORG_IP/ip | jq --raw-output .origin` if [ \u0026#34;$MY_IP\u0026#34; == \u0026#34;$VPN_IP\u0026#34; ]; then echo \u0026#34;$MY_IP and $VPN_IP are matched\u0026#34; exit 0 else echo \u0026#34;$MY_IP and $VPN_IP are not matched\u0026#34; exit 1 fi To run this script, you need to pass the VPN_IP environmental variable to qBittorrent container. You can create a file .vpn_ip and add as env_file in services/torrent/docker-stack.yml#L69\ncat .vpn_ip VPN_IP=xx.xx.xx.xx Mount the script in qBittorrent container like services/torrent/docker-stack.yml#L75\n... volumes: - ./ip-test.sh:/opt/ip-test.sh ... Add healthcheck like services/torrent/docker-stack.yml#L77-L81\n... healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;/opt/ip-test.sh\u0026#34;] interval: 2m timeout: 10s retries: 3 ... Ta-da!!\u0026#x1f389; The script will run every 2 minutes with time out of 10 seconds and 3 retries on failure. If the script exits with non-zero, the qBittorrent container will be stopped.\nThe issue I see with docker healthcheck is that it will only start the first healthcheck after the interval period passes. There is an open GitHub issue #33410, that\u0026rsquo;s I had to keep the interval very short i.e. 2m\nConclusion In this post, I have shown how to route BitTorrent traffic via Wireguard VPN on the docker swarm cluster. Let\u0026rsquo;s summarize what have seen so far\nIn Part 1\nMotivations to use VPN for BitTorent Challenges on docker swarm with alternate options Architecture and deployment of Wireguard VPN client and qBittorrent on Docker swarm In Part 2\nTest connection is secure Configuration of qBittorrent proxy configuration Configuration of kill switch ","permalink":"https://veerendra2.github.io/wireguard-qbittorrent-docker-swarm-2/","summary":"\u003cp\u003ePreviously in \u003ca href=\"https://veerendra2.github.io/wireguard-qbittorrent-docker-swarm-1/\" title=\"part 1\"\u003epart 1\u003c/a\u003e, we have deployed the qBittorrent with Wireguard VPN on docker swarm. In this part, we will test the deployment, configure qBittorrent and finish the blog with the kill switch configuration\u003c/p\u003e\n\u003ch1 id=\"testing\"\u003eTesting\u003c/h1\u003e\n\u003cp\u003eBelow are the tests to make sure qBittorrent has access to the Internet through Wireguard VPN\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026#x27a1;\u0026#xfe0f;Make sure Wireguard and qBittorrent containers are up and running.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026#x27a1;\u0026#xfe0f;Check you are able to ping Wireguard container from qBittorrent\u003c/p\u003e","title":"Wireguard VPN and BitTorrent on Docker Swarm (Part 2)"},{"content":"Introduction I have been working on my home server setup on Raspberry Pi 4. I\u0026rsquo;d like to deploy all of my services/tools in docker containers, and for that, I need a nice and fancy container management tool I want to have on my home server.\nI had checked multiple sources, and finally picked two; they are Portainer and Yacht. Portainer is a well-known container management tool and Yacht is fairly new. In this post, I would like to give my thoughts on both tools.\n\u0026#x26a0;\u0026#xfe0f; Before we start, I just wanted to give a quick disclaimer; \u0026ldquo;This is neither a sponsored nor a promotion, purely came out of my simple observation here\u0026rdquo;.\nPortainer \u0026#x1f449; https://github.com/portainer/portainer\n\u0026#x1f449; https://docs.portainer.io/\nPortainer doesn\u0026rsquo;t require an introduction, is well known, written in Go lang and has 24k stars on Github. I find it a simple and powerful tool to manage docker containers that comes with Community Edition(CE) and Business Edition(BE). It also has integration with Kubernetes.\nArchitecture \u0026#x1f449; https://docs.portainer.io/start/architecture\nAs I said, it is simple and powerful, because of its architecture.\nIt has mainly 2 components, which are \u0026ldquo;server\u0026rdquo; and \u0026ldquo;agent\u0026rdquo;(and edge agent). By using agents it can manage containers on multiple locations/servers. But in my case, I don\u0026rsquo;t have to install an agent, since I only have one Raspberry Pi. I can deploy a single container and manage containers on the Raspberry Pi itself.\nCheck out my docker-compose files here\nYacht \u0026#x1f449; https://github.com/SelfhostedPro/Yacht\nI like Yacht at first look, it is simple and displays CPU and Memory utilization on the dashboard itself which is a big win. It is a very young project, written in Vue and has around 2k stars on GitHub\nUnlike Portainer, it is simple and doesn\u0026rsquo;t have an agent to get stats from multiple locations\nBrief Comparison Table Ok, let\u0026rsquo;s look at the direct comparisons.\n\u0026#x1f4c3; https://github.com/veerendra2/raspberrypi-homeserver/issues/1\nPortainer Yacht UI - Simple UI - Most of info(CPU and Memory usage) visible at a glance - Simple UI- Have to browse info like CPU and Memory Architecture Server and Agent. Can monitor multiple servers Simple, only monitors one host that is deployed Integrations K8s, Docker swarm, Azure ACI, Nomad N/A Docker-compose projects\u0026#x2757; Deploy apps from docker-compose.yml from Git and local files Only deploy apps only from remote git, not from local file system Can deploy from the local file system and remote git repo Container management\u0026#x2757; Like view logs, stop, kill, pause of container etc Yes Yes Predefined app templates\u0026#x2757; Deploys application directly from templates Yes Yes CPU and Memory usage \u0026#x2757; Observed with docker stats command. Screenshot here from #1 Relatively low Relatively high After observing brief differences, I decided to go with portainer, which has relatively low CPU and Memory usage which is important on ARM devices\n","permalink":"https://veerendra2.github.io/portainer-vs-yacht/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI have been working on my home server setup on Raspberry Pi 4. I\u0026rsquo;d like to deploy all of my services/tools in docker containers, and for that, I need a nice and fancy container management tool I want to have on my home server.\u003c/p\u003e\n\u003cp\u003eI had checked multiple sources, and finally picked two; they are Portainer and Yacht. Portainer is a well-known container management tool and Yacht is fairly new. In this post, I would like to give my thoughts on both tools.\u003c/p\u003e","title":"Portainer vs Yacht"},{"content":"There is a cool feature in docker called userns-remap, discovered while doing my RaspberryPi home server project; 15#issuecomment-1296311979, I can just enable userns-remap and docker does all remapping of uid and gid inside docker container to a non-root user on the host.\nhttps://docs.docker.com/engine/security/userns-remap/\nHow to enable *It is better to reinstall docker and remove all existing docker volumes\nAdd below /etc/docker/daemon.json { \u0026#34;userns-remap\u0026#34;: \u0026#34;default\u0026#34; } Restart the docker daemon $ sudo systemctl restart docker Ansible automation here In-Action # Run the Nginx container $ docker run -it -d nginx # Inside, the process thinks it is running as root! veerendra@atom:~$ docker exec -it nginx whoami root # But outside(on host namespace), the process running it as non-root user veerendra@atom:~$ ps aux | grep nginx 165536 350093 0.0 0.0 6320 4688 ? Ss 03:21 0:00 nginx: master process nginx -g daemon off; 165637 350208 0.0 0.0 6788 4288 ? S 03:21 0:01 nginx: worker process 165637 350209 0.0 0.0 6784 4284 ? S 03:21 0:00 nginx: worker process 165637 350210 0.0 0.0 6784 4284 ? S 03:21 0:01 nginx: worker process 165637 350212 0.0 0.0 6784 4284 ? S 03:21 0:01 nginx: worker process veerend+ 937492 0.0 0.0 6420 1844 pts/0 S+ 16:22 0:00 grep --color=auto nginx As you can see I have not specified any user while deploying the container, but the user inside the container is isolated i.e remapped to a non-root user(uid:165637, gid:165637) on the host\nI think it is very helpful and easy, instead of creating a gid and uid and specifying in docker-compose(or docker CLI).\nLimitation It has some limitation, as the docs says\nSharing PID or NET namespaces with the host (\u0026ndash;pid=host or \u0026ndash;network=host). External (volume or storage) drivers which are unaware or incapable of using daemon user mappings. Using the \u0026ndash;privileged mode flag on docker run without also specifying \u0026ndash;userns=host. When I tried to use filebrowser, the file permissions were very unpredictable and I was not able to access mounted partitions.\n","permalink":"https://veerendra2.github.io/docker-userns-remap/","summary":"\u003cp\u003eThere is a cool feature in docker called \u003ccode\u003euserns-remap\u003c/code\u003e, discovered while doing my RaspberryPi home server project; \u003ca href=\"https://github.com/veerendra2/raspberrypi-homeserver/issues/15#issuecomment-1296311979\"\u003e15#issuecomment-1296311979\u003c/a\u003e, I can just enable \u003ccode\u003euserns-remap\u003c/code\u003e and docker does all remapping of \u003ccode\u003euid\u003c/code\u003e and \u003ccode\u003egid\u003c/code\u003e inside docker container to a non-root user on the host.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://docs.docker.com/engine/security/userns-remap/\"\u003ehttps://docs.docker.com/engine/security/userns-remap/\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"how-to-enable\"\u003eHow to enable\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003e*It is better to reinstall docker and remove all existing docker volumes\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd below \u003ccode\u003e/etc/docker/daemon.json\u003c/code\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u0026#34;userns-remap\u0026#34;: \u0026#34;default\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003eRestart the docker daemon\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo systemctl restart docker\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003eAnsible automation \u003ca href=\"https://github.com/veerendra2/raspberrypi-homeserver/blob/main/tasks/docker.yml\"\u003ehere\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"in-action\"\u003eIn-Action\u003c/h1\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e# Run the Nginx container\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ docker run -it -d nginx\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e# Inside, the process thinks it is running as root!\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eveerendra@atom:~$ docker exec -it nginx whoami\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eroot\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e# But outside(on host namespace), the process running it as non-root user\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eveerendra@atom:~$ ps aux | grep nginx\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e165536    350093  0.0  0.0   6320  4688 ?        Ss   03:21   0:00 nginx: master process nginx -g daemon off;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e165637    350208  0.0  0.0   6788  4288 ?        S    03:21   0:01 nginx: worker process\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e165637    350209  0.0  0.0   6784  4284 ?        S    03:21   0:00 nginx: worker process\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e165637    350210  0.0  0.0   6784  4284 ?        S    03:21   0:01 nginx: worker process\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e165637    350212  0.0  0.0   6784  4284 ?        S    03:21   0:01 nginx: worker process\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eveerend+  937492  0.0  0.0   6420  1844 pts/0    S+   16:22   0:00 grep --color=auto nginx\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAs you can see I have not specified any user while deploying the container, but the user inside the container is isolated i.e remapped to a non-root user(uid:\u003ccode\u003e165637\u003c/code\u003e, gid:\u003ccode\u003e165637\u003c/code\u003e) on the host\u003c/p\u003e","title":"User Namespace Isolation in Docker"},{"content":"Introduction I have been working on a RaspberryPi home server project for quite some time. The project is a collection of applications to run on RaspberryPi and all applications are deployable with docker-compose files and ansible automation. One of the applications I was configuring is Pi-hole, a network-wide ad-blocker.\nI decided to use Pi-hole as also DHCP server for my LAN. When I look into docs, it says it has to be run as network_mode: host, because it allows Pi-hole to listen to DHCP broadcast packets. If the Pi-hole is deployed in bridge mode, there is a Linux bridge(Think of it as a router for a second!) which won\u0026rsquo;t allow broadcast packets.\nBut I want to run the Pi-hole container isolated and non-root, besides I\u0026rsquo;m using Nginx proxy for all of my apps(Check wiki pages). I kept scrolling docs, found this which I can use DHCP relay.\nA DHCP relay listens to DHCP broadcast packets and unicasts to the DHCP server which is in another network(We will see how it works below section)\nWith help of DerFetzer\u0026rsquo;s I have created a simple project where you can deploy Pi-hole with DHCP relay.\n\u0026#x1f449; https://github.com/veerendra2/pihole-dhcp-relay-docker\n$ git clone https://github.com/veerendra2/pihole-dhcp-relay-docker $ cd pihole-DHCP-relay-docker $ tree . . ├── dhcp-helper │ └── Dockerfile ├── dnsmasq.d │ ├── 07-dhcp-options.conf │ └── 99-temp.conf ├── docker-compose.yml ├── LICENSE ├── pihole │ └── custom.list └── README.md 3 directories, 7 files How it works Here is the simple network diagram that you can see the definition indocker-compose.yml\n┌───────────────┐ │ dhcp-relay-net│ \u0026lt;-------- Linux bridge └─┬───────────┬─┘ │ │eth1 ┌────┴──┐ ┌───┴───┐ │DHCP │ │PiHole │ │Relay │ │ │ └───┬───┘ └───┬───┘ │ │eth0 │ ┌────┴───────┐ │ │ front-tier │ \u0026lt;-- Linux bridge │ └────┬───────┘ ┌─────┴────────────┴───────┐ │ Host Network │ | (RaspberryPi) | └──────────────────────────┘ ^ eth0 (192.168.0.120) | | v LAN As you can see above the Pi-hole is deployed in bridge mode, there is a Linux bridge(front-tier) between the host network stack and the Pi-hole container. So, any broadcasts won\u0026rsquo;t allow through front-tier bridge(router*). So, I solve the problem by adding one more container DHCP Relay run as network_mode: host in which the container is attached to the host network stack as you can see there is no Linux bridge which means, DHCP Relay container listens all DHCP broadcast and unicast to dhcp-relay-net Linux bridge and Pi-hole also connected to it. That\u0026rsquo;s how devices in LAN get the IP from bridged Pi-hole\nFurther Configurations DHCP Options Config After I deploy the above setup, my phone gets an IP address but not the Internet. I wasn\u0026rsquo;t sure what was going on, maybe it is that DHCP is not providing the DNS address which is my RaspberryPI external LAN IP?\nI installed Wireshark and capture the DHCP packets to see what it contains\nBing go!, it is 172.31.0.10 the IP I specified in DHCP relay. So, after googling, I found\n$ cat dnsmasq.d/07-dhcp-options.conf dhcp-option=option:dns-server,192.168.0.120 Then, my phone gets the correct DNS address and accesses the Internet.\nSuppress dnsmasq warnings I noticed, in the Pi-hole dashboard, dnsmasq is throwing warnings\nThat is due to the IP range I set in the DHCP pool(.env) doesn\u0026rsquo;t cover the network 172.31.0.0/16 which Pi-hole directly connect with eth1 interface(Above diagram). In this case, there are 2 options to get rid of these warnings.\nDisable warnings Like in the below screenshots, but you might miss any other important warnings\nNo DHCP Interface Set no-dhcp-interfaces like below\n$ cat dnsmasq.d/99-temp.conf no-dhcp-interface=eth0,eth1 Conclusion This simple trick allows me to run Pi-host in bridge mode and run behind the Nginx proxy. Hope you find a useful piece of information\n","permalink":"https://veerendra2.github.io/pihole-dhcp-relay/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eI have been working on a RaspberryPi home server project for quite some time. The project is a collection of applications to run on RaspberryPi and all applications are deployable with \u003ccode\u003edocker-compose\u003c/code\u003e files and ansible automation. One of the applications I was configuring is \u003ca href=\"https://pi-hole.net\"\u003ePi-hole\u003c/a\u003e, a network-wide ad-blocker.\u003c/p\u003e\n\u003cp\u003eI decided to use Pi-hole as also DHCP server for my LAN. When I look into \u003ca href=\"https://docs.pi-hole.net/docker/DHCP/\"\u003edocs\u003c/a\u003e, it says it has to be run as \u003ccode\u003enetwork_mode: host\u003c/code\u003e, because it allows Pi-hole to listen to DHCP broadcast packets. If the Pi-hole is deployed in bridge mode, there is a Linux bridge(Think of it as a router for a second!) which won\u0026rsquo;t allow broadcast packets.\u003c/p\u003e","title":"Pi-hole with DHCP Relay in Docker"},{"content":"Introduction Hello my dear fellow humans, hope you are having a great day. Today\u0026rsquo;s guide is on how to recover from a disaster for Strimzi Kafka with Velero. First of all, what is Strmzi Kafka?\nhttps://strimzi.io\nStrimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\nBack in a while, I worked on Strimzi Kafka deployment on Openshift, very easy to set up and manage production-level Kafka cluster on Kubernetes, I have to give credit to the Strimzi project team, did a great job on documentation, support on Github discussions and active developments.\nOne of the important things in IT, when you bring new tech into a team, it should be disaster recovery proof. The team should be able to recover data and bring it back to a normal state after a disaster. For strimzi, it is very easy once you set up backend PV as dynamic storage class provisioner as you can 11.6. Recovering a cluster from persistent volumes. In this guide, I use the velero tool to make things automated and can easily set up in GitOps.\nHeads-up \u0026#x270b; Before dive into this guide, I want to make you aware of my existing setup and scope\nStrimzi Kafka stateful app is up and running on Kubernetes cluster with backend PV storage class provisioner as AzureDisk. Velero server up and running. Check my previous post \u0026ldquo;Velero Deployment with Kustomize (Azure)\u0026rdquo; to know how to set up. This guide does not cover basics of Kafka or how to set up Strimzi Kafka on kubernetes, head over to Strimzi Github project page and browse resources. But for the sake of this guide I create a demo repo containing Strimzi Kafka deployment files \u0026#x1f449; https://github.com/veerendra2/strimzi-kafka-demo\nScenario This is the simple test scenario I picked to test recovery steps. The test is to produce logs/messages to Kafka brokers and read the produced logs/messages from Kafka brokers after recovery. If the recovery is successful, the consumer should able to fetch logs/messages after recovery as you can see in the below diagram\nAssume below are events that happen over time.\nProducer produced some logs/messages to Kafka system Kafka brokers receives logs/messages and stores on disk Disaster happens(Refer \u0026ldquo;Disaster Simulation\u0026rdquo; to know how to simulate) Recovery (Refer \u0026ldquo;Recovery Plan\u0026rdquo; to know how to recover) Kafka recovered from disaster. Up and running Consumers now consume logs/messages which are produced in above point 1. If recovery is successful, the consumer should be able to fetch logs/messages there were stored in Kafka system before disaster. Disaster Simulation Before performing disaster simulation, Kafka cluster should be up and running, there should be some data generated on the cluster. In the prerequisites section, we will see how to prepare for disaster.\nCluster preparation Velero CLI tool should be installed on your local machine.\n$ git clone https://github.com/veerendra2/strimzi-kafka-demo $ cd strimzi-kafka-demo $ kubectl create -f base/namespace.yaml $ kubectl project kafka $ kubectl create -f base/cluster-operator.yaml $ kubectl create -f base/configmaps/ $ kubectl create -f stages/dev/deployment.yaml $ kubectl create -f stages/dev/topics.yaml $ kubectl create -f stages/dev/users.yaml Wait until kafka cluster bootstrap and verify everything is running\n$ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE carbon-dev-entity-operator-df5h6497-6xmqf 3/3 Running 0 4m carbon-dev-kafka-0 1/1 Running 0 6m carbon-dev-kafka-1 1/1 Running 0 6m carbon-dev-kafka-2 1/1 Running 0 6m carbon-dev-kafka-exporter-657956b4a-zpmdg 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-658y5cf364-tw2nh 1/1 Running 0 2h $ kubectl get pvc -n kafka NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-0-carbon-dev-kafka-0 Bound pvc-8092d619-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-0-carbon-dev-kafka-1 Bound pvc-8093210e-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-0-carbon-dev-kafka-2 Bound pvc-8093c74a-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-carbon-dev-zookeeper-0 Bound pvc-4105d01a-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m data-carbon-dev-zookeeper-1 Bound pvc-4106f177-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m data-carbon-dev-zookeeper-2 Bound pvc-41072398-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m $ kubectl get kafkatopic -n kafka NAME CLUSTER PARTITIONS REPLICATION FACTOR my-topic carbon-dev 1 1 $ kubectl get kafkauser -n kafka NAME CLUSTER AUTHENTICATION AUTHORIZATION my-user carbon-dev tls simple Deploy sample producer app to produce logs/messages to Kafka\n## Check deployment config. For example, broker bootstrap route, topic name and user name. Deploy consumer test app $ kubectl create -f kafka-producer.yaml deployment.apps/java-kafka-producer created $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE java-kafka-producer-bfd975945-cnmgg 0/1 Completed 1 42s carbon-dev-entity-operator-df5h6497-6xmqf 3/3 Running 0 4m carbon-dev-kafka-0 1/1 Running 0 6m carbon-dev-kafka-1 1/1 Running 0 6m carbon-dev-kafka-2 1/1 Running 0 6m carbon-dev-kafka-exporter-657956b4a-zpmdg 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-658y5cf364-tw2nh 1/1 Running 0 2h $ kubectl logs java-kafka-producer-bfd975945-cnmgg -n kafka ... 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 997\u0026#34; \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----------------THESE LOG LINES WE NEED TO LOOK TO KNOW PRODUCER SENT TO KAFKA 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 998\u0026#34; 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 999\u0026#34; 2022-08-18 15:29:33 INFO KafkaProducerExample:91 - 1000 messages sent ... 2994 [main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed 3009 [main] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered [2022-08-18T15:29:33.852+0000] Heap [2022-08-18T15:29:33.852+0000] def new generation total 150016K, used 32262K [0x0000000619400000, 0x00000006236c0000, 0x00000006bb800000) [2022-08-18T15:29:33.852+0000] eden space 133376K, 24% used [0x0000000619400000, 0x000000061b3819d0, 0x0000000621640000) [2022-08-18T15:29:33.852+0000] from space 16640K, 0% used [0x0000000621640000, 0x0000000621640000, 0x0000000622680000) [2022-08-18T15:29:33.852+0000] to space 16640K, 0% used [0x0000000622680000, 0x0000000622680000, 0x00000006236c0000) [2022-08-18T15:29:33.852+0000] tenured generation total 333184K, used 7580K [0x00000006bb800000, 0x00000006cfd60000, 0x0000000800000000) [2022-08-18T15:29:33.852+0000] the space 333184K, 2% used [0x00000006bb800000, 0x00000006bbf672b0, 0x00000006bbf67400, 0x00000006cfd60000) [2022-08-18T15:29:33.852+0000] Metaspace used 23126K, capacity 23940K, committed 24192K, reserved 1071104K [2022-08-18T15:29:33.852+0000] class space used 2563K, capacity 2885K, committed 2944K, reserved 1048576K Configure backup Once Kafka is loaded with sample data, configure backup like below.\n# Velero binary uses a local kubeconfig file to manage velero deployment. So, before running velero, login into cluster # Run one time backup for the test scenario $ velero backup create kafka-backup --include-namespaces=kafka --include-resources persistentvolumeclaims, persistentvolumes Backup request \u0026#34;kafka-backup\u0026#34; submitted successfully. Run `velero backup describe kafka-backup` or `velero backup logs kafka-backup` for more details. ## Verify backup is \u0026#34;Completed\u0026#34; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup Completed 2021-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Destroy Delete kafka resources in kafka namespace\n## Note down below PVs $ kubectl get pvc -n kafka | awk \u0026#39;{print $3}\u0026#39; | tail -n+2 pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f pvc-9aadbaf8-4afa-11ec-9048-12rd2ab3g21f pvc-9aadd6b8-4afa-11ec-9048-12rd2ab3g21f pvc-6d3118cf-4afa-11ec-9048-12rd2ab3g21f pvc-6d3172c1-4afa-11ec-9048-12rd2ab3g21f pvc-6d3176b9-4afa-11ec-9048-12rd2ab3g21f ## Delete kafka cluster, PVC and namesapce $ kubectl delete kafka `kubectl get kafka -n kafka | awk \u0026#39;{print $1}\u0026#39; | tail -n+2` $ kubectl delete pv `kubectl get pvc -n kafka | awk \u0026#39;{print $3}\u0026#39; | tail -n+2` $ kubectl delete pvc `kubectl get pvc -n kafka | awk \u0026#39;{print $1}\u0026#39; | tail -n+2` $ kubectl delete -f base/cluster-operator.yaml $ kubectl delete namespace kafka Delete disks in cloud provider portal UI to make disaster more solid if required\nRecovery Steps Preparation Velero CLI should be installed on your local machine (Refere Basic Install). All strimzi deployments files should be exactly the same as before the disaster. Check which backups you want to restore\n# Login into cluster $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup Completed 2021-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Restore disk In below example uses \u0026ldquo;kafka-backup\u0026rdquo; backup to restore from it\n$ velero restore create --from-backup kafka-backup Restore request \u0026#34;kafka-backup\u0026#34; submitted successfully. Run `velero restore describe kafka-backup` or `velero restore logs kafka-backup` for more details. # Wait until the restore completed $ velero restore describe kafka-backup Name: kafka-backup Namespace: velero Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Phase: InProgress \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;---STATUS Backup: kafka-backup Namespaces: Included: all namespaces found in the backup Excluded: \u0026lt;none\u0026gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io Cluster-scoped: auto Namespace mappings: \u0026lt;none\u0026gt; Label selector: \u0026lt;none\u0026gt; Restore PVs: auto Once restore is completed, check PVCs created.\n\u0026#x2757; After restore completed, the PV \u0026ldquo;names\u0026rdquo; will same as during backup, but underneath the actual disk name is different in Azure cloud which you can see in below snippet\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-0-carbon-dev-kafka-0 Bound pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-0-carbon-dev-kafka-1 Bound pvc-9aadbaf8-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-0-carbon-dev-kafka-2 Bound pvc-9aadd6b8-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-carbon-dev-zookeeper-0 Bound pvc-6d3118cf-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m data-carbon-dev-zookeeper-1 Bound pvc-6d3172c1-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m data-carbon-dev-zookeeper-2 Bound pvc-6d3176b9-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m $ kubectl describe pv pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f Name: pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f Labels: velero.io/backup-name=kafka-backup velero.io/restore-name=kafka-backup Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: generic-retain Status: Bound Claim: kafka/data-0-carbon-dev-kafka-0 Reclaim Policy: Retain Access Modes: RWO Capacity: 256Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: restore-1b8257d7-2156-44ae-aa30-36f2gc6c64r6 ## \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----DISK NAME IS DIFFERENT AFTER RESTORE DiskURI: /subscriptions/fd667dc8-1e9e-4ab4-b428-9879d458e8ac/resourceGroups/carbondev-openshift/providers/Microsoft.Compute/disks/restore-1b8257d7-2156-44aeaa30-36f2gc6c64r6 Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u0026lt;none\u0026gt; Re-deploy Strimzi Kafka to recover From strimzi docs; 10.5.3. Recovering a deleted cluster from persistent volumes\nYou can recover a Kafka cluster from persistent volumes (PVs) if they are still present. You might want to do this, for example, after:\nA namespace was deleted unintentionally A whole Kubernetes cluster is lost, but the PVs remain in the infrastructure In our deployment, we use jbod config, the generate PVC name should be data-0-[CLUSTER-NAME]-kafka-0\n1. Bring up cluster-operator $ git clone https://github.com/veerendra2/strimzi-kafka-demo $ cd strimzi-kafka-demo $ kubectl create -f base/namepsace.yaml $ kubectl create -f base/cluster-operator.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-5c8d5cf966-dhhws 1/1 Running 0 2m 2. Deploy topics \u0026#x26a0;\u0026#xfe0f; If you deploy topic-operator before deploying topics, the topic-operator deletes existing topics while bootstrapping. That\u0026rsquo;s why we need to deploy topics first\n$ kubectl create -f stages/dev/topics.yaml 3. Deploy Kafka cluster and users $ kubectl create -f stages/dev/deployment.yaml kafka.kafka.strimzi.io/carbon-dev created $ kubectl get pods NAME READY STATUS RESTARTS AGE carbon-dev-entity-operator-cd54f496-vmsfm 3/3 Running 0 3m carbon-dev-kafka-0 1/1 Running 0 5m carbon-dev-kafka-1 1/1 Running 0 5m carbon-dev-kafka-2 1/1 Running 0 5m carbon-dev-kafka-exporter-c67976b6b-vzdff 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-5c8d5cf966-dfhwf 1/1 Running 0 10m $ kubectl create -f stages/dev/users.yaml # Verify all users and topics are created $ kubectl get kafkatopic NAME CLUSTER PARTITIONS REPLICATION FACTOR my-topic carbon-dev 1 1 $ kubectl get kafkauser NAME CLUSTER AUTHENTICATION AUTHORIZATION my-user carbon-dev tls simple If everything works well, the deployment should pick up existing PVCs and running!\nVerification \u0026#x1f449; This verification is based on the scenario we picked.\nIn the above section \u0026ldquo;Disaster Simulation\u0026rdquo;, we deployed a sample producer java app to produce logs/messages into Kafka. Now in this step(after recovery), we fetch those logs/messages to see recovery was successful\n# Check deployment config. For example, broker bootstrap route, topic name and user name. Deploy consumer test app $ kubectl create -f kafka-consumer.yaml deployment.apps/java-kafka-consumer created $ kubectl get pods NAME READY STATUS RESTARTS AGE java-kafka-consumer-7456748dbc-vvftf 1/1 Running 0 37s carbon-dev-entity-operator-cd54f496-vmsvm 3/3 Running 0 11m carbon-dev-kafka-0 1/1 Running 0 4m carbon-dev-kafka-1 1/1 Running 0 3m carbon-dev-kafka-2 1/1 Running 0 3m carbon-dev-kafka-exporter-c67976b6b-vz6f6 1/1 Running 0 10m carbon-dev-zookeeper-0 1/1 Running 0 15m carbon-dev-zookeeper-1 1/1 Running 0 15m carbon-dev-zookeeper-2 1/1 Running 0 15m strimzi-cluster-operator-5c8d5cf966-dhhws 1/1 Running 0 18m ## Check logs of the app, see it is fetching messages that were pushed before disaster $ kubectl logs java-kafka-consumer-7456748dbc-vvftf 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20891 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 891\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20892 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 892\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20893 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 893\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20894 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 894\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: ... If you able to see output like above(hello message), the Strimzi Kafka is restored properly\nConclusion In this guide, we have seen how to set up a disaster recovery plan for Strimzi Kafka by using a simple scenario and simulating disaster with Velero.\n","permalink":"https://veerendra2.github.io/strimzi-kafka-disaster-recovery/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eHello my dear fellow humans, hope you are having a great day. Today\u0026rsquo;s guide is on how to recover from a disaster for \u003ca href=\"https://strimzi.io/\"\u003eStrimzi Kafka\u003c/a\u003e with \u003ca href=\"https://velero.io/\"\u003eVelero\u003c/a\u003e. First of all, what is Strmzi Kafka?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://strimzi.io\"\u003ehttps://strimzi.io\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStrimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static_content/images/strimzi-logo.png\" alt=\"Strizi Logo\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eBack in a while, I worked on Strimzi Kafka deployment on Openshift, very easy to set up and manage production-level Kafka cluster on Kubernetes, I have to give credit to the Strimzi project team, did a great job on documentation, support on Github discussions and active developments.\u003c/p\u003e","title":"Strimzi Kafka Disaster Recovery with Velero"},{"content":" \u0026#x1f449; This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series.\nImportant Elasticsearch Configuration \u0026#x1f4c4; Official Docs\nMainly 3 configuration files\nelasticsearch.yml - Elasticsearch config jvm.options - Elasticsearch JVM settings config log4j2.properties - Elasticsearch logging config Environment Variables\nexport the ES_PATH_CONF etc/default/elasticsearch (Sourced environment variables from. Recommended) Settings \u0026#x1f4c4; Official Docs\nBefore going to production, it is recommended go through be below elasticsearch configs. Refer sample_config directory for configuration\nConfiguration Description Configuration Reference Path settings Log and data config Refer here Cluster name Cluster name Refer here Node name Node name Refer here Network host IP address that elasticsearch bind on Refer here Discovery settings Cluster discovery and initial master config Refer here Heap size JVM heap memory configuration Recommended heap size should be half of system memory. Make sure min and max heap memory same value. Refer here Heap dump path Heap dump location path config Default config is sufficient. Refer here GC logging Garbage collection logging configuration Default config is sufficient. Refer here Temp directory Configure private temporary directory that Elasticsearch uses is excluded from periodic cleanup Important System Configuration \u0026#x1f4c4; Offical Docs\nBefore going to production, it is recommended go through be blow system configs Configuration Description Remark Disable swapping Disable swapping to prevent JVM heap or even its executable pages being swapped out to disk File descriptors Increase file descriptors for the user running Elasticsearch Virtual memory Increase mmap counts to prevent memory exceptions. DNS cache settings Overide JVM DNS positive/negetive cache settings (Leave default value) Temporary directory not mounted with noexec As the native library is mapped into the JVM virtual address space as executable, the underlying mount point of the location that this code is extracted to must not be mounted with noexec as this prevents the JVM process from being able to map this code as executable Bootstrap Checks \u0026#x1f4c4; Offical Docs\nOnce you configured above configuration, elasticsearch performs some checks during bootstrap to verify configuration. If Elasticsearch is in development mode, any bootstrap checks that fail appear as warnings in the Elasticsearch log. If Elasticsearch is in production mode, any bootstrap checks(below) that fail will cause Elasticsearch to refuse to start.\nBelow are the boostrap checks.(In case, elasticsearch failed to start, below is the check list to verify)\nCheck Name Description Heap size check Enforces to start the JVM with the initial heap size equal to the maximum heap size to avoid these resize pauses File descriptor checks Enforces elasticsearch have good number of file descriptor Memory lock check Enforces JVM heap memory lock to avoid swapping pages to disk. Maximum number of theard pool checks Enforces Elasticsearch process has the rights to create enough threads under normal use. Max file size check Enforces that the Elasticsearch process can create max file size is unlimited Max size virtual memory check Enforces that the Elasticsearch process has unlimited address space Max map count check Enforces that the kernel allows a process to have at least 262,144 memory-mapped areas Client JVM check Enforces that the Elasticsearch start with the server JVM. Refer doc Use serial collector check Enforces that the Elasticsearch is not configured to run with the \u0026ldquo;serial collector\u0026rdquo; type JVM System call filter check Enforces system call filters are enabled which is an ability to execute system calls related to forking against arbitrary code execution attacks on Elasticsearch OnError and OnOutOfMemoryError check Enforces JVM has options related to OnError or OnOutOfMemoryError enabled Early-access check Enforces to start Elasticsearch on a release build of the JVM. Nor early-access snapshots of upcoming releases which are not suitable for production G1GC check Checks versions of the HotSpot JVM, Refer docs All permission check Enforces security policy used during bootstrap does not grant the java.security.AllPermission to Elasticsearch Discovery config check Enforces discovery is not running with the default configuration Docker Container Labeling Useful to filter logs, events, etc at logstash and also at kibana dashboard\ncom.yourdomain.container.type: \u0026#34;heartbeat\u0026#34; \u0026#34;metricbeat\u0026#34; \u0026#34;filebeat\u0026#34; \u0026#34;application\u0026#34; com.yourdomain.container.app.version: \u0026#34;1.2\u0026#34; com.yourdomain.container.environment: \u0026#34;stagging\u0026#34; \u0026#34;production\u0026#34; com.yourdomain.container.name: \u0026#34;auditlog-1\u0026#34; \u0026#34;odoo-1\u0026#34; Index Life Cycle Management Definitions Index Life Cycle Actions Index Roll Over Concept Index Aliasing - Stackoverflow Other elasticsearch terminologies - Stackoverflow ","permalink":"https://veerendra2.github.io/elasticsearch-deploy/config-overview/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f449; This writeup is part of \u003ca href=\"https://veerendra2.github.io/posts/elasticsearch-deploy/\" title=\"previous post\"\u003e\u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo;\u003c/a\u003e series.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"important-elasticsearch-configuration\"\u003eImportant Elasticsearch Configuration\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html\"\u003eOfficial Docs\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMainly 3 configuration files\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eelasticsearch.yml - Elasticsearch config\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003ejvm.options       - Elasticsearch JVM settings config\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003elog4j2.properties - Elasticsearch logging config\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEnvironment Variables\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eexport\u003c/code\u003e the ES_PATH_CONF\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eetc/default/elasticsearch\u003c/code\u003e (Sourced environment variables from. Recommended)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"settings\"\u003eSettings\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html\"\u003eOfficial Docs\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBefore going to production, it is recommended go through be below elasticsearch configs. Refer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config\"\u003e\u003ccode\u003esample_config\u003c/code\u003e\u003c/a\u003e directory for configuration\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eConfiguration\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eConfiguration Reference\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePath settings\u003c/td\u003e\n          \u003ctd\u003eLog and data config\u003c/td\u003e\n          \u003ctd\u003eRefer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/elasticsearch.yml#L40\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCluster name\u003c/td\u003e\n          \u003ctd\u003eCluster name\u003c/td\u003e\n          \u003ctd\u003eRefer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/elasticsearch.yml#L17\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eNode name\u003c/td\u003e\n          \u003ctd\u003eNode name\u003c/td\u003e\n          \u003ctd\u003eRefer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/elasticsearch.yml#L23\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eNetwork host\u003c/td\u003e\n          \u003ctd\u003eIP address that elasticsearch bind on\u003c/td\u003e\n          \u003ctd\u003eRefer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/elasticsearch.yml#L59\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDiscovery settings\u003c/td\u003e\n          \u003ctd\u003eCluster discovery and initial master config\u003c/td\u003e\n          \u003ctd\u003eRefer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/elasticsearch.yml#L72\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eHeap size\u003c/td\u003e\n          \u003ctd\u003eJVM heap memory configuration\u003c/td\u003e\n          \u003ctd\u003eRecommended heap size should be half of system memory. Make sure min and max heap memory same value. Refer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/jvm.options#L22\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eHeap dump path\u003c/td\u003e\n          \u003ctd\u003eHeap dump location path config\u003c/td\u003e\n          \u003ctd\u003eDefault config is sufficient. Refer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elasticsearch/sample_config/jvm.options#L61\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGC logging\u003c/td\u003e\n          \u003ctd\u003eGarbage collection logging configuration\u003c/td\u003e\n          \u003ctd\u003eDefault config is sufficient. Refer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elastic/search/sample_config/jvm.options#L66\"\u003ehere\u003c/a\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eTemp directory\u003c/td\u003e\n          \u003ctd\u003eConfigure private temporary directory that Elasticsearch uses is excluded from periodic cleanup\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"important-system-configuration\"\u003eImportant System Configuration\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/system-config.html\"\u003eOffical Docs\u003c/a\u003e\u003c/p\u003e","title":"Elasticsearch Configuration Overview"},{"content":" \u0026#x1f449; This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nInstall \u0026#x1f4c4; Office Docs\nHardware Requirement \u0026#x1f4c4; Offical Docs\nResource Minimum Recommended Memory 16 GB 64 GB CPU 8 Cores 16 Disk Depends Depends JDK Installation Pick JVM compatibility version with elasticsearch from here Install OpenJDK from here Download and install JDK 11 (Another guide here) $ apt-get install openjdk-11-jdk -y $ java -version openjdk version \u0026#34;11.0.6\u0026#34; 2020-01-14 OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1) OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing) Elasticsearch Installation Download latest elasticsearch from here (As of today the latest version is 7.6.2) Recommended to download/install package via .dep or PPA which postscripts creates user, groups and adds under systemd Install via apt-get from here\n$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - $ sudo apt-get install apt-transport-https $ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install elasticsearch *DON\u0026rsquo;T START elasticsearch DAEMON YET!!! We need to configure\nSystem Configuration \u0026#x1f4c4; Official Docs\nCreate data directory(This data directory you have to specify in elasticsearch config) $ mkdir /var/data/elasticsearch $ chown -R elasticsearch:elasticsearch /var/data/elasticsearch Disable swapping $ sudo swapoff -a ## *** Perminent Config *** ## Comment out any lines that contain the word \u0026#39;swap\u0026#39; in \u0026#39;/etc/fstab\u0026#39; Increase mmap count $ sudo sysctl -w vm.max_map_count=262144 ## *** Perminent Config *** ## update the vm.max_map_count setting in /etc/sysctl.conf Increase file descriptors for elasticseach user $ sudo su # ulimit -n 65535 ## *** Perminent Config *** ## echo \u0026#39;elasticsearch - nofile 65535\u0026#39;| sudo tee /etc/security/limits.conf ## Ubuntu ignores the limits.conf file for processes started by init.d. To enable the limits.conf file, edit /etc/pam.d/su and uncomment the following line ## # session required pam_limits.so Systems which uses systemd limits need to be specified via systemd\n$ sudo systemctl edit elasticsearch ## Above command opens editor to override the config. Add below setting in it [Service] LimitMEMLOCK=infinity Increase number of threads count for elasticsearch user $ sudo su # ulimit -u 4096 ## *** Perminent Config *** ## echo \u0026#39;elasticsearch - nproc 4096\u0026#39;| sudo tee /etc/security/limits.conf ## Ubuntu ignores the limits.conf file for processes started by init.d. To enable the limits.conf file, edit /etc/pam.d/su and uncomment the following line ## # session required pam_limits.so JVM Configuration Elasticsearch needs more heap memory. Change the JVM heap memory accordingly, please be noted that more heap means more garbage collection i.e more CPU utilization\nChange max and min heap in /etc/elasticsearch/jvm.options like below (Currently we are setting 15GB for heap)\n... -Xms15g -Xmx15g ... Go through other JVM config or logging config if required NOTE: Make sure min and max heap memories are same value to avoid bootstrap check failures\nElasticsearch Configuration \u0026#x1f4c4; Official Docs\nIf you install elasticsearch via apt-get or .dep file, the config files are located in /etc/elasticsearch/ directory Refer elasticsearch.yml configuration in config_sample directory specify nodes and copy configurations to nodes accordingly Once everything is ready, start elasticsearch daemon in all nodes\n$ sudo systemctl enable elasticsearch $ sudo systemctl start elasticsearch If everything is ok, you should get responses like below\n$ curl -XGET 10.29.103.18:9200 { \u0026#34;name\u0026#34; : \u0026#34;carbon-1\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;carbon\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;zpoOdANFSXujgNMplMz1fQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.6.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;deb\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;ef48eb35cf30adf4db14086e8aabd07ef6fb113f\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2020-03-26T06:34:37.794943Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.4.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } $ curl -XGET 10.29.103.18:9200/_cat/nodes 10.29.103.18 7 74 5 0.07 0.91 1.95 dilm * carbon-1 10.29.103.10 2 87 25 0.08 1.08 1.68 dil - carbon-3 10.29.103.16 5 76 25 0.18 1.13 1.75 dil - carbon-2 Configure Security Starting from versions 6.8.0 and 7.1.0, security comes in basic. Read update blog here Watch Getting Started with Free Elasticsearch Security Features Configure transport layer SSL for node communication There is tool elasticsearch-certutil which comes with elasticsearch installation. Depends on requirement generate keys csr to get certificate signing request and sign it with CA cert to get self signed cerificates ca to get certificate authority http for certificates fot HTTP $ cd /usr/share/elasticsearch/bin $ ./elasticsearch-certutil --help Simplifies certificate creation for use with the Elastic Stack Commands -------- csr - generate certificate signing requests cert - generate X.509 certificates and keys ca - generate a new local certificate authority http - generate a new certificate (or certificate request) for the Elasticsearch HTTP interface Non-option arguments: command Option Description ------ ----------- -E \u0026lt;KeyValuePair\u0026gt; Configure a setting -h, --help Show help -s, --silent Show minimal output -v, --verbose Show verbose output Generating .p12 and copy the file to every node\n./elasticsearch-certutil cert ... Please enter the desired output file [elastic-certificates.p12]:/etc/elasticsearch/elastic-certificates.p12 ... Enable SSL and specify .p12 file location in config like below in every node in elasticsearch.yml xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/elastic-certificates.p12 Restart elasticsearch daemon in all nodes\n$ sudo systemctl restart elasticsearch Generate Passwords Login into master node and run below commands $ cd /usr/share/elasticsearch $ sudo bin/elasticsearch-setup-passwords auto Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user. The passwords will be randomly generated and printed to the console. Please confirm that you would like to continue [y/N]y Changed password for user apm_system PASSWORD apm_system = xxxxxxxxxxxxxx Changed password for user kibana PASSWORD kibana = xxxxxxxxxxxxxx Changed password for user logstash_system PASSWORD logstash_system = xxxxxxxxxxxxxx Changed password for user beats_system PASSWORD beats_system = xxxxxxxxxxxxxx Changed password for user remote_monitoring_user PASSWORD remote_monitoring_user = xxxxxxxxxxxxxx Changed password for user elastic PASSWORD elastic = xxxxxxxxxxxxxx Copy those credentials and store it safe.\nThe elastic user is cluster admin user The kibana user is for kibana to authenticate with elasticsearch Configure HTTPS for REST API Generate certificates(Below is the process to generate certificates) $ cd /usr/share/elasticsearch $ sudo bin/elasticsearch-certutil http ## Elasticsearch HTTP Certificate Utility The \u0026#39;http\u0026#39; command guides you through the process of generating certificates for use on the HTTP (Rest) interface for Elasticsearch. This tool will ask you a number of questions in order to generate the right set of files for your needs. ## Do you wish to generate a Certificate Signing Request (CSR)? A CSR is used when you want your certificate to be created by an existing Certificate Authority (CA) that you do not control (that is, you don\u0026#39;t have access to the keys for that CA). If you are in a corporate environment with a central security team, then you may have an existing Corporate CA that can generate your certificate for you. Infrastructure within your organisation may already be configured to trust this CA, so it may be easier for clients to connect to Elasticsearch if you use a CSR and send that request to the team that controls your CA. If you choose not to generate a CSR, this tool will generate a new certificate for you. That certificate will be signed by a CA under your control. This is a quick and easy way to secure your cluster with TLS, but you will need to configure all your clients to trust that custom CA. Generate a CSR? [y/N]n ## Do you have an existing Certificate Authority (CA) key-pair that you wish to use to sign your certificate? If you have an existing CA certificate and key, then you can use that CA to sign your new http certificate. This allows you to use the same CA across multiple Elasticsearch clusters which can make it easier to configure clients, and may be easier for you to manage. If you do not have an existing CA, one will be generated for you. Use an existing CA? [y/N]n A new Certificate Authority will be generated for you ## CA Generation Options The generated certificate authority will have the following configuration values. These values have been selected based on secure defaults. You should not need to change these values unless you have specific requirements. Subject DN: CN=Elasticsearch HTTP CA Validity: 5y Key Size: 2048 Do you wish to change any of these options? [y/N]n ## CA password We recommend that you protect your CA private key with a strong password. If your key does not have a password (or the password can be easily guessed) then anyone who gets a copy of the key file will be able to generate new certificates and impersonate your Elasticsearch cluster. IT IS IMPORTANT THAT YOU REMEMBER THIS PASSWORD AND KEEP IT SECURE CA password: [\u0026lt;ENTER\u0026gt; for none] ## How long should your certificates be valid? Every certificate has an expiry date. When the expiry date is reached clients will stop trusting your certificate and TLS connections will fail. Best practice suggests that you should either: (a) set this to a short duration (90 - 120 days) and have automatic processes to generate a new certificate before the old one expires, or (b) set it to a longer duration (3 - 5 years) and then perform a manual update a few months before it expires. You may enter the validity period in years (e.g. 3Y), months (e.g. 18M), or days (e.g. 90D) For how long should your certificate be valid? [5y] 5Y ## Do you wish to generate one certificate per node? If you have multiple nodes in your cluster, then you may choose to generate a separate certificate for each of these nodes. Each certificate will have its own private key, and will be issued for a specific hostname or IP address. Alternatively, you may wish to generate a single certificate that is valid You entered the following hostnames. - elk-staging.mydomain.com Is this correct [Y/n]y ## Which IP addresses will be used to connect to your nodes? If your clients will ever connect to your nodes by numeric IP address, then you can list these as valid IP \u0026#34;Subject Alternative Name\u0026#34; (SAN) fields in your certificate. If you do not have fixed IP addresses, or not wish to support direct IP access to your cluster then you can just press \u0026lt;ENTER\u0026gt; to skip this step. across all the hostnames or addresses in your cluster. If all of your nodes will be accessed through a single domain (e.g. node01.es.example.com, node02.es.example.com, etc) then you may find it simpler to generate one certificate with a wildcard hostname (*.es.example.com) and use that across all of your nodes. However, if you do not have a common domain name, and you expect to add additional nodes to your cluster in the future, then you should generate a certificate per node so that you can more easily generate new certificates when you provision new nodes. Generate a certificate per node? [y/N]n ## Which hostnames will be used to connect to your nodes? These hostnames will be added as \u0026#34;DNS\u0026#34; names in the \u0026#34;Subject Alternative Name\u0026#34; (SAN) field in your certificate. You should list every hostname and variant that people will use to connect to your cluster over http. Do not list IP addresses here, you will be asked to enter them later. If you wish to use a wildcard certificate (for example *.es.example.com) you can enter that here. Enter all the hostnames that you need, one per line. When you are done, press \u0026lt;ENTER\u0026gt; once more to move on to the next step. elk-staging.mydomain.com Enter all the IP addresses that you need, one per line. When you are done, press \u0026lt;ENTER\u0026gt; once more to move on to the next step. 85.xx.xx.xx 85.xx.xx.xx 85.xx.xx.xx You entered the following IP addresses. - 85.xx.xx.xx - 85.xx.xx.xx - 85.xx.xx.xx Is this correct [Y/n]y ## Other certificate options The generated certificate will have the following additional configuration values. These values have been selected based on a combination of the information you have provided above and secure defaults. You should not need to change these values unless you have specific requirements. Key Name: elk.mydomain.com Subject DN: CN=elk, DC=mydomain, DC=org Key Size: 2048 Do you wish to change any of these options? [y/N]n ## What password do you want for your private key(s)? Your private key(s) will be stored in a PKCS#12 keystore file named \u0026#34;http.p12\u0026#34;. This type of keystore is always password protected, but it is possible to use a blank password. If you wish to use a blank password, simply press \u0026lt;enter\u0026gt; at the prompt below. Provide a password for the \u0026#34;http.p12\u0026#34; file: [\u0026lt;ENTER\u0026gt; for none] ## Where should we save the generated files? A number of files will be generated including your private key(s), public certificate(s), and sample configuration options for Elastic Stack products. These files will be included in a single zip archive. What filename should be used for the output zip file? [/usr/share/elasticsearch/elasticsearch-ssl-http.zip] ./elasticsearch-certutil http 6.38s user 0.47s system 6% cpu 1:38.49 total The zip file /usr/share/elasticsearch/elasticsearch-ssl-http.zip contains below files\n$ unzip elasticsearch-ssl-http.zip $ tree . . ├── ca ├── README.txt └── ca.p12 ├── elasticsearch ├── README.txt ├── http.p12 └── sample-elasticsearch.yml └── kibana ├── README.txt ├── elasticsearch-ca.pem └── sample-kibana.yml Create new directory /etc/elasticsearch/http_ssl and copy http.p12 to all server\nAdd below config to /etc/elasticsearchelasticsearch.yml to enable http ssl like below\nxpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: \u0026#34;/etc/elasticsearch/http_ssl/http.p12\u0026#34; HTTPS configurations for HTTP clients For Kibana \u0026#x2757; Refer Kibana Installation to know more\nCreate /etc/kibana/ca and copy elasticsearch-ca.pem to kibana server\nAdd below config to kibana.yml to enable HTTPS communication between elasticsearch and kibana\nelasticsearch.ssl.certificateAuthorities: [ \u0026#34;/etc/kibana/ca/elasticsearch-ca.pem\u0026#34; ] For Beats Refer elasticsearch-deploy-notes/beats for demo config\nUse same .pem file to enable HTTPS communication between beats and elasticsearch(sample snippet below) output.elasticsearch: hosts: [\u0026#39;https://elk-staging.mydomain.com:9201\u0026#39;] username: admin password: 123123123 ssl: certificate_authorities: [\u0026#34;/etc/elasticsearch-ca.pem\u0026#34;] setup.dashboards: enabled: true setup.kibana: host: \u0026#34;https://elk-staging.mydomain.com:5691\u0026#34; ssl: certificate_authorities: [\u0026#34;/etc/letsencryptauthorityx3.pem\u0026#34;] Alerting The official Elasticsearch \u0026ldquo;Basic\u0026rdquo; version doesn\u0026rsquo;t include alerting. Below are the 2 opensource plugin available for elasticsearch\n1. Opendistro Elasticsearch Alerting \u0026#x26a0;\u0026#xfe0f; Opendistro Alerting IS NOT compatible with X-Pack Security, for more information refer this issue. If the x-pack is already enabled, DO NOT install this plugin, won\u0026rsquo;t work!. In order to install this plugin, disable x-pack security and install opendistro security\nPick Opendistro alerting standalone plugin version compatibility here\nLogin into master node and install necessary plugins and alerting plugin like below ## Go elasticseatch bin directory $ cd /usr/share/elasticsearch/ ## Install Job Scheduler plugin $ sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-job-scheduler/opendistro-job-scheduler-1.8.0.0.zip ## Install Alerting plugin $ sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-alerting/opendistro_alerting-1.8.0.0.zip ## Restart Elasticsearch $ sudo systemctl restart elasticsearch 2. Elastalert Refer elastalert docs for installation instructions.\nStack Monitoring Offical Docs Kibana Docs Elasticsearch can monitor itself, enable stack monitoring using REST API like below\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;xpack.monitoring.collection.enabled\u0026#34;: true } } We can also monitor with metricbeat, this method is useful especially want to monitor multiple clusters. Please refer metricbeat elasticsearch-xpack module config\nElatic Beats Beats is a free and open platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash or Elasticsearch.\nRefer elasticsearch-deploy-notes/beats for example config\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/install/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f449; This writeup is part of \u003ca href=\"https://veerendra2.github.io/posts/elasticsearch-deploy/\" title=\"previous post\"\u003e\u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo;\u003c/a\u003e series\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"install\"\u003eInstall\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html\"\u003eOffice Docs\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"hardware-requirement\"\u003eHardware Requirement\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html\"\u003eOffical Docs\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eResource\u003c/th\u003e\n          \u003cth\u003eMinimum\u003c/th\u003e\n          \u003cth\u003eRecommended\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMemory\u003c/td\u003e\n          \u003ctd\u003e16 GB\u003c/td\u003e\n          \u003ctd\u003e64 GB\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCPU\u003c/td\u003e\n          \u003ctd\u003e8 Cores\u003c/td\u003e\n          \u003ctd\u003e16\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDisk\u003c/td\u003e\n          \u003ctd\u003eDepends\u003c/td\u003e\n          \u003ctd\u003eDepends\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"jdk-installation\"\u003eJDK Installation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePick JVM compatibility version with elasticsearch from \u003ca href=\"https://www.elastic.co/support/matrix#matrix_jvm\"\u003ehere\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eInstall OpenJDK from \u003ca href=\"https://openjdk.java.net/install/index.html\"\u003ehere\u003c/a\u003e\nDownload and install JDK 11 (Another guide \u003ca href=\"https://jdk.java.net/java-se-ri/11\"\u003ehere\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ apt-get install openjdk-11-jdk -y\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ java -version\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eopenjdk version \u0026#34;11.0.6\u0026#34; 2020-01-14\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1)\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eOpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"elasticsearch-installation\"\u003eElasticsearch Installation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDownload latest elasticsearch from \u003ca href=\"https://www.elastic.co/downloads/elasticsearch\"\u003ehere\u003c/a\u003e (As of today the latest version is \u003ccode\u003e7.6.2\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eRecommended to download/install package via \u003ccode\u003e.dep\u003c/code\u003e or \u003ccode\u003ePPA\u003c/code\u003e which postscripts creates user, groups and adds under systemd\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eInstall via \u003ccode\u003eapt-get\u003c/code\u003e from \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.6/deb.html#deb-repo\"\u003ehere\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e","title":"Elasticsearch Installation"},{"content":" \u0026#x1f449; This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nRolling Upgrade Elasticsearch \u0026#x1f4c4; Official Docs\n\u0026#x26a0;\u0026#xfe0f; A rolling upgrade allows an Elasticsearch cluster to be upgraded one node at a time so upgrading does not interrupt service\nAs of now, the current latest version of elasticsearch is v7.7.1. Below procedure is for rolling upgrade from 7.6.2=\u0026gt;7.7.1.\n1. Divide the cluster into 2 groups Example node names carbon-x\na. Non master-eligible nodes carbon-2 carbon-3 b. Master-eligible nodes carbon-1 Upgrade order (Important!)\nNon master eligible nodes Master eligible nodes NOTE: Newer nodes can always join a cluster with an older master, BUT older nodes cannot always join a cluster with a newer master\n2. Prepare for upgrade Below points taken from official docs which are recomended\nCheck the deprecation log to see if you are using any deprecated features and update your code accordingly.(While checking this step, I found some histogram deprecation logs. But I performed upgrade anyways!) Review the breaking changes and make any necessary changes to your code and configuration for version 7.7.1. If you use any plugins, make sure there is a version of each plugin that is compatible with Elasticsearch version 7.7.1. Test the upgrade in an isolated environment before upgrading your production cluster. Back up your data by taking a snapshot 3. Disable shard allocation When the node is down, elasticsearch try to replicate the shards on that node to other nodes in the cluster which involves lot of I/O. Since the node will be down for short time, it is recommended disable allocation with below settings\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: \u0026#34;primaries\u0026#34; } } 4. Upgrade Elasticsearch NOTE: Follow the upgrade order like mentioned above. *Perform below operation on each node by node and group by group.\nSince in our cluster, the elasticsearch package installed from APT repository, we can upgrade package from APT repository like below\n$ sudo systemctl stop elasticsearch.service $ sudo apt-get update $ sudo apt-get install --only-upgrade elasticsearch $ sudo systemctl start elasticsearch $ sudo systemctl status elasticsearch Check logs if it required\nBefore upgrading to next node, wait for the cluster to finish shard allocation.You can check progress by below API(Important!)\nGET _cat/health?v NOTE: In case, the ip routes are disappeared which happened in one node while install package, follow below steps Take routing table reference from other nodes and add routes accordingly.\nip link set eth1 down ip addr add 10.29.103.10/24 dev eth1 ip route add 10.48.0.0/16 via 10.29.103.1 ip link set eth1 up 5. Upgrade any plugins *This step was not performed, since there were no plugins installed\nUse the elasticsearch-plugin script to install the upgraded version of each installed Elasticsearch plugi\n6. Re-enable shard allocation Verify all nodes are joined in the cluster\nGET _cat/nodes Reenable shard allocation\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: null } } Once all nodes are upgraded, verify the version and health\nGET /_cat/health?v GET /_cat/nodes?h=ip,name,version\u0026amp;v \u0026#x2757; Once cluster is upgraded and all nodes joined, it will take some for shard reallocation.\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/upgrade/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f449; This writeup is part of \u003ca href=\"https://veerendra2.github.io/posts/elasticsearch-deploy/\" title=\"previous post\"\u003e\u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo;\u003c/a\u003e series\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"rolling-upgrade-elasticsearch\"\u003eRolling Upgrade Elasticsearch\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html\"\u003eOfficial Docs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026#x26a0;\u0026#xfe0f; A rolling upgrade allows an Elasticsearch cluster to be upgraded one node at a time so upgrading does not interrupt service\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAs of now, the current latest version of elasticsearch is \u003ccode\u003ev7.7.1\u003c/code\u003e. Below procedure is for rolling upgrade from \u003ccode\u003e7.6.2\u003c/code\u003e=\u0026gt;\u003ccode\u003e7.7.1\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id=\"1-divide-the-cluster-into-2-groups\"\u003e1. Divide the cluster into 2 groups\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eExample node names \u003ccode\u003ecarbon-x\u003c/code\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003ea. Non master-eligible nodes\n\u003cul\u003e\n\u003cli\u003ecarbon-2\u003c/li\u003e\n\u003cli\u003ecarbon-3\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eb. Master-eligible nodes\n\u003cul\u003e\n\u003cli\u003ecarbon-1\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUpgrade order (Important!)\u003c/p\u003e","title":"Elasticsearch Upgrade"},{"content":" \u0026#x1f449; This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nKibana Installation \u0026#x1f4c4; Office docs\n\u0026#x2757; The elasticsearch should be up and running before you start kibana installation procedure\nInstall via apt-get from here\nAs of today the kibana version is 7.6.2 $ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - $ sudo apt-get install apt-transport-https $ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install kibana Configure Kibana Refer kibana.yml configuration file in current directory and modify accordingly\nSpecify kibana credentials in kibana.yml which were generated while configuration elasticsearch security Configure Security \u0026#x1f4c4; Official doc\nHTTPS Configuration with Let\u0026rsquo;s Encrypt certificates\nDownload certbot and generate certificate $ wget https://dl.eff.org/certbot-auto $ chmod 755 certbot-auto $ ./certbot-auto certonly Copy certificates to kibana config directory and change permission $ mkdir /etc/kibana/ssl $ cp -pr /etc/letsencrypt/archive/data.example.com /etc/kibana/ssl/ $ chmod 750 /etc/kibana/ssl/data.example.com $ chmod 640 /etc/kibana/ssl/data.example.com/* $ chown -R root:kibana /etc/kibana/ssl/data.example.com Add below config to kibana.yml to enable HTTPS\nserver.ssl.enabled: true server.ssl.certificate: /etc/kibana/ssl/elk-staging.yourdomain.com/cert1.pem server.ssl.key: /etc/kibana/ssl/elk-staging.yourdomain.com/privkey1.pem SSL Config for Kibana to Communicate Elasticsearch\nCopy elasticsearch-ca.pem(This the pem file that you were generated while configuring elasticsearch security) to /etc/kibana/ca/ $ mkdir /etc/kibana/ca/ $ ls /etc/kibana/ca/elasticsearch-ca.pem Add below config to kibana.yml\nelasticsearch.ssl.certificateAuthorities: [ \u0026#34;/etc/kibana/ca/elasticsearch-ca.pem\u0026#34; ] Upgrade Kibana \u0026#x1f4c4; Official docs\nAs of now, the current latest version of kibana is v7.7.1. Below is the procedure for upgrade from 7.6.2=\u0026gt;7.7.1.\nUpgrade $ sudo systemctl stop kibana $ sudo apt-get install --upgrade-only kibana $ sudo systemctl start kibana \u0026#x2757; Kibana install will take some, especially while \u0026ldquo;Unpacking\u0026rdquo; package. So, please be patient\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/kabana-install/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f449; This writeup is part of \u003ca href=\"https://veerendra2.github.io/posts/elasticsearch-deploy/\" title=\"previous post\"\u003e\u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo;\u003c/a\u003e series\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"kibana-installation\"\u003eKibana Installation\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html\"\u003eOffice docs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026#x2757; The elasticsearch should be up and running before you start kibana installation procedure\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInstall via \u003ccode\u003eapt-get\u003c/code\u003e from \u003ca href=\"https://www.elastic.co/downloads/kibana\"\u003ehere\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAs of today the kibana version is \u003ccode\u003e7.6.2\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo apt-get install apt-transport-https\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install kibana\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"configure-kibana\"\u003eConfigure Kibana\u003c/h2\u003e\n\u003cp\u003eRefer \u003ccode\u003ekibana.yml\u003c/code\u003e configuration file in current directory and modify accordingly\u003c/p\u003e","title":"Kibana Installation"},{"content":" \u0026#x1f449; This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nElastalert \u0026#x1f4c4; Official docs \u0026#x1f4c2; Official project Repo\nElastalert is developed by Yelp written in python, queries docs in elasticsearch and send alerts depends on the rules.\nSince Elastalert is not part of Elasticsearch plugin, we can install it where ever we want to.\nInstallation \u0026#x2757; Refer elasticsearch-deploy-notes/elastalert for example config\n$ sudo apt-get install python3-pip $ sudo pip3 install elastalert $ sudo pip3 install -U PyYAML $ mkdir -p /opt/elastalert/rules ## Copy alert rules yaml files and config file to /opt/elastalert and /opt/elastalert/rules accordingly from this repo Recommended to create index in elasticsearch for elastalert to store metadata $ elastalert-create-index Elastic Version: 7.7.0 Reading Elastic 6 index mappings: Reading index mapping \u0026#39;es_mappings/6/silence.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert_status.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/past_elastalert.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert_error.json\u0026#39; New index elastalert_status created Done! Test rules in case if it is needed $ elastalert-test-rule --config /opt/elastalert/config.yaml /opt/elastalert/rules/heartbeat_checks.yml Postfix Gmail SMTP In oder to use Gmail as SMTP, you need to enable 2-Factor authentication and generate app password\nFirst configure 2-Factor Authentication at google accounts security Generate app passwords at app password generation Click Select app and choose Other (custom name) from the dropdown. Enter “Postfix” and click Generate. Copy App Password Install Postfix package sudo apt-get -y install postfix mailutils libsasl2-2 ca-certificates libsasl2-modules While postfix package install, it will prompt for configuration, default options/configs are sufficient i.e don\u0026rsquo;t have to change anything\nAdd below config in /etc/postfix/main.cf\nrelayhost = [smtp.gmail.com]:587 smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_sasl_security_options = noanonymous smtp_tls_CApath = /etc/ssl/certs smtpd_tls_CApath = /etc/ssl/certs smtp_use_tls = yes Add email and app password in /etc/postfix/sasl_passwd [smtp.gmail.com]:587 xxxxx@yourdomain.de:xxx xxxxx xxxxx xxxxx Start daemon sudo chmod 400 /etc/postfix/sasl_passwd sudo postmap /etc/postfix/sasl_passwd sudo systemctl restart postfix Test it echo \u0026#34;Testing\u0026#34; | mail -s \u0026#34;Test Email\u0026#34; soap@gmail.com sudo postqueue -p Daemonize the Elastalert We will use supervisord to run ElastAlert.\nInstall supervisord $ sudo pip3 install supervisord Review supervisor config and copy $ ls /etc/supervisordconf /etc/supervisord.conf Enable and start the deamon root@s-root-odoo03 ~ # supervisorctl supervisor\u0026gt; add elastalert supervisor\u0026gt; start elastalert supervisor\u0026gt; exit ","permalink":"https://veerendra2.github.io/elasticsearch-deploy/elastalert-demo/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f449; This writeup is part of \u003ca href=\"https://veerendra2.github.io/posts/elasticsearch-deploy/\" title=\"previous post\"\u003e\u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo;\u003c/a\u003e series\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"elastalert\"\u003eElastalert\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x1f4c4; \u003ca href=\"https://github.com/Yelp/elastalert\"\u003eOfficial docs\u003c/a\u003e \u0026#x1f4c2; \u003ca href=\"https://github.com/Yelp/elastalert\"\u003eOfficial project Repo\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eElastalert is developed by Yelp written in python, queries docs in elasticsearch and send alerts depends on the rules.\u003c/p\u003e\n\u003cp\u003eSince Elastalert is not part of Elasticsearch plugin, we can install it where ever we want to.\u003c/p\u003e\n\u003ch2 id=\"installation\"\u003eInstallation\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026#x2757; Refer \u003ca href=\"https://github.com/veerendra2/elasticsearch-deploy-notes/tree/main/elastalert\"\u003eelasticsearch-deploy-notes/elastalert\u003c/a\u003e for example config\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo apt-get install python3-pip\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo pip3 install elastalert\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo pip3 install -U PyYAML\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ mkdir -p /opt/elastalert/rules\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e## Copy alert rules yaml files and config file to /opt/elastalert and /opt/elastalert/rules accordingly from this repo\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eRecommended to create index in elasticsearch for elastalert to store metadata\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ elastalert-create-index\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eElastic Version: 7.7.0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading Elastic 6 index mappings:\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading index mapping \u0026#39;es_mappings/6/silence.json\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading index mapping \u0026#39;es_mappings/6/elastalert_status.json\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading index mapping \u0026#39;es_mappings/6/elastalert.json\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading index mapping \u0026#39;es_mappings/6/past_elastalert.json\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eReading index mapping \u0026#39;es_mappings/6/elastalert_error.json\u0026#39;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eNew index elastalert_status created\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eDone!\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eTest rules in case if it is needed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ elastalert-test-rule --config /opt/elastalert/config.yaml /opt/elastalert/rules/heartbeat_checks.yml\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"postfix-gmail-smtp\"\u003ePostfix Gmail SMTP\u003c/h3\u003e\n\u003cp\u003eIn oder to use Gmail as SMTP, you need to enable 2-Factor authentication and generate app password\u003c/p\u003e","title":"Elastalert Demo Config"},{"content":"Introduction Hello guys, today I came up with an interesting write-up, that is how to set up backup and restore with Velero on Kubernetes. A year back I worked on Strimzi Kafka, a deployment solution for deploying production-level Kafka on Kubernetes. Strimzi Kafka uses persistance volume(PV) as a disk which is a managed disk from a cloud provider(e.g. Azure, AWS, etc), but I couldn\u0026rsquo;t find a proper backup solution in order to configure PV backup and restore. Sure, you can configure these managed disk backups from Terraform or manually in cloud provider portals. But tools like Velero, backup PV from kubernetes side which is more visible and easy to manage which is what you will see in a moment.\n\u0026#x2757; Velero supports multiple cloud provider, this write-up covers only deployment of Velero on kubernetes with storage class provisioner as AzureDisk\n\u0026#x1f449; All deployment files are in my repo https://github.com/veerendra2/velero-demo\nVelero https://velero.io\nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes\nDisaster Recovery → Reduces time to recovery in case of infrastructure loss, data corruption, and or service outage Data Migration → Enables cluster portability by easily migrating Kubernetes resources from one cluster to another. Data Protection → Offers key data protection features such as scheduled backups, retention schedules, and pre or post-backup hooks for custom actions. Features Backup Clusters Backup your Kubernetes resources and volumes for an entire cluster, or part of a cluster by using namespaces or label selectors. Schedule Backups Set schedules to automatically kickoff backups at recurring intervals. Backup Hooks Configure pre and post-backup hooks to perform custom operations before and after Velero backups. How it works ┌ │ │ │ │ │ │ │ │ │ └ ─ ─ ─ ─ ─ ┌ │ │ │ │ └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┬ │ │ └ ─ ─ ─ V ─ a ─ ─ ─ ─ e ─ z ─ K ─ ─ ─ l ─ m u ─ u ─ ─ ─ e ─ i r ─ b ─ ─ ─ r ─ c e ─ e ─ ─ ─ o ─ r ─ r ─ ─ ─ ─ o p ─ n ─ ─ ─ S ─ s l ─ e ─ ─ ─ e ─ o u ─ t ─ ─ ─ r ─ f g ─ e ─ ─ ─ v ─ t i ─ s ─ ─ ─ e ─ n ─ ─ ─ ─ r ─ ─ ─ ─ ─ ┬ │ │ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ ├ │ ├ ┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┼ │ ┼ │ | │ │ │ ┘ m ─ ─ e ─ ─ P b t ─ ─ e a a ─ ─ r c a d ─ ─ s k p a ─ ─ i u p t ─ ─ s p s a ─ ─ t ─ ─ a m d b ─ ─ n a e a ─ ─ c n p c ─ ─ e a l k ─ ─ - g o u ─ ─ v e y p ─ ─ o m m ─ ─ l e e m ─ ─ u n n a ─ ─ m t t n ─ ─ e a ─ ┐ │ │ └ g ─ ─ e ─ ─ m ─ ─ e ─ ─ n ─ ─ t ► ► ┌ │ │ └ ┌ │ | └ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ M ─ ─ ─ ─ a ─ ─ A ─ ─ n ─ ─ z ─ ─ A a ─ ─ u S ─ ─ z g ─ ─ r t ─ ─ u e ─ ─ e o ─ ─ r d ─ ─ r ─ ─ e ─ ─ B a ─ ─ D ─ ─ l g ─ ─ i ─ ─ o e ─ ─ s ─ ─ b ─ ─ k ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐ │ │ ┘ ┐ │ │ ┘ Velero uses service principle to access cloud resources for managing backups of deployments on K8s. It supports various plugins for the cloud, while deploying Velero we have to specify the plugin and provide credentials(/base/deployment.yaml#L67). By using Velero, we can backup deployments, PVs, namespaces and entire clusters.\nFor deployment related backups, it uses storage account to store all deployments metadata For PV backup, it uses a snapshot from the cloud provider. \u0026#x26a0;\u0026#xfe0f; Backups may fail sometimes if it doesn\u0026rsquo;t have necessary memory(#3234)\nThe below diagram gives a bird\u0026rsquo;s eye view of how Velero works Deploy Velero provides binary to manage velero server deployment and scheduled backups. But, in our deployment, we do something different, that is; use kustomize and manage in git repo. But first we need to download velero binary from release page here\n$ curl -O https://github.com/vmware-tanzu/velero/releases/download/v1.9.1/velero-v1.9.1-linux-amd64.tar.gz $ tar -xf velero-v1.9.1-linux-amd64.tar.gz $ cd velero-v1.9.1-linux-amd64/ $ ls examples LICENSE velero $ sudo mv velero /usr/local/bin/ Prerequisites Before deploying Velero server on kubernetes, we need to customize the velero server according to our needs. Below is the configuration for Velero to manage Azure resources.\nBelow are the variables that will be used during installation\nEnvironmental Variable Name Description SUBSCRIPTION_ID Subscription ID TENANT_ID Tenant ID of service principle CLIENT_ID Client ID of service principle CLIENT_SECRET Service principal secret RESOURCE_GROUP Source resource groups where K8s PVs(Azure Managed Disks) are present to backup BACKUP_RESOURCE_GROUP Destination resource group where PV(Azure Managed Disks)snapshots are stored STORAGE_ACCOUNT_ID Storage account name BLOB_CONTAINER Container name in storage account where K8s deployment metadata stored BACKUP_SUBSCRIPTION_ID Subscription ID where backs are stored(In our case, this value SUBSCRIPTION_ID) STORAGE_ACCOUNT_ACCESS_KEY Storage account access key CLOUD_NAME Cloud name, the value is AzurePublicCloud Create service principal $ az account list --query \u0026#34;[].{name:name, id:id}\u0026#34; --output tsv $ export SUBSCRIPTION_ID=\u0026#34;[SUBSCRIPTION_ID_HERE]\u0026#34; $ az login $ CLIENT_SECRET=`az ad sp create-for-rbac --name \u0026#34;velero-sp\u0026#34; --role\u0026#34;Contributor\u0026#34; --query \u0026#39;password\u0026#39; -o tsv --scopes subscriptions/$SUBSCRIPTION_ID` $ CLIENT_ID=`az ad sp list --display-name \u0026#34;velero-sp\u0026#34; --query\u0026#39;[0].appId\u0026#39; -o tsv` $ TENANT_ID=`az ad sp list --display-name \u0026#34;velero-sp\u0026#34; --query\u0026#34;[].appOwnerTenantId\u0026#34; -o tsv Create Azure storage account $ az storage account create \\ --name $STORAGE_ACCOUNT_ID \\ --resource-group $RESOURCE_GROUP \\ --location \u0026lt;location\u0026gt; \\ --sku Standard_ZRS \\ --encryption-services blob $ az storage container create \\ --account-name STORAGE_ACCOUNT_ID \\ --name $BLOB_CONTAINER \\ --auth-mode login Set variables Below are dummy values of variables.\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; ./credentials-velero SUBSCRIPTION_ID=\u0026#34;my-sub-id\u0026#34; TENANT_ID=\u0026#34;my-tnt-id\u0026#34; CLIENT_ID=\u0026#34;my-client-id\u0026#34; CLIENT_SECRET=\u0026#34;secure-secret\u0026#34; RESOURCE_GROUP=\u0026#34;myresgrp\u0026#34; CLOUD_NAME=AzurePublicCloud EOF $ export BACKUP_RESOURCE_GROUP=myresgrp $ export STORAGE_ACCOUNT_ID=myaccount $ export BACKUP_SUBSCRIPTION_ID=my-sub-id $ export BLOB_CONTAINER=backup Generate Deployment Files Once variables are set and verified, generate velero deployment files using velero cli\n\u0026#x2757; You can also get the velero deployment here\n$ velero install \\ --provider azure \\ --plugins velero/velero-plugin-for-microsoft-azure:v1.1.0 \\ --bucket $BLOB_CONTAINER \\ --secret-file ./credentials-velero \\ --backup-location-config resourceGroup=$BACKUP_RESOURCE_GROUP,storageAccount=$STORAGE_ACCOUNT_ID,subscriptionId=$BACKUP_SUBSCRIPTION_ID \\ --snapshot-location-config apiTimeout=5m,resourceGroup=$BACKUP_RESOURCE_GROUP,subscriptionId=$BACKUP _SUBSCRIPTION_ID \\ --use-volume-snapshots=true --dry-run -o yaml ... Once you run the above command, it displays deployment files on stdout, because of the --dry-run -o yaml option. Copy the content to separate yaml files like below.\n\u0026#x1f449; You can check here how I seperated https://github.com/veerendra2/velero-demo/tree/main/base\n## https://github.com/veerendra2/velero-demo/tree/main/base $ tree . . ├── cluster-role-binding.yaml ├── deployment.yaml └── velero-crds.yaml 0 directories, 3 files Once all files are arranged(and verify variables in below deployment files that we configured in above section), login into kubernetes and run the deployment one-by-one\n$ kubectl create -f velero-crds.yaml $ kubectl create -f cluster-role-binding.yaml $ kubectl create -f deployment.yaml Kubernetes deploys the velero server in the velero namespace as a pod. Verify velero server is installed with kubectl get pods -n velero\nConfigure Backups We can configure backups with velero cli\n\u0026#x2757; A handy tool to write cronjob -\u0026gt; https://crontab.guru/\n$ velero schedule create kafka-backup-schedule \\ --schedule=\u0026#34;@every 168h\u0026#34; --ttl 2160h0m0s \\ --include-namespaces=kafka $ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; The above example, backups all resources in namespace kafka(including PVs and K8s deployment files) for every 168 hours. But we want setup every thing in yaml files so that we store in git repo(e.g. /overlay/dev). So, we can ask velero cli to display yaml like below\n$ velero schedule create kafka-backup-schedule --schedule=\u0026#34;@every 168h\u0026#34; --ttl 2160h0m0s--include-namespaces=kafka -o yaml apiVersion: velero.io/v1 kind: Schedule metadata: creationTimestamp: null name: kafka-backup-schedule namespace: kafka spec: schedule: \u0026#39;@every 168h\u0026#39; template: hooks: {} includeClusterResources: true includedNamespaces: - \u0026#39;*\u0026#39; ttl: 2160h0m0s status: {} You can also look for help like below\n$ velero schedule create --help The --schedule flag is required, in cron notation, using UTC time: | Character Position | Character Period | Acceptable Values | | ------------------ | :--------------: | ----------------: | | 1 | Minute | 0-59,* | | 2 | Hour | 0-23,* | | 3 | Day of Month | 1-31,* | | 4 | Month | 1-12,* | | 5 | Day of Week | 0-7,* | The schedule can also be expressed using \u0026#34;@every \u0026lt;duration\u0026gt;\u0026#34; syntax. The duration can be specified using a combination of seconds (s), minutes (m), and hours (h), for example: \u0026#34;@every 2h30m\u0026#34;. Usage: velero schedule create NAME --schedule [flags] We can get backup status if the schedule that we configured above.\n$ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup-schedule-20201121202523 Completed 2020-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; kafka-backup-schedule-20201121201523 Completed 2020-11-21 21:15:23 +0100 STD 88d default \u0026lt;none\u0026gt; \u0026#x2757; schedule consists of multiple backup as you can see above\nOnce everything is setup we can do magic with kustomize. So, below is finally show off with kustomize. This kustomize setup is very useful when you configure with GitOps tools like Argo CD.\n\u0026#x1f449; You can find this demo deployment files in my repo here https://github.com/veerendra2/velero-demo\n$ tree . . ├── base │ ├── cluster-role-binding.yaml │ ├── deployment.yaml │ ├── kustomization.yaml │ └── velero-crds.yaml ├── overlay │ └── dev │ ├── backup-locations.yaml │ ├── backup-schedules │ │ └── pvc.yaml │ ├── kustomization.yaml │ └── secrets │ └── cloud-credentials.yaml └── README.md 5 directories, 9 files Terminology Before concluding this write up, we need to get familiar with 2 terminologies that helps us to understand how Velero works internally\nBackup Storage Location The BackupStorageLocation holds info about storage account, container in storage account. We can have multiple \u0026ldquo;Backup Storage Location\u0026rdquo; with difference containers and storage account(e.g. dev/backup-locations.yaml#L2-L15)\n## Since there velero CRDs are installed, we can query like below $ kubectl get backupstoragelocations -n velero NAME AGE default 1d kafka 1d Volume Snapshot Location The VolumeStorageLocation holds info about resource group where snapshots are stored(e.g. dev/backup-locations.yaml#L17-L21)\n## Since there velero CRDs are installed, we can query like below $ kubectl get volumesnapshotlocations -n velero NAME AGE default 1d Restore What good is the backup if you can\u0026rsquo;t restore?!\u0026hellip;\nGet schedule and backup like below\n## Login into kubernetes with kubectl cli $ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup-schedule-20201121202523 Completed 2020-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Create restore\n$ velero restore create --from-backup kafka-backup-schedule-20201121202523 Restore request \u0026#34;kafka-backup-schedule-20201121202523-20201123033701\u0026#34; submitted successfully. Run `velero restore describe kafka-backup-schedule-20201121202523-20201123033701` or `velero restore logs kafka-backup-schedule-20201121202523-20201123033701` for more details. We can also describe to know the status\n$ velero restore describe kafka-backup-schedule-20201121202523-20201123033701 Name: kafka-backup-schedule-20201121202523-20201123033701 Namespace: velero Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Phase: InProgress \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----STATUS-HERE Backup: kafka-backup-schedule-20201121202523 Namespaces: Included: all namespaces found in the backup Excluded: \u0026lt;none\u0026gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io Cluster-scoped: auto Namespace mappings: \u0026lt;none\u0026gt; Label selector: \u0026lt;none\u0026gt; Restore PVs: auto Once restore is completed, verify has it been restored\n$ kubectl get pods -n kafka ... Conclusion In this write up, I have covered\nSneak peek on Velero How Velero works How to deploy and configure to manage resources on Azure. Configure scheduled backups Restore BONUS; I have collected handfull of velero cli example in my Github gist here Stay tuned, my next write-up will be published very soon which is related to backup and restore on Kubernetes \u0026#x1f603;\n","permalink":"https://veerendra2.github.io/velero-deployment/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eHello guys, today I came up with an interesting write-up, that is how to set up backup and restore with Velero on Kubernetes. A year back I worked on \u003ca href=\"https://strimzi.io/\"\u003eStrimzi Kafka\u003c/a\u003e, a deployment solution for deploying production-level Kafka on Kubernetes. Strimzi Kafka uses \u003ca href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\"\u003epersistance volume(PV)\u003c/a\u003e as a disk which is a managed disk from a cloud provider(e.g. Azure, AWS, etc), but I couldn\u0026rsquo;t find a proper backup solution in order to configure PV backup and restore. Sure, you can configure these managed disk backups from Terraform or manually in cloud provider portals. But tools like Velero, backup PV from kubernetes side which is more visible and easy to manage which is what you will see in a moment.\u003c/p\u003e","title":"Velero Deployment with Kustomize (Azure)"},{"content":"It has been 2 years since I wrote a new post. Due to busy work, moving to a new city, new jobs and getting married, I wasn’t able to keep up with writing blog posts. Finally, I’m back now, I have been thinking of changing blog themes for a long time. I spent some time exploring Jekyll themes and tried to modify them according to my requirements. As you can see here GitHub issue.\nHugo A month ago I discovered Hugo, a static website builder like Jekyll, so I decided to take a look at it. And thus I decided to move to Hugo, because of its speed, simple and single binary\nJust like Hugo, I discovered Jekyll a few years back, as you can see from my old blog post. After using Jekyll for a while I see below annoying things while building and installing Jekyll every time I change the OS in my laptop. #7\nJekyll is relatively slow I get confused with gems versions and their dependencies while picking new themes Screw up my system ruby packages with other packages like Vagrant etc. because of that I had to use docker containers and these some permissions issues, etc. So, when I see Hugo, it solved my above problems\nIt is faster like they say on their website To install a new theme, I can use the git module or simply drop the theme in themes directly and configure Single binary! No BS! Simple directory structure Now my blog is powered by Hugo \u0026#x1f389;\nOther updates I removed the disqus tool for commenting and moved to GitHub discussion with tool https://giscus.app From now onwards I will try to keep up writing blogs regularly. I’m planning some “nugget” blogs like small posts containing info from books I’m reading or maybe some of my troubleshooting on tools, etc. \u0026#x1f91e; Here is my Hugo site Github Actions workflow https://gist.github.com/veerendra2/28d35b44c8340d5a801e5606edb32f45\n","permalink":"https://veerendra2.github.io/moving-to-hugo/","summary":"\u003cp\u003eIt has been 2 years since I wrote a new post. Due to busy work,  moving to a new city, new jobs and getting married, I wasn’t able to keep up with writing blog posts. Finally, I’m back now, I have been thinking of changing blog themes for a long time. I spent some time exploring Jekyll themes and tried to modify them according to my requirements. As you can see \u003ca href=\"https://github.com/veerendra2/veerendra2.github.io/issues/6\"\u003ehere\u003c/a\u003e GitHub issue.\u003c/p\u003e","title":"Moving to Hugo and other updates!"},{"content":"Looks like my blog posts are like Sherlock TV Show episodes, posting once in a while\u0026hellip;anyways I\u0026rsquo;m back now. As you might know, GitHub recently launched GitHub Actions where people can automate workflows like build, test, and deploy code from GitHub.\nI have started reading docs a little bit, I have to admit, setting up workflows on GitHub Actions is not that hard. I can directly start creating a workflow from available workflow templates in the \u0026ldquo;Actions\u0026rdquo; tab. Then I saw the workflow template for \u0026ldquo;Jekyll Site CI/CD\u0026rdquo;, which builds the Jekyll site. I got some feeling that I automate my workflow of building and deploying my blog (which currently you are reading) on GitHub Pages. So, Let\u0026rsquo;s see how I setup CI/CD\nBut first, let me explain how my blog site works and then the manual steps every time I do to publish a blog.\nGitHub Pages This blog site is powered by GitHub Pages which you can publish static Html site right from the GitHub repository (For example my repo here). All you have to do is to create a repo name like \u0026lt;YourGitHubUsername\u0026gt;.github.io and drop static Html pages in the master branch. That\u0026rsquo;s it, GitHub does the magic, and serves your website.\nFor this, I\u0026rsquo;m maintaining 2 branches, one for my markdown source files in source branch and another for static Html site in master branch\nJekyll Everything\u0026rsquo;s good so far, but we know that we are too lazy to write Html pages. So, that\u0026rsquo;s where this tool \u0026ldquo;Jekyll\u0026rdquo; comes into the picture, which converts markdown files to static Html websites. Once you jekyll build, it will build a static website in the _site directory. For local testing, you can run jekyll serve to see how the site looks. (Check out my other post to know how to install jekyll)\nMy Manual Workflow Write a blog post in markdown files which I keep in the source branch (Obviously I can not automate this step \u0026#x1f61c;) Run jekyll build to build static website Copy static website content from _site to master branch Push master branch changes to the repo to publish the website Push source branch changes to the repo to store/version tracking GitHub Actions Below is the jekyll.yml to automate my workflow\nLet\u0026rsquo;s go through the jekyll.yml line by line very briefly\nLine No. Description 3 Event subscription; I want to trigger the workflow only when there is any push in the \u0026ldquo;source\u0026rdquo; branch 11 I want to run this build procedure on ubuntu box(GitHub action supports other box as well like windows, mac, etc) 16 In order to build, first I have to clone the repo, for this, there is ready made action called \u0026ldquo;actions/checkout@v2\u0026rdquo; which checkouts my repo with the \u0026ldquo;source\u0026rdquo; branch 21 Since I can\u0026rsquo;t expect GitHub servers to have \u0026ldquo;jekyll\u0026rdquo; installed, I built my own docker image for jekyll with dependency gems to build a website. (You can find other jekyll docker image, I\u0026rsquo;m using an older version of jekyll, that\u0026rsquo;s why I built my own) 28 I want to push static website to the \u0026ldquo;master\u0026rdquo; branch, so I have to check out \u0026ldquo;master\u0026rdquo; branch as well 33 Copy \u0026ldquo;_site\u0026rdquo; content to master branch 37 Normal shell commands, git add and git commit 44 Finally push changes to \u0026ldquo;master\u0026rdquo; to publish the website with help of using ready made action \u0026ldquo;ad-m/GitHub-push-action@master\u0026rdquo; Now, all I have to do is drop jekyll.yml in the .github/workflows/ directory to GitHub to pick up my workflow. Below is the picture showing the pipeline for my website deployment.\n","permalink":"https://veerendra2.github.io/ci-cd-github-pages-with-github-actions/","summary":"\u003cp\u003eLooks like my blog posts are like Sherlock TV Show episodes, posting once in a while\u0026hellip;anyways I\u0026rsquo;m back now. As you might know, GitHub recently launched \u003ca href=\"https://github.com/features/actions\"\u003eGitHub Actions\u003c/a\u003e where people can automate workflows like build, test, and deploy code from GitHub.\u003c/p\u003e\n\u003cp\u003eI have started reading docs a little bit, I have to admit, setting up workflows on GitHub Actions is not that hard. I can directly start creating a workflow from available workflow templates in the \u0026ldquo;Actions\u0026rdquo; tab. Then I saw the workflow template for \u0026ldquo;Jekyll Site CI/CD\u0026rdquo;, which builds the Jekyll site. I got some feeling that I automate my workflow of building and deploying my blog (which currently you are reading) on \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e. So, Let\u0026rsquo;s see how I setup CI/CD\u003c/p\u003e","title":"CI/CD for GitHub Pages with GitHub Actions"},{"content":"Hallo alle zusammen, after a long time I\u0026rsquo;m writing this blog and I come with an interesting and long post\nI know what you are thinking, I steal Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial, but hey!, I did some research and try to fit K8s cluster(Multi-Master!) in a laptop with Docker as \u0026lsquo;CRI\u0026rsquo; and Flannel as \u0026lsquo;CNI\u0026rsquo;.\nThis blog post follows Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way, I highly recommend go through his repo. I\u0026rsquo;m writing this blog post to keep it as a reference for me and share it with other people who want to try it. So, feel free to correct me if there are any mistakes and ping me for any queries. This series is divided into 3 parts and all configuration/scripts are available in my github repo. Well, that has been said, let\u0026rsquo;s start building the cluster.\nBelow is my laptop configuration. Make sure you have enough resources on your laptop. (or depends on resources, you can reduce nodes in the cluster, etc.)\nLaptop Acer Predator Helios 200 CPU Intel Core i5 8th Gen RAM 8 GB Host OS Ubuntu 18.04 Hostname ghost First let\u0026rsquo;s talk about the cluster in Kubernetes The Hard Way which has 3 controller nodes, 3 worker nodes and a load balancer on GCP. I want to deploy a cluster with multiple masters, but I was afraid it is too much for my laptop. So, I reduced it to 2 controller nodes, 2 worker nodes (or VMs in my case) and replaced the GCP load balancer with the nginx docker container as a load balancer, the clusters look like below.\n1. Prerequisites Installation of packages *NOTE: The following components will be installed on host machine(laptop)\nInstall KVM hypervisor. $ sudo apt-get install qemu-kvm quem-system \\ libvirt-bin bridge-utils \\ virt-manager -y Install Docker, because we want to run nginx load balancer container on host Install cfssl and cfssljson binaries $ wget -q --show-progress --https-only --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 $ chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 $ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl $ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson $ wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl $ chmod +x kubectl $ sudo mv kubectl /usr/local/bin/ Subnets In the official \u0026ldquo;Kubernetes The Hard Way\u0026rdquo;, cluster network configuration is done via gcloud and obviously we are not going to use it. We have to choose subnets manually for our cluster nodes, CIDR for pods and K8s services. So, here is what I come with\nNo. Name Subnet 1 Cluster Nodes 10.200.1.0/24 2 POD CIDR(cluster-cidr) 10.100.0.0/16 3 Service(service-cluster-ip) 10.32.0.0/24 Linux Bridge \u0026amp; NAT As you can see in the above diagram, we are going to use a linux bridge to connect our VMs and Nginx docker container. Also, we need to do NATing for our VMs in order to access the Internet.\n$ EXTERNAL_IFACE=\u0026#34;wlan0\u0026#34; ## Enable ip forwarding $ sudo sysctl net.ipv4.conf.all.forwarding=1 ## Create br0 bridge $ sudo ip link add name br0 type bridge $ sudo ip link set dev br0 up $ sudo ip addr add 10.200.1.1/24 dev br0 ## iptables NAT configuration $ sudo iptables -t nat -A POSTROUTING -o $EXTERNAL_IFACE -j MASQUERADE $ sudo iptables -A FORWARD -i $EXTERNAL_IFACE -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT $ sudo iptables -A FORWARD -i br0 -o $EXTERNAL_IFACE -j ACCEPT ## Bridge config. Read more @ https://tinyurl.com/yan5jnd4 $ sudo sysctl -w net.bridge.bridge-nf-call-arptables=0 $ sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=0 $ sudo sysctl -w net.bridge.bridge-nf-call-iptables=0 In order to launch docker container(nginx load balancer container) on different linux bridge(other than default docker0), we need to create a docker network and specify that network while launching the container. Below command creates docker network with br0 as bridge\n$ docker network create --driver=bridge \\ --ip-range=10.200.1.0/24 \\ --subnet=10.200.1.0/24 -o \u0026#34;com.docker.network.bridge.name=br0\u0026#34; br0 Create a workspace directory We can save all configuration and generate certificates in this directory\n$ mkdir ~/kubernetes-the-hard-way $ cd ~/kubernetes-the-hard-way 2. Provisioning Compute Resources Specify cluster info(hostname, IP and user to login) in controllers.txt and workers.txt files respectively like in below. In the same way, add those VM IPs in /etc/hosts file like below. These files are useful to automate things like copy files to nodes or generating certificates for these nodes, etc. You will see it in a moment.\n$ cd ~/kubernetes-the-hard-way $ cat controllers.txt m1 10.200.1.10 veeru m2 10.200.1.11 veeru $ cat workers.txt n2 10.200.1.13 veeru n2 10.200.1.14 veeru $ cat nginx_proxy.txt proxy 10.200.1.15 $ cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 ghost 10.200.1.10 m1 10.200.1.11 m2 10.200.1.12 n1 10.200.1.13 n2 10.200.1.15 nginx Below are the IPs, hostnames and usernames for the nodes that I choose\nNode Role Node Hostname Node IP Node Login Username Controller m1 10.200.1.10 veeru Controller m2 10.200.1.11 veeru Worker n1 10.200.1.13 veeru Worker n2 10.200.1.14 veeru Load Balancer(nginx Container) proxy 10.200.1.15 N/A Download Ubuntu 18.04 server .iso from https://www.ubuntu.com/ In the previous section, we installed the kvm hypervisor and now let\u0026rsquo;s spin up 4 VMs and specify the bridge name under the network section like in the below screenshot. (I used \u0026ldquo;Virtual Machine Manager\u0026rdquo; GUI to launch VMs) *I\u0026rsquo;m not covering OS installation in VM. You can easily find it on the Internet.\n*NOTE: While installing OS, please select static IP and specify IPs according to their node names\n*TIP: Install OS in VM and clone VM 3 times\nOnce the OS installation is completed, check the connectivity between the host-VM and VM-VM and you should be able to ssh both host-to-VM and VM-to-VM. For convenience, you can copy ssh keys, so that you don\u0026rsquo;t have to enter a password every time.\n$ ssh-keygen $ ssh-copy-id guest-username@guest-ip 3. Provisioning a CA and Generating TLS Certificates It is a good practice to set up encrypted communication between the components of K8s. In this section, we will create public key certificates and private keys for the below components using CloudFlare\u0026rsquo;s PKI toolkit as we downloaded earlier. (Know more about PKI)\nadmin user kubelet kube-controller-manager kube-proxy kube-scheduler kube-api But first, we have to create Certificate Authority(CA) which e-signatures the certificates that we are going to generate.\n$ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/ca-config.json $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/ca-csr.json ## Generate CA $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca-key.pem ca.pem Admin User Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/admin-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin $ ls admin-key.pem admin.pem Kubelet Client Certificates As docs says\nK8s uses node authorization which is a special-purpose authorization mode that specifically authorizes API requests made by kubelets\nIn order to be authorized by the Node Authorizer, Kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;. Let\u0026rsquo;s create a certificate and private key for each worker node (In my case n1 and n2)\n$ cd ~/kubernetes-the-hard-way IFS=$\u0026#39;\\n\u0026#39; for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` INTERNAL_IP=`echo $line | awk \u0026#39;{print $2}\u0026#39;` cat \u0026gt; ${instance}-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;system:node:${instance}\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;Westeros\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;The North\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:nodes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes The Hard Way\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;Winterfell\u0026#34; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${INTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done $ ls n1-key.pem n1.pem n2-key.pem n2.pem Controller Manager Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-controller-manager-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager $ ls kube-controller-manager-key.pem kube-controller-manager.pem Kube Proxy Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-proxy-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy $ ls kube-proxy-key.pem kube-proxy.pem Scheduler Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-scheduler-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler $ ls kube-scheduler-key.pem kube-scheduler.pem Kubernetes API Server Certificate kube-api server certificate\u0026rsquo;s hostname should include the following things\nAll controller\u0026rsquo;s hostname All controller\u0026rsquo;s IP Load balancer\u0026rsquo;s hostname Load balancer\u0026rsquo;s IP Kubernetes\u0026rsquo;s service(Both \u0026lsquo;service name\u0026rsquo; and IP which are 10.32.0.1 and kubernetes.default) localhost $ cd ~/kubernetes-the-hard-way ## CERT_HOSTNAME=10.32.0.1,\u0026lt;master node 1 Private IP\u0026gt;,\u0026lt;master node 1 hostname\u0026gt;,\u0026lt;master node 2 Private IP\u0026gt;,\u0026lt;master node 2 hostname\u0026gt;,\u0026lt;API load balancer Private IP\u0026gt;,\u0026lt;API load balancer hostname\u0026gt;,127.0.0.1,localhost,kubernetes.default $ CERT_HOSTNAME=10.32.0.1,m1,10.200.1.10,m2,10.200.1.11,proxy,10.200.1.15,127.0.0.1,localhost,kubernetes.default $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kubernetes-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${CERT_HOSTNAME} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes $ ls Service Account Key Pair Service account key pair certificate is used to sign service account tokens\n$ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/service-account-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account $ ls service-account-key.pem service-account.pem Copy Certificates to Nodes $ cd ~/kubernetes-the-hard-way # Minion $ IFS=$\u0026#39;\\n\u0026#39; $ for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ca.pem ${instance}-key.pem ${instance}.pem ${user}@${instance}:~/ done # Master $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat controllers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem ${user}@${instance}:~/ done 4. Generating kubeconfig Files for Authentication kubeconfig is used for authentication between the kubernetes components and users-to-kubernetes. kubeconfig consists of mainly 3 things\nNo. Entity Description 1 Cluster api-server\u0026rsquo;s IP and its certificate which encodes in base64 2 Users User related info like who is authenticating, their certificate and key or service account token 3 Context Holds Cluster\u0026rsquo;s and User\u0026rsquo;s reference. If you have multiple clusters and users, this context becomes handy In this section, we are going to generate kubeconfig for the below components\nGenerating kubelet kubeconfig The user in kubeconfig should be system:node:\u0026lt;Worker_name\u0026gt; which should match the Kubelet hostname that we specified while generating the kubelet client certificate. This will ensure Kubelets are properly authorized by the Kubernetes Node Authorizer.\n$ cd ~/kubernetes-the-hard-way $ KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done $ ls n1.kubeconfig n2.kubeconfig Generate kube-proxy kubeconfig $ cd ~/kubernetes-the-hard-way $ KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig } $ ls kube-proxy.kubeconfig Generate kube-controller-manager kubeconfig $ cd ~/kubernetes-the-hard-way { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig } $ ls kube-controller-manager.kubeconfig Generate kube-scheduler kubeconfig $ cd ~/kubernetes-the-hard-way $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig } $ ls kube-scheduler.kubeconfig Generate admin kubeconfig $ cd ~/kubernetes-the-hard-way $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig } $ ls admin.kubeconfig Copy kubeconfig files to nodes $ cd ~/kubernetes-the-hard-way $ IFS=$\u0026#39;\\n\u0026#39; $ for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ${instance}.kubeconfig kube-proxy.kubeconfig ${user}@${instance}:~/ done $ for instance in `cat controllers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${user}@${instance}:~/ done 5. Generating the Data Encryption Config and Key Kubernetes stores different types of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest. In this section, we will generate an encryption key and an encryption config suitable for encrypting Kubernetes Secrets.\nThe Encrypted Key ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) The Encryption Config File cat \u0026gt; encryption-config.yaml \u0026lt;\u0026lt;EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF Copy to Controller Nodes $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat controller.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh encryption-config.yaml ${user}@${instance}:~/ done Till now we have done following things\nProvisioned compute resources Generated certificates Generated kubeconfig files Copied certificate files and kubeconfigs to nodes In the next post, we will bootstrap controller nodes\n","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-1/","summary":"\u003cp\u003eHallo alle zusammen, after a long time I\u0026rsquo;m writing this blog and I come with an interesting and long post\u003c/p\u003e\n\u003cp\u003eI know what you are thinking, I steal \u003ca href=\"https://github.com/kelseyhightower/kubernetes-the-hard-way\"\u003eKelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial\u003c/a\u003e, but hey!, I did some research and try to \u003cstrong\u003efit K8s cluster(Multi-Master!) in a laptop with Docker as \u0026lsquo;\u003ca href=\"https://kubernetes.io/docs/setup/cri/\"\u003eCRI\u003c/a\u003e\u0026rsquo; and Flannel as \u0026lsquo;\u003ca href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/\"\u003eCNI\u003c/a\u003e\u0026rsquo;.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis blog post follows \u003ca href=\"https://github.com/kelseyhightower\"\u003eKelsey Hightower\u0026rsquo;s\u003c/a\u003e \u003ca href=\"https://github.com/kelseyhightower/kubernetes-the-hard-way\"\u003eKubernetes The Hard Way\u003c/a\u003e, I highly recommend go through his repo. I\u0026rsquo;m writing this blog post to keep it as a reference for me and share it with other people who want to try it. So, feel free to correct me if there are any mistakes and ping me for any queries. This series is divided into 3 parts and all configuration/scripts are available in my \u003ca href=\"https://github.com/veerendra2/k8s-the-hard-way-blog\"\u003egithub repo\u003c/a\u003e. Well, that has been said, let\u0026rsquo;s start building the cluster.\u003c/p\u003e","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 1)"},{"content":"Welcome back to \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series part 2. In previous post we have provisioned compute resources, generated certificates and kubeconfig files. In this post, we will install and configure controller nodes\n6. Bootstrapping the etcd Cluster etcd is a consistent and highly-available key value storage DB. Kubernetes stores all cluster data in etcd via api-server. In this section, we will install and configure etcd on all controller nodes.\n*NOTE: The below commands must run on all controller nodes\n*TIP: You can use tumx to run command on multiple nodes at same time\n## On controller nodes $ wget -q --show-progress --https-only --timestamping \\ \u0026#34;https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz\u0026#34; $ tar -xvf etcd-v3.3.9-linux-amd64.tar.gz $ sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ $ sudo mkdir -p /etc/etcd /var/lib/etcd $ sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ Set up the following environment variables which are usefull generate etcd systemd unit file\n## On controller nodes $ ETCD_NAME=`hostname` $ INTERNAL_IP=`hostname -i` # IP of the current node #INITIAL_CLUSTER=\u0026lt;controller 1 hostname\u0026gt;=https://\u0026lt;controller 1 private ip\u0026gt;:2380,\u0026lt;controller 2 hostname\u0026gt;=https://\u0026lt;controller 2 private ip\u0026gt;:2380 $ INITIAL_CLUSTER=m1=https://10.200.1.10:2380,m2=https://10.200.1.11:2380 Create a systemd unit file\n## On controller nodes $ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start the etcd service\n## On controller nodes $ { sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd } Once etcd installation and configuration are done in all controller nodes, verify that etcd cluster is working properly\n## On controller nodes $ sudo ETCDCTL_API=3 etcdctl member list \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem You should see output like below\n7. Bootstrapping the Kubernetes Control Plane The control plane binaries are\nkube-apiserver kube-controller-manager kube-scheduler Download control plane binaries\n*NOTE: The below commands must run on all controller nodes\n## On controller nodes $ sudo mkdir -p /etc/kubernetes/config $ KUBERNETES_VERSION=v1.10.13 $ wget -q --show-progress --https-only --timestamping \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-apiserver\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-controller-manager\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-scheduler\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kubectl\u0026#34; *TIP: You can get version number from kubernetes releases page\nMove the binaries to /usr/local/bin/\n## On controller nodes $ chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl $ sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ Kubernetes API Server Configuration Move certificates to kubernetes directory\n## On controller nodes $ sudo mkdir -p /var/lib/kubernetes/ $ sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ Create a kube-api server systemd unit file.\n## On controller nodes $ CONTROLLER0_IP=10.200.1.10 $ CONTROLLER1_IP=10.200.1.11 $ INTERNAL_IP=`hostname -i` # Current node\u0026#39;s IP $ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kubernetes Controller Manager Configuration Move kubeconfig files to kubernetes directory\n## On controller nodes $ sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/ Create kube-controller-manager systemd unit file\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kubernetes Scheduler Configuration Move kube-scheduler kubeconfig to kubernetes directory\n# On controller nodes $ sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/ Create kube-scheduler configuration file\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \u0026#34;/var/lib/kubernetes/kube-scheduler.kubeconfig\u0026#34; leaderElection: leaderElect: true EOF {% endhighlight %} Create kube-scheduler systemd unit file {% highlight shell %} # On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start the controller services ## On controller nodes $ sudo systemctl daemon-reload $ sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler $ sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler Enable HTTP Health Checks In the original \u0026ldquo;Kubernetes The Hard Way\u0026rdquo;, Kelsey used a GCP load balancer to load balance the requests among controllers. Since it is difficult to set up HTTPS health checks on the GCP network load balancer and kube-apiserver supports only HTTPS health checks. He created HTTP Nginx proxy for kube-api server, GCP network load balancer performs a health check via HTTP Nginx proxy. But in our case, we can skip this step since we are not using a GCP network load balancer\nVerification Check the component\u0026rsquo;s status using the below commands.\n## On controller nodes $ kubectl get componentstatuses --kubeconfig admin.kubeconfig Run the above command on all controller nodes and verify statuses which should be like below\nRBAC for Kubelet Authorization In this section, we will configure RBAC permissions to allow the kube-api server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs, and executing commands in pods.\nCreate the system:kube-apiserver-to-kubelet ClusterRole with permissions to access the Kubelet.\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \u0026#34;*\u0026#34; EOF The kube-api server authenticates to the Kubelet as the \u0026ldquo;kubernetes\u0026rdquo; user using the client certificate as defined by the --kubelet-client-certificate flag which has been defined in the kube-apiserver systemd unit file above.\nBind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \u0026#34;\u0026#34; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF The Kubernetes Frontend Load Balancer As I said earlier, we are not going to use a GCP load network load balancer, but we are going to use the nginx docker container on the host(Laptop) to load balance the requests.\nIn this section, we will build an nginx docker image with the appropriate configuration to load balance requests among controller nodes(m1 and m2)\nnginx configuration Specify controllers IPs with kube-api server\u0026rsquo;s port in nginx configuration like below\n## On host cd ~/kubernetes-the-hard-way $ cat \u0026lt;\u0026lt;EOF | tee kubernetes.conf stream { upstream kubernetes { server 10.200.1.10:6443; server 10.200.1.11:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF Dockerfile Create Dockerfile to build nginx load balancer docker image\n# On host $ cd ~/kubernetes-the-hard-way $ cat \u0026lt;\u0026lt;EOF | tee Dockerfile FROM nginx:latest MAINTAINER Veerendra Kakumanu RUN mkdir -p /etc/nginx/tcpconf.d \u0026amp;\u0026amp; echo \u0026#34;include /etc/nginx/tcpconf.d/*;\u0026#34; \u0026gt;\u0026gt; /etc/nginx/nginx.conf COPY kubernetes.conf /etc/nginx/tcpconf.d/kubernetes.conf EOF Build and launch the container\n# On host $ cd ~/kubernetes-the-hard-way $ sudo docker build -t nginx_proxy . $ sudo docker run -it -d -h proxy --net br0 --ip 10.200.1.15 nginx-proxy Verification curl the HTTPS endpoint of the load balancer(Nginx docker container) which forwards the requests to the controller node with certificate.\n## On host $ KUBERNETES_PUBLIC_ADDRESS=10.200.1.15 $ curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version If everything is good, you should see the output below.\nIn this post, we have successfully provisioned controller nodes and load balancers. We will bootstrap the worker nodes in the next post\n","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-2/","summary":"\u003cp\u003eWelcome back to \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series part 2. In \u003ca href=\"https://veerendra2.github.io/kubernetes-the-hard-way-1/\" title=\"previous post\"\u003eprevious post\u003c/a\u003e we have provisioned compute resources, generated certificates and kubeconfig files. In this post, we will install and configure controller nodes\u003c/p\u003e\n\u003ch1 id=\"6-bootstrapping-the-etcd-cluster\"\u003e6. Bootstrapping the etcd Cluster\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://coreos.com/etcd/\"\u003e\u003ccode\u003eetcd\u003c/code\u003e\u003c/a\u003e is a consistent and highly-available key value storage DB. Kubernetes stores all cluster data in \u003ccode\u003eetcd\u003c/code\u003e via api-server. In this section, we will install and configure \u003ccode\u003eetcd\u003c/code\u003e on all controller nodes.\u003c/p\u003e","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 2)"},{"content":"Welcome to the final part of \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series. In part-1, we discussed our cluster architecture, provisioned compute resources, generated certificates and kubeconfig. In part-2, we have bootstrapped controller nodes.\nIn this post, we will bootstrap worker nodes and at the end, perform a smoke test on the cluster\n9. Bootstrapping the Kubernetes Worker Nodes As the title of this post \u0026ldquo;Kubernetes The Hard Way With Docker \u0026amp; Flannel\u0026rdquo;, what we are going to do now is different from Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial i.e. container runtime interface is docker instead of containerd\n*NOTE: The below commands must run on all worker nodes\nInstall the below packages. conntack is required for iptables, since it tracks the connections for K8s services\n## On worker nodes $ { sudo apt-get update sudo apt-get -y install socat conntrack ipset } Install docker You can follow official docs to install docker on ubuntu\nKubelet Configuration Move certificate files to kubernetes directory\n## On worker nodes $ { sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ } Create kubelet configuration file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \u0026#34;/var/lib/kubernetes/ca.pem\u0026#34; authorization: mode: Webhook clusterDomain: \u0026#34;cluster.local\u0026#34; clusterDNS: - \u0026#34;10.32.0.10\u0026#34; podCIDR: \u0026#34;10.100.0.0/16\u0026#34; #resolvConf: \u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; runtimeRequestTimeout: \u0026#34;15m\u0026#34; tlsCertFile: \u0026#34;/var/lib/kubelet/n1.pem\u0026#34; tlsPrivateKeyFile: \u0026#34;/var/lib/kubelet/n1-key.pem\u0026#34; EOF Create a kubelet systemd unit file. Below you can notice I have specified --docker* flag which indicates that kubelet intracts with docker daemon\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --docker=unix:///var/run/docker.sock \\ --docker-endpoint=unix:///var/run/docker.sock \\ --image-pull-progress-deadline=2m \\ --network-plugin=cni \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --register-node=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kube Proxy Configuration Move kubeconfig to kubernetes directory\n## On worker nodes $ sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig Create kube-proxy configuration file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \u0026#34;/var/lib/kube-proxy/kubeconfig\u0026#34; mode: \u0026#34;iptables\u0026#34; clusterCIDR: \u0026#34;10.100.0.0/16\u0026#34; EOF Create kube-proxy systemd unit file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start Worker services ## On worker nodes $ { sudo systemctl daemon-reload sudo systemctl enable kubelet kube-proxy sudo systemctl start kubelet kube-proxy } Verification Once worker services configuration is done on all worker nodes, get the nodes list like the below command in any controller node\n10. Configuring kubectl for Remote Access In this section, we will generate a kubeconfig file for admin user. The kubeconfig file requires Kubernetes API server IP which is the nginx load balancer docker container’s IP\n## On host $ { KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster=kubernetes-the-hard-way \\ --user=admin kubectl config use-context kubernetes-the-hard-way } Verification Check the health of the remote Kubernetes cluster List the nodes in the remote Kubernetes cluster Provisioning CNI In this section, we will set up CNI i.e Flannel as the title of this blog post says.\n**If you want to know other CNIs and there performances, check Alexis Ducastel\u0026rsquo;s post here\nFirst login into worker nodes and enable ip forwarding\n## On worker nodes $ sudo sysctl net.ipv4.conf.all.forwarding=1 Get kube-flannel.yml from coreos\u0026rsquo;s flannel github repo\n## On host $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Wait for a few seconds and verify the flannel daemonset status\n$ kubectl get daemonsets -n kube-system Once pods are up, we have to test pod networking so that they can connect to each other\nFor that, we will deploy Nginx deployment with 2 replicas and a busy box pod. Then we will try to curl the nginx home page from busybox via nginx\u0026rsquo;s POD IP\nCreate nginx deployment with 2 replicas\n$ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF Create service for the deployment\n$ kubectl expose deployment/nginx Get nginx pods IP\n$ kubectl get ep nginx Now let curl nginx home of nginx pods\n$ kubectl run busybox --image=odise/busybox-curl --command -- sleep 3600 $ POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ kubectl exec $POD_NAME -- curl \u0026lt;first nginx pod IP address\u0026gt; $ kubectl exec $POD_NAME -- curl \u0026lt;second nginx pod IP address\u0026gt; $ kubectl get svc 11. Deploying the DNS Cluster Add-on In this section, we will deploy DNS add-on which provides DNS based service discovery. We will use coreDNS as DNS add-on in our K8s\nDeploy core DNS\n$ kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns.yaml Verification Verify core DNS pods are up\n$ kubectl get pods -l k8s-app=kube-dns -n kube-system In order to verify DNS resolution in K8s, we need to create a busybox pod and try nslookup the kubernetes service\nCreate a busybox deployment\n$ kubectl run busybox --image=odise/busybox-curl --command -- sleep 3600 Retrieve the full name of the busybox pod and execute a DNS lookup for the kubernetes service inside the busybox pod\n$ POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ kubectl exec -ti $POD_NAME -- nslookup kubernetes If everything is good, you should see \u0026ldquo;kubernetes\u0026rdquo; name resolution like above\nThat completes our objectives, we have installed necessary components to bring up the kubernetes.You can perform some other smoke test from official Kubernetes The Hard Way\nConclusion It has been a long post for readers. I have modified the official Kubernetes The Hard Way to set up Docker as CRI and Flannel as CNI. So, let\u0026rsquo;s conclude what we have done so far\nProvisioning compute resources in Laptop with kvm hypervisor 2 controllers, 2 computers and nginx docker containers which serves as load balancer. Generated certificates to setup TLS communication between the kubernetes components kubeconfig files generations Provisioning controller and worker nodes with docker and Flannel You can go even further to set up K8s dashboard,K8s logging and Prometheus monitoring, etc. (For starters, you can refer my prometheus-k8s-monitoring)\nReferences https://github.com/kelseyhightower/kubernetes-the-hard-way https://developer.ibm.com/recipes/tutorials/bridge-the-docker-containers-to-external-network/ https://docs.docker.com/config/containers/container-networking/ https://coreos.com/flannel/docs/latest/kubernetes.html https://unix.stackexchange.com/questions/490893/not-able-to-ssh-from-vm-to-vm-via-linux-bridge ","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-3/","summary":"\u003cp\u003eWelcome to the final part of \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series. In \u003ca href=\"https://veerendra2.github.io/kubernetes-the-hard-way-1/\" title=\"part-1\"\u003epart-1\u003c/a\u003e, we discussed our cluster architecture, provisioned compute resources, generated certificates and kubeconfig. In \u003ca href=\"https://veerendra2.github.io/kubernetes-the-hard-way-2/\" title=\"part-2\"\u003epart-2\u003c/a\u003e, we have bootstrapped controller nodes.\u003c/p\u003e\n\u003cp\u003eIn this post, we will bootstrap worker nodes and at the end, perform a smoke test on the cluster\u003c/p\u003e\n\u003ch2 id=\"9-bootstrapping-the-kubernetes-worker-nodes\"\u003e9. Bootstrapping the Kubernetes Worker Nodes\u003c/h2\u003e\n\u003cp\u003eAs the title of this post \u0026ldquo;Kubernetes The Hard Way With Docker \u0026amp; Flannel\u0026rdquo;, what we are going to do now is different from \u003ca href=\"https://github.com/kelseyhightower/kubernetes-the-hard-way\"\u003eKelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial\u003c/a\u003e i.e. container runtime interface is \u003ccode\u003edocker\u003c/code\u003e instead of \u003ccode\u003econtainerd\u003c/code\u003e\u003c/p\u003e","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 3)"},{"content":"As we all know, enabling HTTPS to endpoints/websites is essential now-a-days. When it comes to Kubernetes, when we expose a service as LoadBalancer, the cloud provider doesn\u0026rsquo;t provide an HTTPS mechanism for the endpoint by default.\nIf we look at the K8s setup that is deployed on AWS(For example kops), there is an actual ELB(Elastic Load Balancer) sits in front of K8s service and load balance the traffic. AWS\u0026rsquo;s ELB is not TLS enabled by default. With help of aws-cli, we can deploy certificates(self-signed) on the load balancer and make the endpoint secure.\nNote that the K8s cluster is deployed on AWS and configured \u0026ldquo;type: LoadBalancer\u0026rdquo; for service which applications can access from outside of the cluster.\nPrerequisites Get cfssl and cfssljson binary files from https://pkg.cfssl.org/ Get aws-cli. Check installation docs Configure aws-cli with aws configure. It should create files like below veeru@ultron:~$ cat ~/.aws/credentials [default] aws_access_key_id = ATIA2HTxxxV5Cqwe aws_secret_access_key = ATIA2HTxxxV5Cqwexxxxxx veeru@ultron:~$ cat ~/.aws/config [default] region = us-east-2 output = text Create certificate $ cat \u0026lt;\u0026lt;EOF \u0026gt;csr_ca.json { \u0026#34;CN\u0026#34;: \u0026#34;My Awesome CA\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;Westeros\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Winterfell\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;House Stark\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;CA Secsr_ca.jsonrvices\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;The North\u0026#34; } ] } EOF Generate the CA certificate and private key:\n$ cfssl gencert -initca csr_ca.json | cfssljson -bare ca $ ls ca-key.pem ca.pem Upload your self signed certificate to aws $ aws iam upload-server-certificate --server-certificate-name your-name --certificate-body file://ca.pem --private-key file://ca-key.pem List certificates\n$ aws iam list-server-certificates SERVERCERTIFICATEMETADATALIST arn:aws:iam::xxxxx:server-certificate/your-name 2023-04-30T07:52:00Z / ASCAIxxxxxCHES3FxxIQO cf 2018-05-01T08:17:30Z Specify annotation in Kuberenetes service Edit service with \u0026ldquo;kubectl edit svc {svc-name}\u0026rdquo; or you can also edit with the help of K8s dashboard like me.\n\u0026#34;service.beta.kubernetes.io/aws-load-balancer-ssl-cert\u0026#34;: \u0026#34;arn:aws:iam::xxxxx:server-certificate/your-name\u0026#34; Now you should be able to access the endpoint on https.\nFor example: https://xxxx-xxxx.us-east-2.elb.amazonaws.com:9090/graph Check out other AWS service annotations\n","permalink":"https://veerendra2.github.io/ssl-config-k8s-service-aws/","summary":"\u003cp\u003eAs we all know, enabling HTTPS to endpoints/websites is essential now-a-days. When it comes to Kubernetes, when we expose a service as \u003ccode\u003eLoadBalancer\u003c/code\u003e, the cloud provider doesn\u0026rsquo;t provide an HTTPS mechanism for the endpoint by default.\u003c/p\u003e\n\u003cp\u003eIf we look at the K8s setup that is deployed on AWS(For example \u003ca href=\"https://github.com/kubernetes/kops\"\u003e\u003ccode\u003ekops\u003c/code\u003e\u003c/a\u003e), there is an actual \u003ccode\u003eELB\u003c/code\u003e(Elastic Load Balancer) sits in front of K8s service and load balance the traffic. AWS\u0026rsquo;s \u003ccode\u003eELB\u003c/code\u003e is not TLS enabled by default. With help of aws-cli, we can deploy certificates(self-signed) on the load balancer and make the endpoint secure.\u003c/p\u003e","title":"SSL Configuration for Kubernetes External LoadBalancer - [AWS ELB]"},{"content":"*A blog post that I’m actively collecting “Linux pseudo files info, cheat sheets and tips”\nTips \u0026amp; Tricks How to force a command to return exit code 0 even if the command exited non-zero?\nHow to install dependencies of .deb automatically which failed to install previously?\nExample Solution:\n$ dpkg -i r-base-core_3.3.3-1trusty0_amd64.deb || : \\ \u0026amp;\u0026amp; apt-get --yes --force-yes -o Dpkg::Options::=\u0026#34;--force-confdef\u0026#34; -o Dpkg::Options::=\u0026#34;--force-confold\u0026#34; -f install -y \\ How to traverse directories in shell script?\ncd command should not be used to traverse directories. Remember that each commands in shell script will spawn as individual process unlink programming language, entire script as single process i.e. The scope of cd command is only for the child process, not the parent. By using pushd and popd we can achieve traversing directories.\nExample Solution:\n$ pushd Downloads $ cat download.txt $ popd $ pushd Downloads/movies $ ls $ popd Files: /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq - Real time speed of the CPU(ability to adjust their speed to help in saving on battery/power usage)\n/proc Directory\n/proc/cpuinfo | grep MHz - The absolute (max) CPU speed /proc/sys/net/ipv4/* - Get more info under this directory from kernel.org docs /proc/net/tcp and /proc/net/tcp6 - Get complete info of variables for these files from kernel.org.docs /proc/sysctl https://www.kernel.org/doc/Documentation/sysctl/ https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Reference_Guide/s3-proc-sys-net.html\nNetwork related in Linux - Refer kernel.org.doc\nSpecial Device Files /dev/null - Discards all data written to it but reports that the write operation succeeded Read man pages\n/dev/full - Returns the error code ENOSPC (meaning “No space left on device”) on writing Read man pages\n/dev/random - Special file that serves as a blocking pseudorandom number generator. It allows access to environmental noise collected from device drivers and other sources.(Block until additional environmental noise is gathered)Read man\n/dev/urandom - Without block Read man pages\n/dev/zero - Provides as many null characters as are read from it Read More\nudev - Linux dynamic device management Read man pages\nudevadm - command to query the udev database and sysfs Read More Commands/Tools lscpu - Display CPU architecture information\ncat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1 - Generates 40 characters long random string\nmtr - mtr combines the functionality of the traceroute and ping programs in a single network diagnostic tool.\nlsblk - Lists block devices\nDirectories /var/lock/ - Store lock files, which are simply files used to indicate that a certain resource (a database, a file, a device) is in use and should not be accessed by another process. Aptitude, for example, locks its database when a package manager is running.\n/var/run - Used to store .pid files, which contain the process id of a running program. This is commonly used in services or programs that need to make their process id available to other processes.\n","permalink":"https://veerendra2.github.io/linux-cheatseets/","summary":"\u003cp\u003e*\u003cem\u003eA blog post that I’m actively collecting “Linux pseudo files info, cheat sheets and tips”\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"tips--tricks\"\u003eTips \u0026amp; Tricks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to force a command to return exit code 0 even if the command exited non-zero?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to install dependencies of .deb automatically which failed to install previously?\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eExample Solution:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ dpkg -i r-base-core_3.3.3-1trusty0_amd64.deb \u003cspan class=\"o\"\u003e||\u003c/span\u003e : \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e apt-get --yes --force-yes -o Dpkg::Options::\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;--force-confdef\u0026#34;\u003c/span\u003e -o Dpkg::Options::\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;--force-confold\u0026#34;\u003c/span\u003e -f install -y \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to traverse directories in shell script?\u003c/p\u003e","title":"Linux pseudo files \u0026 cheat sheet"},{"content":"Wireshark is a great tool for analyzing traffic, whether it could be live traffic on the interface or the .cap file. The tool enables different types of filtering on packets like follow a stream, filter by protocol and IP, etc\nIn order to install the latest version of Wireshark on Linux, one should build and install it from the source. Sometimes, building from a source is difficult because we have to hunt down the dependencies. That\u0026rsquo;s what I did for this software.\nDepending on your OS and package availability, you may need to install other dependencies. I\u0026rsquo;m using Ubuntu Mate 16 and I found the below are sufficient for me.\nInstall Dependencies $ apt-get install -y \\ qtbase5-dev qtbase5-dev-tools \\ qttools5-dev qttools5-dev-tools \\ qtmultimedia5-dev libqt5svg5-dev \\ libpcap-dev libgcrypt11-dev \\ glib2.0 libgcrypt20-dev \\ libglib2.0-dev ibglib2.0-dev Get the latest tarball from wireshark $ wget https://2.na.dl.wireshark.org/src/wireshark-2.4.5.tar.xz $ tar -xf wireshark-2.4.5.tar.xz $ cd wireshark-2.4.5 Start building $ ./configure $ sudo make install -j2 $ sudo ldconfig $ sudo wireshark ./configure checks dependencies for Wireshark in your machines. That\u0026rsquo;s why while running ./configure you may get dependency missing errors. If that is the case, it will show the missing dependency packages name i.e. you can google it and install it.\nmake install -j2 will take some time, you can have coffee. (Specify jobs that are equal to your number of CPU cores. Ex.-j4 for quad core)\n","permalink":"https://veerendra2.github.io/wireshark-install/","summary":"\u003cp\u003eWireshark is a great tool for analyzing traffic, whether it could be live traffic on the interface or the \u003ccode\u003e.cap\u003c/code\u003e file. The tool enables different types of filtering on packets like follow a stream, filter by protocol and IP, etc\u003c/p\u003e\n\u003cp\u003eIn order to install the latest version of Wireshark on Linux, one should build and install it from the source. Sometimes, building from a source is difficult because we have to hunt down the dependencies. That\u0026rsquo;s what I did for this software.\u003c/p\u003e","title":"Build and Install Wireshark"},{"content":"Long back before I worked on Openshift which is really a great container platform tool from Redhat. But installation is not as simple as Kubernetes(relatively). One of the prerequisites for the cluster deployment is Open vSwitch.\nNow let\u0026rsquo;s see how to install Open vSwitch v2.6.1 in RedHat7 step by step\nInstall dependencies\n$ sudo yum install gcc make python-devel openssl-devel \\ kernel-devel graphviz kernel-debug-devel \\ autoconf automake rpm-build redhat-rpm-config \\ libtool Grab OpenvSwitch source from http://www.openvswitch.org/download/\n$ wget http://openvswitch.org/releases/openvswitch-2.6.1.tar.gz $ tar -xf openvswitch-2.6.1.tar.gz $ cd openvswitch-2.6.1 Create a distribution tarball\n$ ./boot.sh $ ./configure $ make dist Now you have distribution tarball(openvswitch-2.6.1.tar.gz) in current directory. Copy this file into the RPM sources directory, e.g.:\n$ cp openvswitch-2.6.1.tar.gz $HOME/rpmbuild/SOURCES Extract distribution tarball openvswitch-2.6.1.tar.gz\n$ tar -xf openvswitch-2.6.1.tar.gz $ cd openvswitch-2.6.1 $ pwd /home/ec2-user/openvswitch-2.6.1/openvswitch-2.6.1 Build Open vSwitch\n$ rpmbuild -bb --without check rhel/openvswitch.spec ... Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/openvswitch-2.6.1-1.x86_64 Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-2.6.1-1.x86_64.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-devel-2.6.1-1.x86_64.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/noarch/openvswitch-selinux-policy-2.6.1-1.noarch.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-debuginfo-2.6.1-1.x86_64.rpm ... At the end of building, it will generate openvswitch RPM files.\nInstall the openvswitch RPM files\n$ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-2.6.1-1.x86_64.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-devel-2.6.1-1.x86_64.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/noarch/openvswitch-selinux-policy-2.6.1-1.noarch.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-debuginfo-2.6.1-1.x86_64.rpm Start the openvswitch daemon\n$ sudo service openvswitch start $ sudo service openvswitch status Then you should able to run ovs-appctl --help\nSource http://www.openvswitch.org//support/dist-docs-2.5/INSTALL.RHEL.md.html\n","permalink":"https://veerendra2.github.io/openvswitch-redhat/","summary":"\u003cp\u003eLong back before I worked on Openshift which is really a great container platform tool from Redhat. But installation is not as simple as Kubernetes(relatively). One of the prerequisites for the cluster deployment is Open vSwitch.\u003c/p\u003e\n\u003cp\u003eNow let\u0026rsquo;s see how to install Open vSwitch v2.6.1 in RedHat7 step by step\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall dependencies\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ sudo yum install gcc make python-devel openssl-devel \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e      kernel-devel graphviz kernel-debug-devel \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e      autoconf automake rpm-build redhat-rpm-config \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e      libtool\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGrab OpenvSwitch source from \u003ca href=\"http://www.openvswitch.org/download/\"\u003ehttp://www.openvswitch.org/download/\u003c/a\u003e\u003c/p\u003e","title":"Open vSwitch installation on Redhat7 OS"},{"content":"Ok, getting metrics(CPU, Memory \u0026amp; Network) from Windows OS is completely different from Linux. In Linux, people can easily develop scripts to get system metrics by simply reading /proc pseudo files. In fact, there are so many open source tools to do this in Linux, like tcollector which is my favourite.\nNow, Let\u0026rsquo;s look at this Telegraf tool and what it does. I found Telegraf tool is a really simple, elegant way to collect Windows OS metrics and lightweight too, unlike others which some are paid and crappy. This tool doesn\u0026rsquo;t provide any wizard installation to set up, but one has to run a command in Windows Powershell to install it as Windows service. It supports multiple TSDB backend storage like Graphite, OpenTSDB, etc but I have tested only with OpenTSDB.\nAs they said in Github repo and I quote\nTelegraf is an agent written in Go for collecting, processing, aggregating, and writing metrics.\nDesign goals are to have a minimal memory footprint with a plugin system so that developers in the community can easily add support for collecting metrics from local or remote services.\nGoto influxdata download portal and download Telegraf zip file\nCreate a folder and name it as Telegraf in C:\\Program Files and extract the .zip content to Telegraf folder (C:\\Program Files\\Telegraf)\nDownload telegraf configuration from here (telegraf.conf) and place it in C:\\Program Files\\Telegraf\nSpecify OpenTSDB server IP in outputs.opentsdb section in the configuration Open \u0026ldquo;Windows PowerShell\u0026rdquo; with administrator rights(Run as administrator) and paste below command to create \u0026ldquo;windows service\u0026rdquo;\nC:\\\u0026#34;Program Files\u0026#34;\\Telegraf\\telegraf.exe --config C:\\\u0026#34;Program Files\u0026#34;\\Telegraf\\telegraf.conf –-service install In Windows Services, you should see Telegraf service. Right-click on the Telegraf service, open \u0026quot;Properties\u0026quot;-\u0026gt; Select \u0026quot;Automatic\u0026quot; for \u0026ldquo;Startup Type\u0026rdquo; and click \u0026ldquo;Start\u0026rdquo; button to start the Telegraf service.\nYou should able to see these metrics in your OpenTSDB\nDocs - https://docs.influxdata.com/telegraf/v1.5/introduction/getting_started/ ","permalink":"https://veerendra2.github.io/windows-metrics-collection/","summary":"\u003cp\u003eOk, getting metrics(CPU, Memory \u0026amp; Network) from Windows OS is completely different from Linux. In Linux, people can easily develop scripts to get system metrics by simply reading \u003ca href=\"https://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/proc.html\"\u003e/proc\u003c/a\u003e pseudo files. In fact, there are so many open source tools to do this in Linux, like \u003ca href=\"https://github.com/OpenTSDB/tcollector\"\u003etcollector\u003c/a\u003e which is my favourite.\u003c/p\u003e\n\u003cp\u003eNow, Let\u0026rsquo;s look at this \u003ca href=\"https://www.influxdata.com/time-series-platform/telegraf/\"\u003eTelegraf tool\u003c/a\u003e and what it does. I found Telegraf tool is a really simple, elegant way to collect Windows OS metrics and lightweight too, unlike others which some are paid and crappy. This tool doesn\u0026rsquo;t provide any wizard installation to set up, but one has to run a command in Windows Powershell to install it as Windows service. It supports multiple TSDB backend storage like Graphite, OpenTSDB, etc but I have tested only with OpenTSDB.\u003c/p\u003e","title":"Windows OS metrics collection with Telegraf"},{"content":"1. Install Packages Check system is capable of running KVM by running kvm-ok\n$ apt-get install qemu-kvm qemu-system libvirt-bin bridge-utils virt-manager -y Create KVM/Qemu Hard Disk File $ qemu-img create -f raw \u0026lt;name\u0026gt;.img \u0026lt;Size\u0026gt; ## Example $ qemu-img create -f raw ubuntu14-HD.img 10G Then copy the HD file to /var/lib/libvirt/images/ Launch VM with virt-install virt-install --name spinnaker \\ --ram 11096 \\ --vcpus=4 \\ --os-type linux \\ --os-variant=ubuntutrusty \\ --accelerate \\ --nographics -v \\ --disk path=/var/lib/libvirt/images/ubuntu14-HD.img,size=8 \\ --extra-args \u0026#34;console=ttyS0\u0026#34; \\ --location /opt/ubuntu14.iso --force \\ --network bridge:virbr0 Explanation\nCreate bridge virbr0 if it is necessary To know what are --os-variant available, run virt-install --os-variant list Specify --location and --disk locations Specify --ram (By default in MBs) Other Options --boot hd Boot from HD file --force Force to use existing HD that is used by another VM --debug verbose --description Description of VM Connect to console virsh list --all - : List VMs virsh console \u0026lt;name\u0026gt; - : Connect to tty of the VM Note down the IP of the VM once you connect to tty. we can ssh\nNOTE: If console/tty is already being used or active, you can reconnect to that tty by using --extra-args='console=ttyS0' option\nExport VM as .qcow2 $ qemu-img convert -f raw -O qcow2 \u0026lt;source .img file\u0026gt; \u0026lt;destination .qcow2 file\u0026gt; ## Example $ qemu-img convert -f raw -O qcow2 /var/lib/libvirt/images/ubuntu14-HD.img /home/opsmx/spinnaker.qcow2 Commands CheatSheet Command Description virsh list --all Shows all VMs virsh console \u0026lt;VM name\u0026gt; Connect to tty of the VM (If tty is enables) virsh shutdown \u0026lt;VM name\u0026gt; Shutdown the VM vish destroy \u0026lt;VM name\u0026gt; Destroys VM (Won\u0026rsquo;t deletes the VM/ Similar to shutdown) vish undefine \u0026lt;VM name\u0026gt; Deletes the VM (Run after destroy) virsh net-list List the available networks virsh net-edit \u0026lt;net name\u0026gt; Edit the network virt-install --os-variant list Lists OS variants Resource Links https://www.wavether.com/2016/11/import-qcow2-images-into-aws https://docs.openstack.org/image-guide/convert-images.html https://serverfault.com/questions/604862/any-way-to-convert-qcow2-to-ovf https://docs.openstack.org/image-guide/convert-images.html ","permalink":"https://veerendra2.github.io/kvm-hyperviour-cheatsheets/","summary":"\u003ch3 id=\"1-install-packages\"\u003e1. Install Packages\u003c/h3\u003e\n\u003cp\u003eCheck system  is  capable of running KVM by running \u003ca href=\"http://manpages.ubuntu.com/manpages/trusty/man1/kvm-ok.1.html\"\u003e\u003ccode\u003ekvm-ok\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ apt-get install qemu-kvm qemu-system libvirt-bin bridge-utils virt-manager -y\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"create-kvmqemu-hard-disk-file\"\u003eCreate KVM/Qemu Hard Disk File\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ qemu-img create -f raw \u0026lt;name\u0026gt;.img \u0026lt;Size\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e## Example\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e$ qemu-img create -f raw ubuntu14-HD.img 10G\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eThen copy the HD file to \u003ccode\u003e/var/lib/libvirt/images/\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"launch-vm-with-virt-install\"\u003eLaunch VM with \u003ccode\u003evirt-install\u003c/code\u003e\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003evirt-install --name spinnaker \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--ram \u003cspan class=\"m\"\u003e11096\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--vcpus\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e4\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--os-type linux \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--os-variant\u003cspan class=\"o\"\u003e=\u003c/span\u003eubuntutrusty \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--accelerate \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--nographics -v  \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--disk \u003cspan class=\"nv\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e/var/lib/libvirt/images/ubuntu14-HD.img,size\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"m\"\u003e8\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--extra-args \u003cspan class=\"s2\"\u003e\u0026#34;console=ttyS0\u0026#34;\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--location /opt/ubuntu14.iso --force \u003cspan class=\"se\"\u003e\\\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"se\"\u003e\u003c/span\u003e--network bridge:virbr0\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eExplanation\u003c/p\u003e","title":"KVM Hypervisor Cheat Sheets"},{"content":"We think that connecting to a website over HTTPS is secure, which is true(not true sometimes!), but what about DNS queries that you(browser) send?\nSure if we use HTTPS, all your (POST or GET) data is encrypted end-to-end which prevents eavesdropping, MITM attack and have Confidentiality, but again what about DNS queries?\nI got this question back a while ago, so after a quick Internet search, I found DNSCrypt protocol which is cool because I can encrypt DNS queries.\nFirst of all, what the heck is DNS? in simple, DNS or Domain Name System is a service that resolves/translates domain \u0026ldquo;name\u0026rdquo; to \u0026ldquo;IP\u0026rdquo; or vice versa. So once you hit google.com in your browser, a DNS query fired to DNS host(for example 8.8.8.8) like asking \u0026ldquo;what is the IP of google.com\u0026rdquo; and gets DNS responses which contain IP of google.com. Now we got the IP of google.com, and the browser initiates a connection and establishes HTTPS.\nSo, you see these DNS queries are not part of \u0026ldquo;HTTPS\u0026rdquo;. So let\u0026rsquo;s encrypt DNS queries with DNCrypt.\nWhy should we care about \u0026ldquo;DNS queries encryption\u0026rdquo;? Well, sometimes eavesdroppers are interested in the metadata of communication rather than actual communication.\nWhat is DNSCrypt? DNSCrypt is a protocol that authenticates communications between a DNS client and a DNS resolver. It prevents DNS spoofing. It uses cryptographic signatures to verify that responses originate from the chosen DNS resolver and haven\u0026rsquo;t been tampered with.\nIt is an open specification, with free and open source reference implementations, and it is not affiliated with any company nor organisation.\nThere are some points to be noted\nIn order to use this protocol, we should install a package called dnscrypt-proxy Normal name servers(like 8.8.8.8) won\u0026rsquo;t support this protocol. We should use these DNS resolvers dnscrypt-proxy by default binds on loopback interface (127.0.0.1) 53 port. So, I have to change the title configuration. Install dnscrypt-proxy From Ubuntu 16/ Linux Mint 18.x, dnscrypt-proxy is available in the official repo.\nsudo apt-get install dnscrypt-proxy I found a PPA for Ubuntu 14.04 and Linux Mint 17.x\nsudo add-apt-repository ppa:anton+/dnscrypt sudo apt-get update sudo apt-get install dnscrypt-proxy Start dnscrypt-proxy After installation, with --help argument get options and run accordingly. But luckily I created a python script which will do it for you.\nwget -qO dnscrypt.py https://goo.gl/zjZYVR sudo python dnscrypt.py After you run the script, it will list the DNS resolver details like below.(The script downloads resolvers csv and passes this file as argument to `dnscrypt-proxy``)\nSelect one name server. You can see these name servers have options DNSSec \u0026amp; No Logging which provider can log your queries, choose one accordingly (These options/table header you can\u0026rsquo;t see in the above screenshot. You have to scroll up)\nNext, configure your network settings like below\nRestart the network (disconnect and connect wifi) and you are done!\nTo verify run tcpdump -i any -n port 2053 (Why 2053 port? because in the above screenshot I selected the 66 option which has 178.216.201.222:2053)\nWhat\u0026rsquo;s happening? Go beyond this script! I created an init script which runs at system boot. So that there is no need to run the above script again and again.\nDownload resolvers csv file with \u0026ndash;\u0026gt; python dsncrypt.py -d Specify resolver_name(By default it has soltysiak which has No Logging policy and DNSSec) in the script. sudo wget -O /etc/init.d/encryptdns https://goo.gl/opZ78J sudo chmod +x /etc/init.d/encryptdns sudo update-rc.d encryptdns defaults sudo service encryptdns start Github Repository Link\nhttps://github.com/veerendra2/useless-scripts\nDNSCrypt in Windows Simple DNSCrypt Other resources you can try\nhttps://github.com/jedisct1/dnscrypt-proxy ","permalink":"https://veerendra2.github.io/dns-encrypt/","summary":"\u003cp\u003eWe think that connecting to a website over HTTPS is secure, which is true(not true sometimes!), but what about DNS queries that you(browser) send?\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static_content/images/https_example.jpg\" alt=\"HTTPS Example\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eSure if we use HTTPS, all your (\u003ca href=\"https://en.wikipedia.org/wiki/POST_(HTTP)\"\u003ePOST\u003c/a\u003e or GET) data is encrypted end-to-end which prevents eavesdropping, \u003ca href=\"https://en.wikipedia.org/wiki/Man-in-the-middle_attack\"\u003eMITM attack\u003c/a\u003e and have \u003ca href=\"https://en.wikipedia.org/wiki/Confidentiality\"\u003eConfidentiality\u003c/a\u003e, but again what about DNS queries?\u003c/p\u003e\n\u003cp\u003eI got this question back a while ago, so after a quick Internet search, I found \u003ca href=\"https://en.wikipedia.org/wiki/DNSCrypt\"\u003eDNSCrypt\u003c/a\u003e protocol which is cool because I can encrypt DNS queries.\u003c/p\u003e","title":"Encrypt your DNS queries, stay anonymous"},{"content":" A Wi-Fi deauthentication attack is a type of denial-of-service attack that targets communication between a user and a Wi-Fi wireless access point.\n-Wikipedia\nAs you can see, this type of attack is pretty powerful and difficult to detect who is attacking. There are some tools(like “aircrack-ng”) for this attack(You can check the commands here).\nSo, basically the concept is the attacker broadcasts a wifi management “Deauthentication” frame to the victim\u0026rsquo;s devices/PC to tell them to deauthenticate. It is like, “Hey client! Can you please deauthenticate”. Once deauthenticated, then the client will reconnect to AP (Access Point). These types of frames are supposed to send by valid “AP” to its clients, but the attacker can mimic these frames and broadcast in the network.\nInterestingly, the victim’s device/PC could not differentiate between the attacker and valid AP. Here, the attacker creates a “Deauthentication” packet/frame with the source MAC address of valid AP’s MAC address. So, every device thinks, the management frame came from valid AP.\nThe attacker not just sends the frame once, but sends continuously. Things get pretty bad, now the clients are continuously trying to reconnect. In this way, the clients never connect to its valid AP until the attacker stops sending the “deauth” frames.\nSo, how to avoid this attack? Simple, use 802.1w supported routers. Know more about 802.1w and read cisco document here.\nCheck if your wifi network is vulnerable to this attack or not\u0026hellip; I have created a Python script which sends deauth packets using the scapy python module. You can use this script to check if your wifi network is vulnerable or not. Just run the script, select the wifi network that you want to test and if you see a network outage, your wifi is vulnerable!\nDependencies wireless Install aircrack-ng and scapy\n$ sudo apt-get install aircrack-ng -y $ sudo apt-get install python-scapy -y Download and run the script $ wget -O deauth.py https://raw.githubusercontent.com/veerendra2/wifi-deauth-attack/master/deauth.py $ python deauth.py When you run the command, you should see it like below.\nWhen you start the script, it will create a “mon0” interface(A monitoring virtual interface used to send our deauth frames) and observe wifi signals. After a few seconds, it will display near APs and its MAC addresses. Choose one to broadcast the “deauth” frames to that network which results network outage for connected clients to that AP.\nNOTE: Inorder to work a deauthentication attack successfully, you should be near the target network. The deauth packets should reach the connected devices of the target network\nUse my docker image to kick the environment quickly. Github Repository Link - https://github.com/veerendra2/wifi-deauth-attack ","permalink":"https://veerendra2.github.io/wifi-deathentication-attack/","summary":"\u003cblockquote\u003e\n\u003cp\u003eA Wi-Fi deauthentication attack is a type of denial-of-service attack that targets communication between a user and a Wi-Fi wireless access point.\u003c/p\u003e\n\u003cp\u003e-Wikipedia\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAs you can see, this type of attack is pretty powerful and difficult  to detect who is attacking. There are some tools(like “aircrack-ng”) for this attack(You can check the commands \u003ca href=\"https://www.aircrack-ng.org/doku.php?id=deauthentication\"\u003ehere\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eSo, basically the concept is the attacker broadcasts a wifi management “Deauthentication” frame to the victim\u0026rsquo;s devices/PC to tell them to deauthenticate. It is like, “Hey client! Can you please deauthenticate”. Once  deauthenticated, then the client will reconnect to AP (Access Point). These types of frames are supposed to send by valid “AP” to its clients, but the attacker can mimic these frames and broadcast in the network.\u003c/p\u003e","title":"Wifi Deauthentication Attack"},{"content":" GNU Screen is a terminal multiplexer, a software application that can be used to multiplex several virtual consoles, allowing a user to access multiple separate login sessions inside a single terminal window, or detach and reattach sessions from a terminal. It is useful for dealing with multiple programs from a command line interface, and for separating programs from the session of the Unix shell that started the program, particularly so a remote process continues running even when the user is disconnected. more\n-Wikipedia\nInstall screen $ sudo apt-get install screen Keys/Commands Description screen Enables Screen Ctrl+a and then c Create new screen Ctrl+a and then n Go to next screen Ctrl+a and then p Go to previous screen Ctrl+a and then Shift+s Split screen horizontally Ctrl+a and then Shift+\\ Split screen vertically Ctrl+a and then Tab Traverse between splited screens Ctrl+a and then Shift+x Unsplit screens Ctrl+a and then Esc (Hit Esc, once you are done) Scroll screen Ctrl+a and then d Detach screens screen -r \u0026lt;PID\u0026gt; Reattach screen screen -ls List screens screen -S \u0026lt;session\u0026gt; Attach screen screen -XS \u0026lt;session\u0026gt; quit Kills screen Few more in my Github Gist ","permalink":"https://veerendra2.github.io/gnu-screen-commands/","summary":"\u003cblockquote\u003e\n\u003cp\u003eGNU Screen is a terminal multiplexer, a software application that can be used to multiplex several virtual consoles, allowing a user to access multiple separate login sessions inside a single terminal window, or detach and reattach sessions from a terminal. It is useful for dealing with multiple programs from a command line interface, and for separating programs from the session of the Unix shell that started the program, particularly so a remote process continues running even when the user is disconnected. \u003ca href=\"https://en.wikipedia.org/wiki/GNU_Screen\"\u003emore\u003c/a\u003e\u003c/p\u003e","title":"GNU screen commands(Cheat Sheet)"},{"content":"\u0026#x1f449; Update on 27-08-2022 Moving to Hugo and other updates! I was very excited to try Jekyll and Github Pages when I heard about it. When I try to install jekyll, I got below error\nroot@veeru:/home/veeru# gem install jekyll bundler Fetching: public_suffix-3.0.1.gem (100%) ERROR: Error installing jekyll: public_suffix requires Ruby version \u0026gt;= 2.1. Fetching: bundler-1.16.1.gem (100%) Successfully installed bundler-1.16.1 1 gem installed Installing ri documentation for bundler-1.16.1... Installing RDoc documentation for bundler-1.16.1... I don\u0026rsquo;t even know what that means(I\u0026rsquo;m not a Ruby guy, so..). Clearly jekyll needs more than Ruby version 2.1, but in Ubuntu 14.04 if you type apt-get install ruby -y you will end up having Ruby 1.9. So let\u0026rsquo;s install Ruby 2.4 like below\nsudo apt-add-repository ppa:brightbox/ruby-ng sudo apt-get update sudo apt-get install ruby2.4 ruby2.4-dev make g++ -y Then install jekyll\n$ gem install jekyll bundler What\u0026rsquo;s next? I found a blog which exactly I was looking for to deploy a website on Github Pages. As Drew Silcock said in the blog, it is better to maintain website source code and compiled websites on the same repository. Just head over to his blog and check the stuff\n","permalink":"https://veerendra2.github.io/jeklly-website/","summary":"\u003cp\u003e\u0026#x1f449; Update on 27-08-2022 \u003ca href=\"https://veerendra2.github.io/moving-to-hugo/\" title=\"Moving to Hugo and other updates!\"\u003e Moving to Hugo and other updates! \u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eI was very excited to try \u003ca href=\"https://jekyllrb.com/\"\u003e\u003ccode\u003eJekyll\u003c/code\u003e\u003c/a\u003e and \u003ca href=\"https://pages.github.com/\"\u003eGithub Pages\u003c/a\u003e when I heard about it. When I try to install \u003ccode\u003ejekyll\u003c/code\u003e, I got below error\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eroot@veeru:/home/veeru# gem install jekyll bundler\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFetching: public_suffix-3.0.1.gem \u003cspan class=\"o\"\u003e(\u003c/span\u003e100%\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eERROR:  Error installing jekyll:\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    public_suffix requires Ruby version \u0026gt;\u003cspan class=\"o\"\u003e=\u003c/span\u003e 2.1.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFetching: bundler-1.16.1.gem \u003cspan class=\"o\"\u003e(\u003c/span\u003e100%\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eSuccessfully installed bundler-1.16.1\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"m\"\u003e1\u003c/span\u003e gem installed\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eInstalling ri documentation \u003cspan class=\"k\"\u003efor\u003c/span\u003e bundler-1.16.1...\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eInstalling RDoc documentation \u003cspan class=\"k\"\u003efor\u003c/span\u003e bundler-1.16.1...\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eI don\u0026rsquo;t even know what that means(I\u0026rsquo;m not a Ruby guy, so..). Clearly \u003ccode\u003ejekyll\u003c/code\u003e needs more than Ruby version 2.1, but in Ubuntu 14.04 if you type \u003ccode\u003eapt-get install ruby -y\u003c/code\u003e you will end up having Ruby 1.9. So let\u0026rsquo;s install Ruby 2.4 like below\u003c/p\u003e","title":"Install jekyll in Ubuntu 14.04"},{"content":"“MAC Address Scrambling“- By the name itself we can understand, instead of using a burned-in address, the machine uses a random MAC address. The machine/device changes MAC addresses regularly to improve security. MAC address is a 48-bit hexadecimal digit which is burned in every electronic device that has the capability of “connectivity” such as mobile devices, smart TV, PC, etc. “Apple” added this feature to iPhones from iOS8 to protect users’ privacy.\nSo, how does a static MAC address cause some security issues? The first thing caught in my mind is this\nAccording to Edward Snowden, the National Security Agency has a system that tracks the movements of everyone in a city by monitoring the MAC addresses of their electronic devices. As a result of users being trackable by their devices’ MAC addresses, Apple has started using random MAC addresses in their iOS line of devices while scanning for networks.If random MAC addresses are not used, researchers have confirmed that it is possible to link a real identity to a particular wireless MAC address.\n-wikipedia (https://en.wikipedia.org/wiki/MAC_address)\nAs I said it is “Burned-in”, which means it never changes which network you connect, unlike IP address. Another possible attack is “Man-in-Middle” with ARP poisoning. I highly recommend you to read wikipedia article: ARP spoofing for a better understanding of ARP poisoning. IEEE group also recommends random MAC addresses for Wifi security. Read this article for more info\nFor Linux, soon will get this feature. But now, I made a script(init script: I know init scripts are not meant for this, but I made it anyway!) which changes the MAC address every time the machine boots. Not only on boot, but we can also change whenever we want with simple commands and can restore to the original or we can go one step further with cron job to schedule the script that changes the MAC address every 1 hour or 30 minutes (Depending on your need).\nIt is a shell script that uses macchanger, which executes every time the machine boots thus the interface gets a random MAC address every time.\nNOTE: The \u0026ldquo;macchanger\u0026rdquo; or any other script never changes the device’s actual MAC address which is burned on the interface, but macchanger creates a proxy which machines use this proxy MAC address for network communication\nHow to install? Install macchanger\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install macchanger -y Download and place changer script in /etc/init.d/\n$ wget -q -O /etc/init.d/changer https://goo.gl/tRfoJo Give executable permission\n$ sudo chmod +x /etc/init.d/changer Run update-rc.d\n$ sudo update-rc.d changer defaults Commands $ service changer restore # To restores original MAC MAC Address Restored 0X:XX:XX:27:d8:XX $ sudo service changer new # To assign a new MAC. Note that, interface will go down and up MAC Address Changed Successfully $ service changer show # To shows current MAC Current MAC: 08:00:0c:27:d8:39 NOTE:\nChange the interface in changer after you download, by default the interface is wlan0 There will be a network restart when you run service changer new or service changer restore Kali Linux’s latest version(kali-rolling) has this feature. While upgrading(apt-get install upgrade), there is a macchanger prompt asking to enable this feature. ","permalink":"https://veerendra2.github.io/mac-scrambling/","summary":"\u003cp\u003e“\u003cstrong\u003eMAC Address Scrambling\u003c/strong\u003e“- By the name itself we can understand, instead of using a burned-in address, the machine uses a random MAC address. The machine/device changes MAC addresses regularly to improve security.  MAC address is a 48-bit hexadecimal digit which is burned in every electronic device that has the capability of “connectivity” such as mobile devices, smart TV, PC, etc. “Apple” added this feature to iPhones from iOS8 to protect users’ privacy.\u003c/p\u003e","title":"MAC Address Scrambling in Linux"},{"content":"","permalink":"https://veerendra2.github.io/about/","summary":"","title":"About"},{"content":" Network Blogs Capturing Wireless LAN Packets on Ubuntu with tcpdump and Kismet Linux Bridging Phishing With a Rogue Wi-Fi Access Point Fast DDoS analyzer with sflow/netflow/mirror support China\u0026rsquo;s Man-on-the-Side Attack on GitHub SSH testing tool checks the configuration of given server accessible over internet Infinite possibilities with the Scapy Module An Illustrated Guide to the Kaminsky DNS Vulnerability A penetration tester’s guide to sub-domain enumeration A source for pcap files and malware samples Explanation of how https works Daniels Networking Blog Python for Network Engineers Fuzzing proprietary protocols with Scapy, radamsa and a handful of PCAPs How to Decrypt 802.11 Tutorials for Network Simulator \u0026ldquo;ns\u0026rdquo; The First Few Milliseconds of an HTTPS Connection HTTPS explained with carrier pigeons How To Run Your Own Mail Server Bettercap - MITM attack tool Tcpdump Examples Bluetooth BLEAH-A BLE scanner for \u0026ldquo;smart\u0026rdquo; devices hacking Proxying Bluetooth devices for security analysis using btproxy Nike+ FuelBand SE BLE Protocol Reversed Ubertooth - Can sniff some data from Basic Rate (BR) Bluetooth Classic connections. A Bluetooth LE interface for Python Github Projects Wireless MITM WPA attacks Framework for Rogue Wi-Fi Access Point Attack FruityWiFi is a wireless network auditing tool. Tool for sniffing unencrypted wireless probe requests from devices [Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing](Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing) This script creates a NATed or Bridged WiFi Access Point Kick devices off your network by performing an ARP Spoof attack A framework for wireless pentesting. Crack WPA/WPA2 Wi-Fi Routers with Airodump-ng and Aircrack-ng/Hashcat Bluetooth Proxy tool DNS DNS over HTTPS Analyze the security of any domain by finding all the information possible. Made in python Domain name permutation engine for detecting typo squatting, phishing and corporate espionage Selective DNS proxy forwarding based on DNS threat blocking providers intelligence DNS Enumeration Script Fast subdomains enumeration tool for penetration testers Open redirect subdomains scanner Analyze the security of any domain by finding all the information possible. Made in python A set of tools for performing reconnaissance on domain names Scan Scrapy, a fast high-level web crawling \u0026amp; scraping framework for Python TCP port scanner, spews SYN packets asynchronously, scanning entire Internet in under 5 minutes. Nikto web server scanner Scan .onion hidden services with nmap using Tor, proxychains and dnsmasq A small TOR Onion Address harvester for checking if the address is available or not. Tool for capturing and replaying live HTTP traffic into a test environment netdiscover Layer 2 network neighbourhood discovery tool that uses scapy SSL/TLS layers for scapy the interactive packet manipulation tool The python-based interactive packet manipulation program \u0026amp; library Python wrapper for tshark, allowing python packet parsing using wireshark dissectors Stores your data in ICMP ping packets TCP/IP packet demultiplexer Python binding of libnetfilter_conntrack Nipe is a script to make Tor Network your default gateway A reverse TCP tunnel let you access target behind NAT or firewall A Tunnel which Turns UDP Traffic into Encrypted UDP/FakeTCP/ICMP Traffic by using Raw Socket Search MAC Address Linux Blogs Boot Run Levels \u0026amp; How to make init scripts Realmode Assembly – Writing bootable stuff Making scripts run at boot time with Debian Writing a Bootloader init script template systemd, Beyond init-Youtube Talk Analyzing the Linux boot process [Write Simple OS from scratch [PDF]]({{ site.url }}/assets/os-dev.pdf) OS Development The little book about OS development How to Make a Computer Operating System Let\u0026rsquo;s Write a Kernel Information about the creation of operating systems Writing OS in Rust Commands htop explained visually with screenshot Explain Shell Learn VIM 30 interesting commands for the Linux shell An Introduction to Linux Permissions 3 Ways to Permanently and Securely Delete Two great uses for the cp command: Bash shortcuts Files Understanding and generating the hash stored in /etc/shadow What is setiud, setgid and sticky bit in Linux? /proc/sys/net/ipv4 /proc/net/tcp and /proc/net/tcp6 /dev/null /dev/full /dev/random /dev/urandom /dev/zero Warden.NET is an easy to use process management library for keeping track of processes on Windows. [Android Internals[PDF]]({{ \u0026ldquo;/assets/android_internals.pdf \u0026quot; | absolute_url }}) A list of reading materials for BPF How Linux CPU Usage Time and Percentage is calculated Removing Your PDF Metadata \u0026amp; Protecting PDF Files Linux Process Hunter Linux Memory Managment Frequently Asked Questions Attack Infrastructure Logging Virtualization Internals Github Projects A toolkit for creating efficient kernel tracing and manipulation programs Tool for in-depth analysis of USB HID devices communication From finding text to search and replace, from sorting to beautifying text and more Programmable completion functions for bash CheatSheets Bash Python Blog How to recover lost Python source code if it\u0026rsquo;s still resident in-memory Cpython Internals: Codewalk through the Python interpreter source codes [Youtube Playlist] Problem Solving with Algorithms and Data Structures using Python Natural Language Processing with Python Python Anti-Patterns Python Plays: Grand Theft Auto V https://pythonprogramming.net/ Pythonic Data Structures and Algorithms An automation tool that models a user’s actions on a terminal. Regx in easy way You Should Learn Regx Let’s Build A Simple Interpreter Python Excel Tutorial: The Definitive Guide C++ Data Structures [Scapy Docs [PDF]]({{ \u0026ldquo;/assets/scapydoc.pdf\u0026rdquo; | absolute_url }}) python-course.eu Github Projects Android Package Inspector - dynamic analysis with api hooks, start unexported activities and more. (Xposed Module) A public list of APIs from round the web. A Python toolbox for building complex digital hardware A collection of (mostly) technical things every software developer should know What happens when\u0026hellip; Command and Rule over your Shell Shutit-Automation framework for programmers A general-purpose fuzzer Minimal examples of data structures and algorithms in Python Python By Examples Security/Privacy Blogs Privacy What Is Intelligent Tracking Prevention and How Does It Work? The Truth About Online Privacy: How Your Data is Collected, Shared, and Sold What is Cookie Syncing and How Does it Work? Webpage tracking only using CSS (and no JS) How to Monitor Mobile App Traffic With Sniffers Python for PenTesters Intro to basic Disassembly \u0026amp; Reverse Engineering Python for Pentesters-pentesteracademy Start Your Own ISP Details of the implementation of Spectre, Attacking secure USB keys, behind the scene How to Install Tripwire IDS (Intrusion Detection System) on Linux Hacker101! Four Ways to Bypass Android SSL Verification and Certificate Pinning Tracing API calls in Burp with Frida Open Source CyberSecurity - n0where.net [Command Injection [PDF]]({{ \u0026ldquo;/assets/Command_Injection_Shell_Injection.pdf\u0026rdquo; | absolute_url }}) [Recon-ng Guid [PDF]]({{ \u0026ldquo;/assets/recon-ng-guide.pdf\u0026rdquo; | absolute_url }}) Android Applications Reversing 101 moveax.me The New zANTI: Mobile Penetration \u0026amp; Security Analysis Toolkit Reverse Engineering Reverse Engineering Basics A Primer Guide to Reverse Engineering Binary patching and intro to assembly with r2 Github Projects WhatsApp Discover Mobile App Pentest cheat sheet ","permalink":"https://veerendra2.github.io/bookmarks/","summary":"\u003cul\u003e\n\u003cli\u003e\n\u003ch2 id=\"network\"\u003eNetwork\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"blogs\"\u003eBlogs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sandilands.info/sgordon/capturing-wireless-lan-with-ubuntu-tcpdump-kismet\"\u003eCapturing Wireless LAN Packets on Ubuntu with tcpdump and Kismet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://goyalankit.com/blog/linux-bridge\"\u003eLinux Bridging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://xeushack.com/phishing-with-a-rogue-wifi-access-point/\"\u003ePhishing With a Rogue Wi-Fi Access Point\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://packages.debian.org/sid/fastnetmon\"\u003eFast DDoS analyzer with sflow/netflow/mirror support\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.netresec.com/?page=Blog\u0026month=2015-03\u0026post=China%27s-Man-on-the-Side-Attack-on-GitHub\"\u003eChina\u0026rsquo;s Man-on-the-Side Attack on GitHub\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sshcheck.com/\"\u003eSSH testing tool checks the configuration of given server accessible over internet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://bt3gl.github.io/black-hat-python-infinite-possibilities-with-the-scapy-module.html\"\u003eInfinite possibilities with the Scapy Module \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.unixwiz.net/techtips/iguide-kaminsky-dns-vuln.html\"\u003eAn Illustrated Guide to the Kaminsky DNS Vulnerability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.appsecco.com/a-penetration-testers-guide-to-sub-domain-enumeration-7d842d5570f6\"\u003eA penetration tester’s guide to sub-domain enumeration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.malware-traffic-analysis.net/\"\u003eA source for pcap files and malware samples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://dev.to/ruidfigueiredo/briefish-explanation-of-how-https-works\"\u003eExplanation of how https works \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://lostintransit.se/\"\u003eDaniels Networking Blog \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pynet.twb-tech.com/\"\u003ePython for Network Engineers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.blazeinfosec.com/fuzzing-proprietary-protocols-with-scapy-radamsa-and-a-handful-of-pcaps/\"\u003eFuzzing proprietary protocols with Scapy, radamsa and a handful of PCAPs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://wiki.wireshark.org/HowToDecrypt802.11\"\u003eHow to Decrypt 802.11\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.isi.edu/nsnam/ns/tutorial/\"\u003eTutorials for Network Simulator \u0026ldquo;ns\u0026rdquo;\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.moserware.com/2009/06/first-few-milliseconds-of-https.html\"\u003eThe First Few Milliseconds of an HTTPS Connection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://medium.freecodecamp.org/https-explained-with-carrier-pigeons-7029d2193351\"\u003eHTTPS explained with carrier pigeons\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.c0ffee.net/blog/mail-server-guide\"\u003eHow To Run Your Own Mail Server\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.bettercap.org/\"\u003eBettercap -  MITM attack tool\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://hackertarget.com/tcpdump-examples/\"\u003eTcpdump Examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ch4 id=\"bluetooth\"\u003eBluetooth\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.evilsocket.net/2017/09/23/This-is-not-a-post-about-BLE-introducing-BLEAH/\"\u003eBLEAH-A BLE scanner for \u0026ldquo;smart\u0026rdquo; devices hacking\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://conorpp.com/proxying-bluetooth-devices-for-security-analysis-using-btproxy\"\u003eProxying Bluetooth devices for security analysis using btproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.evilsocket.net/2015/01/29/nike-fuelband-se-ble-protocol-reversed/\"\u003eNike+ FuelBand SE BLE Protocol Reversed\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://ubertooth.sourceforge.net/\"\u003eUbertooth - Can sniff some data from Basic Rate (BR) Bluetooth Classic connections.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ianharvey.github.io/bluepy-doc/\"\u003eA Bluetooth LE interface for Python\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"github-projects\"\u003eGithub Projects\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch5 id=\"wireless\"\u003eWireless\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/wi-fi-analyzer/fluxion\"\u003eMITM WPA attacks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/P0cL4bs/WiFi-Pumpkin\"\u003eFramework for Rogue Wi-Fi Access Point Attack \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/xtr4nge/FruityWifi\"\u003eFruityWiFi is a wireless network auditing tool.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/xdavidhu/probeSniffer\"\u003eTool for sniffing unencrypted wireless probe requests from devices\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing](Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/oblique/create_ap\"\u003eThis script creates a NATed or Bridged WiFi Access Point\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/k4m4/kickthemout\"\u003eKick devices off your network by performing an ARP Spoof attack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Tylous/SniffAir\"\u003eA framework for wireless pentesting. \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/brannondorsey/wifi-cracking\"\u003eCrack WPA/WPA2 Wi-Fi Routers with Airodump-ng and Aircrack-ng/Hashcat\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/conorpp/btproxy\"\u003eBluetooth Proxy tool\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch5 id=\"dns\"\u003eDNS\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/curl/curl/wiki/DNS-over-HTTPS\"\u003eDNS over HTTPS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/eldraco/domain_analyzer\"\u003eAnalyze the security of any domain by finding all the information possible. Made in python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/elceef/dnstwist\"\u003eDomain name permutation engine for detecting typo squatting, phishing and corporate espionage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/NavyTitanium/DNSMasterChef\"\u003eSelective DNS proxy forwarding based on DNS threat blocking providers intelligence\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/darkoperator/dnsrecon\"\u003eDNS Enumeration Script \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aboul3la/Sublist3r\"\u003eFast subdomains enumeration tool for penetration testers \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ak1t4/open-redirect-scanner\"\u003eOpen redirect subdomains scanner\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/eldraco/domain_analyzer\"\u003eAnalyze the security of any domain by finding all the information possible. Made in python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/michenriksen/aquatone\"\u003eA set of tools for performing reconnaissance on domain names\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch5 id=\"scan\"\u003eScan\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/scrapy/scrapy\"\u003eScrapy, a fast high-level web crawling \u0026amp; scraping framework for Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/robertdavidgraham/masscan\"\u003eTCP port scanner, spews SYN packets asynchronously, scanning entire Internet in under 5 minutes. \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/sullo/nikto\"\u003eNikto web server scanner \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/milesrichardson/docker-onion-nmap\"\u003eScan .onion hidden services with nmap using Tor, proxychains and dnsmasq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mirsamantajbakhsh/OnionHarvester\"\u003eA small TOR Onion Address harvester for checking if the address is available or not.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/buger/goreplay\"\u003eTool for capturing and replaying live HTTP traffic into a test environment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/alexxy/netdiscover\"\u003enetdiscover\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/bwaldvogel/neighbourhood\"\u003eLayer 2 network neighbourhood discovery tool that uses scapy \u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tintinweb/scapy-ssl_tls\"\u003eSSL/TLS layers for scapy the interactive packet manipulation tool\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/secdev/scapy\"\u003eThe python-based interactive packet manipulation program \u0026amp; library\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/KimiNewt/pyshark\"\u003ePython wrapper for tshark, allowing python packet parsing using wireshark dissectors\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/yarrick/pingfs\"\u003eStores your data in ICMP ping packets \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/simsong/tcpflow\"\u003eTCP/IP packet demultiplexer \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/regit/pynetfilter_conntrack\"\u003ePython binding of libnetfilter_conntrack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/GouveaHeitor/nipe\"\u003eNipe is a script to make Tor Network your default gateway\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aploium/shootback\"\u003eA reverse TCP tunnel let you access target behind NAT or firewall \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/wangyu-/udp2raw-tunnel\"\u003eA Tunnel which Turns UDP Traffic into Encrypted UDP/FakeTCP/ICMP Traffic by using Raw Socket\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://regauth.standards.ieee.org/standards-ra-web/pub/view.html#registries\"\u003eSearch MAC Address\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch2 id=\"linux\"\u003eLinux\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"blogs-1\"\u003eBlogs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch5 id=\"boot\"\u003eBoot\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.tldp.org/HOWTO/HighQuality-Apps-HOWTO/boot.html#boot.runlevel\"\u003eRun Levels \u0026amp; How to make init scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://0x00sec.org/t/realmode-assembly-writing-bootable-stuff-part-5/3667\"\u003eRealmode Assembly – Writing bootable stuff\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://debian-administration.org/article/28/Making_scripts_run_at_boot_time_with_Debian\"\u003eMaking scripts run at boot time with Debian\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://3zanders.co.uk/2017/10/13/writing-a-bootloader/\"\u003eWriting a Bootloader \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/fhd/init-script-template\"\u003einit script template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=TyMLi8QF6sw\"\u003esystemd, Beyond init-Youtube Talk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://opensource.com/article/18/1/analyzing-linux-boot-process\"\u003eAnalyzing the Linux boot process\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[Write Simple OS from scratch [PDF]]({{ site.url }}/assets/os-dev.pdf)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch5 id=\"os-development\"\u003eOS Development\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://littleosbook.github.io/#introduction\"\u003eThe little book about OS development\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://samypesse.gitbooks.io/how-to-create-an-operating-system/\"\u003eHow to Make a Computer Operating System\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://arjunsreedharan.org/post/82710718100/kernel-101-lets-write-a-kernel\"\u003eLet\u0026rsquo;s Write a Kernel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://wiki.osdev.org/Main_Page\"\u003eInformation about the creation of operating systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://os.phil-opp.com/\"\u003eWriting OS in Rust\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch5 id=\"commands\"\u003eCommands\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://codeahoy.com/2017/01/20/hhtop-explained-visually/\"\u003ehtop explained visually with screenshot\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://explainshell.com/\"\u003eExplain Shell\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://vim-adventures.com/\"\u003eLearn VIM\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.lopezferrando.com/30-interesting-shell-commands/\"\u003e30 interesting commands for the Linux shell\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.digitalocean.com/community/tutorials/an-introduction-to-linux-permissions\"\u003eAn Introduction to Linux Permissions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.tecmint.com/permanently-and-securely-delete-files-directories-linux/\"\u003e3 Ways to Permanently and Securely Delete \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://opensource.com/article/18/1/two-great-uses-cp-command-update\"\u003eTwo great uses for the cp command: Bash shortcuts \u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch5 id=\"files\"\u003eFiles\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.aychedee.com/2012/03/14/etc_shadow-password-hash-formats/\"\u003eUnderstanding and generating the hash stored in /etc/shadow\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.oracle.com/cd/E19683-01/806-4078/secfiles-69/index.html\"\u003eWhat is setiud, setgid and sticky bit in Linux?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\"\u003e/proc/sys/net/ipv4\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kernel.org/doc/Documentation/networking/proc_net_tcp.txt\"\u003e/proc/net/tcp and /proc/net/tcp6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://man7.org/linux/man-pages/man4/null.4.html\"\u003e/dev/null\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://man7.org/linux/man-pages/man4/full.4.html\"\u003e/dev/full\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://man7.org/linux/man-pages/man4/random.4.html\"\u003e/dev/random\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://man7.org/linux/man-pages/man4/random.4.html\"\u003e/dev/urandom\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://unix.stackexchange.com/questions/254384/difference-between-dev-null-and-dev-zero\"\u003e/dev/zero\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/RainwayApp/warden\"\u003eWarden.NET is an easy to use process management library for keeping track of processes on Windows. \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[Android Internals[PDF]]({{ \u0026ldquo;/assets/android_internals.pdf \u0026quot; | absolute_url }})\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://qmonnet.github.io/whirl-offload/2016/09/01/dive-into-bpf/\"\u003eA list of reading materials for BPF\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Leo-G/DevopsWiki/wiki/How-Linux-CPU-Usage-Time-and-Percentage-is-calculated\"\u003eHow Linux CPU Usage Time and Percentage is calculated\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.joshlemon.com.au/protecting-your-pdf-files-and-metadata/\"\u003eRemoving Your PDF Metadata \u0026amp; Protecting PDF Files\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/nowayout/prochunter\"\u003eLinux Process Hunter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://landley.net/writing/memory-faq.txt\"\u003eLinux Memory Managment Frequently Asked Questions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://thevivi.net/2018/03/23/attack-infrastructure-logging-part-1-logging-server-setup/\"\u003eAttack Infrastructure Logging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://saferwall.com/blog/virtualization-internals-part-1-intro-to-virtualization\"\u003eVirtualization Internals\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"github-projects-1\"\u003eGithub Projects\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/iovisor/bcc\"\u003eA toolkit for creating efficient kernel tracing and manipulation programs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ondrejbudai/hidviz/\"\u003eTool for in-depth analysis of USB HID devices communication\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/learnbyexample/Command-line-text-processing\"\u003eFrom finding text to search and replace, from sorting to beautifying text and more\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/scop/bash-completion\"\u003eProgrammable completion functions for bash \u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"cheatsheets\"\u003eCheatSheets\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Idnan/bash-guide\"\u003eBash\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch2 id=\"python\"\u003ePython\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"blog\"\u003eBlog\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gist.github.com/simonw/8aa492e59265c1a021f5c5618f9e6b12\"\u003eHow to recover lost Python source code if it\u0026rsquo;s still resident in-memory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLzV58Zm8FuBL6OAv1Yu6AwXZrnsFbbR0S\"\u003eCpython Internals: Codewalk through the Python interpreter source codes [Youtube Playlist]\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://interactivepython.org/courselib/static/pythonds/index.html\"\u003eProblem Solving with Algorithms and Data Structures using Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.nltk.org/book/?=\"\u003eNatural Language Processing with Python\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.quantifiedcode.com/python-anti-patterns/latex/The-Little-Book-of-Python-Anti-Patterns-1.0.pdf\"\u003ePython Anti-Patterns\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PLQVvvaa0QuDeETZEOy4VdocT7TOjfSA8a\"\u003ePython Plays: Grand Theft Auto V\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pythonprogramming.net/\"\u003ehttps://pythonprogramming.net/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/keon/algorithms\"\u003ePythonic Data Structures and Algorithms\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://learnxinyminutes.com/docs/shutit/\"\u003eAn automation tool that models a user’s actions on a terminal.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/zeeshanu/learn-regex/blob/master/README.md\"\u003eRegx in easy way\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.patricktriest.com/you-should-learn-regex/\"\u003eYou Should Learn Regx\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ruslanspivak.com/lsbasi-part1/\"\u003eLet’s Build A Simple Interpreter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.datacamp.com/community/tutorials/python-excel-tutorial\"\u003ePython Excel Tutorial: The Definitive Guide\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://datastructures.js.org/#/home\"\u003eC++ Data Structures\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[Scapy Docs [PDF]]({{ \u0026ldquo;/assets/scapydoc.pdf\u0026rdquo; | absolute_url }})\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.python-course.eu\"\u003epython-course.eu\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"github-projects-2\"\u003eGithub Projects\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ac-pm/Inspeckage\"\u003eAndroid Package Inspector - dynamic analysis with api hooks, start unexported activities and more. (Xposed Module) \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abhishekbanthia/Public-APIs\"\u003eA public list of APIs from round the web. \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/m-labs/migen\"\u003eA Python toolbox for building complex digital hardware\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mtdvio/every-programmer-should-know\"\u003eA collection of (mostly) technical things every software developer should know\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/alex/what-happens-when\"\u003eWhat happens when\u0026hellip;\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aeroxis/sultan\"\u003eCommand and Rule over your Shell\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ianmiell/shutit\"\u003eShutit-Automation framework for programmers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/aoh/radamsa\"\u003eA general-purpose fuzzer \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/keon/algorithms\"\u003eMinimal examples of data structures and algorithms in Python \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dougsland/python-by-examples\"\u003ePython By Examples\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch2 id=\"securityprivacy\"\u003eSecurity/Privacy\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch3 id=\"blogs-2\"\u003eBlogs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ch6 id=\"privacy\"\u003ePrivacy\u003c/h6\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://clearcode.cc/blog/intelligent-tracking-prevention/\"\u003eWhat Is Intelligent Tracking Prevention and How Does It Work?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://clearcode.cc/blog/online-privacy-user-data/\"\u003eThe Truth About Online Privacy: How Your Data is Collected, Shared, and Sold\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://clearcode.cc/blog/cookie-syncing/\"\u003eWhat is Cookie Syncing and How Does it Work?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jbtronics/CrookedStyleSheets\"\u003eWebpage tracking only using CSS (and no JS) \u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://stanfy.com/blog/monitor-mobile-app-traffic-with-sniffers/\"\u003eHow to Monitor Mobile App Traffic With Sniffers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://vulnerablelife.wordpress.com/2017/05/13/python-for-penetration-testers/\"\u003ePython for PenTesters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.perspectiverisk.com/intro-to-basic-disassembly-reverse-engineering/\"\u003eIntro to basic Disassembly \u0026amp; Reverse Engineering\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://www.pentesteracademy.com/course?id=1\"\u003ePython for Pentesters-pentesteracademy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://startyourownisp.com/\"\u003eStart Your Own ISP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.fortinet.com/2018/01/17/into-the-implementation-of-spectre\"\u003eDetails of the implementation of Spectre,\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.j-michel.org/blog/2018/01/16/attacking-secure-usb-keys-behind-the-scene\"\u003eAttacking secure USB keys, behind the scene\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.tecmint.com/install-tripwire-ids-intrusion-detection-system-on-linux/\"\u003eHow to Install Tripwire IDS (Intrusion Detection System) on Linux\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.hacker101.com/\"\u003eHacker101!\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.netspi.com/four-ways-bypass-android-ssl-verification-certificate-pinning/\"\u003eFour Ways to Bypass Android SSL Verification and Certificate Pinning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cedricvb.be/post/tracing-api-calls-in-burp-with-frida/\"\u003eTracing API calls in Burp with Frida\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://n0where.net/\"\u003eOpen Source CyberSecurity - n0where.net\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e[Command Injection [PDF]]({{ \u0026ldquo;/assets/Command_Injection_Shell_Injection.pdf\u0026rdquo; | absolute_url }})\u003c/li\u003e\n\u003cli\u003e[Recon-ng Guid [PDF]]({{ \u0026ldquo;/assets/recon-ng-guide.pdf\u0026rdquo; | absolute_url }})\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.evilsocket.net/2017/04/27/Android-Applications-Reversing-101/\"\u003eAndroid Applications Reversing 101\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://moveax.me/\"\u003emoveax.me\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blog.zimperium.com/zanti-cyber-threat-detection/\"\u003eThe New zANTI: Mobile Penetration \u0026amp; Security Analysis Toolkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ch6 id=\"reverse-engineering\"\u003eReverse Engineering\u003c/h6\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://scriptdotsh.com/index.php/2018/04/09/ground-zero-part-1-reverse-engineering-basics/\"\u003eReverse Engineering Basics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://puri.sm/posts/primer-to-reverse-engineering/\"\u003eA Primer Guide to Reverse Engineering \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.leungs.xyz/reversing/2018/06/18/radare2-binary-patching-introduction.html\"\u003eBinary patching and intro to assembly with r2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ch3 id=\"github-projects-3\"\u003eGithub Projects\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/deepakdaswani/whatsapp_discover\"\u003eWhatsApp Discover\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tanprathan/MobileApp-Pentest-Cheatsheet\"\u003eMobile App Pentest cheat sheet\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"My Bookmarks"}]