[{"content":"There is a cool feature in docker called userns-remap, discovered while doing my RaspberryPi home server project; 15#issuecomment-1296311979, I can just enable userns-remap and docker does all remapping of uid and gid inside docker container to a non-root user on the host.\nhttps://docs.docker.com/engine/security/userns-remap/\nHow to enable *It is better to reinstall docker and remove all existing docker volumes\nAdd below /etc/docker/daemon.json { \u0026#34;userns-remap\u0026#34;: \u0026#34;default\u0026#34; } Restart the docker daemon $ sudo systemctl restart docker Ansible automation here In-Action # Run the Nginx container $ docker run -it -d nginx # Inside, the process thinks it is running as root! veerendra@atom:~$ docker exec -it nginx whoami root # But outside(on host namespace), the process running it as non-root user veerendra@atom:~$ ps aux | grep nginx 165536 350093 0.0 0.0 6320 4688 ? Ss 03:21 0:00 nginx: master process nginx -g daemon off; 165637 350208 0.0 0.0 6788 4288 ? S 03:21 0:01 nginx: worker process 165637 350209 0.0 0.0 6784 4284 ? S 03:21 0:00 nginx: worker process 165637 350210 0.0 0.0 6784 4284 ? S 03:21 0:01 nginx: worker process 165637 350212 0.0 0.0 6784 4284 ? S 03:21 0:01 nginx: worker process veerend+ 937492 0.0 0.0 6420 1844 pts/0 S+ 16:22 0:00 grep --color=auto nginx As you can see I have not specified any user while deploying the container, but the user inside the container is isolated i.e remapped to a non-root user(uid:165637, gid:165637) on the host\nI think it is very helpful and easy, instead of creating a gid and uid and specifying in docker-compose(or docker CLI).\nLimitation It has some limitation, as the docs says\nSharing PID or NET namespaces with the host (\u0026ndash;pid=host or \u0026ndash;network=host). External (volume or storage) drivers which are unaware or incapable of using daemon user mappings. Using the \u0026ndash;privileged mode flag on docker run without also specifying \u0026ndash;userns=host. When I tried to use filebrowser, the file permissions were very unpredictable and I was not able to access mounted partitions.\n","permalink":"https://veerendra2.github.io/docker-userns-remap/","summary":"There is a cool feature in docker called userns-remap, discovered while doing my RaspberryPi home server project; 15#issuecomment-1296311979, I can just enable userns-remap and docker does all remapping of uid and gid inside docker container to a non-root user on the host.\nhttps://docs.docker.com/engine/security/userns-remap/\nHow to enable *It is better to reinstall docker and remove all existing docker volumes\nAdd below /etc/docker/daemon.json { \u0026#34;userns-remap\u0026#34;: \u0026#34;default\u0026#34; } Restart the docker daemon $ sudo systemctl restart docker Ansible automation here In-Action # Run the Nginx container $ docker run -it -d nginx # Inside, the process thinks it is running as root!","title":"User Namespace Isolation in Docker"},{"content":"Introduction I have been working on a RaspberryPi home server project for quite some time. The project is a collection of applications to run on RaspberryPi and all applications are deployable with docker-compose files and ansible automation. One of the applications I was configuring is Pi-hole, a network-wide ad-blocker.\nI decided to use Pi-hole as also DHCP server for my LAN. When I look into docs, it says it has to be run as network_mode: host, because it allows Pi-hole to listen to DHCP broadcast packets. If the Pi-hole deployed in bridge mode, there is a Linux bridge(Think of it as a router for a second!) which won\u0026rsquo;t allow broadcast packets.\nBut I want to run the Pi-hole container isolated and non-root, besides I\u0026rsquo;m using Nginx proxy for all of my apps(Check wiki pages). I kept scrolling docs, found this which I can use DHCP relay.\nA DHCP relay listens to DHCP broadcast packets and unicasts to the DHCP server which is in another network(We will see how it works below section)\nWith help of DerFetzer\u0026rsquo;s I have created a simple project where you can deploy Pi-hole with DHCP relay.\n$ git clone https://github.com/veerendra2/pihole-dhcp-relay-docker $ cd pihole-DHCP-relay-docker $ tree . . ‚îú‚îÄ‚îÄ dhcp-helper ‚îÇ ‚îî‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ dnsmasq.d ‚îÇ ‚îú‚îÄ‚îÄ 07-dhcp-options.conf ‚îÇ ‚îî‚îÄ‚îÄ 99-temp.conf ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ LICENSE ‚îú‚îÄ‚îÄ pihole ‚îÇ ‚îî‚îÄ‚îÄ custom.list ‚îî‚îÄ‚îÄ README.md 3 directories, 7 files How it works Here is the simple network diagram that you can see the definition indocker-compose.yml\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ dhcp-relay-net‚îÇ \u0026lt;-------- Linux bridge ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò ‚îÇ ‚îÇeth1 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê ‚îÇDHCP ‚îÇ ‚îÇPiHole ‚îÇ ‚îÇRelay ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇeth0 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ front-tier ‚îÇ \u0026lt;-- Linux bridge ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Host Network ‚îÇ | (RaspberryPi) | ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ^ eth0 (192.168.0.120) | | v LAN As you can see above the Pi-hole is deployed in bridge mode, there is a Linux bridge(front-tier) between the host network stack and the Pi-hole container. So, any broadcasts won\u0026rsquo;t allow through front-tier bridge(router*). So, I solve the problem by adding one more container DHCP Relay run as network_mode: host in which the container is attached to the host network stack as you can see there is no Linux bridge which means, DHCP Relay container listens all DHCP broadcast and unicast to dhcp-relay-net Linux bridge and Pi-hole also connected to it. That\u0026rsquo;s how devices in LAN get the IP from bridged Pi-hole\nFurther Configurations DHCP Options Config After I deploy the above setup, my phone gets an IP address but not the Internet. I wasn\u0026rsquo;t sure what was going on, maybe it is that DHCP is not providing the DNS address which is my RaspberryPI external LAN IP?\nI installed Wireshark and capture the DHCP packets to see what it contains\nBing go!, it is 172.31.0.10 the IP I specified in DHCP relay. So, after googling, I found\n$ cat dnsmasq.d/07-dhcp-options.conf dhcp-option=option:dns-server,192.168.0.120 Then, my phone gets the correct DNS address and accesses the Internet.\nSuppress dnsmasq warnings I noticed, in the Pi-hole dashboard, dnsmasq is throwing warnings\nThat is due to the IP range I set in the DHCP pool(.env) doesn\u0026rsquo;t cover the network 172.31.0.0/16 which Pi-hole directly connect with eth1 interface(Above diagram). In this case, there are 2 options to get rid of these warnings.\nDisable warnings Like in the below screenshots, but you might miss any other important warnings\nNo DHCP Interface Set no-dhcp-interfaces like below\n$ cat dnsmasq.d/99-temp.conf no-dhcp-interface=eth0,eth1 Conclusion This simple trick allows me to run Pi-host in bridge mode and run behind the Nginx proxy. Hope you find a useful piece of information\n","permalink":"https://veerendra2.github.io/pihole-dhcp-relay/","summary":"Introduction I have been working on a RaspberryPi home server project for quite some time. The project is a collection of applications to run on RaspberryPi and all applications are deployable with docker-compose files and ansible automation. One of the applications I was configuring is Pi-hole, a network-wide ad-blocker.\nI decided to use Pi-hole as also DHCP server for my LAN. When I look into docs, it says it has to be run as network_mode: host, because it allows Pi-hole to listen to DHCP broadcast packets.","title":"Pi-hole with DHCP Relay in Docker"},{"content":"Introduction Hello my dear fellow humans, hope you are having a great day. Today\u0026rsquo;s guide is on how to recover from a disaster for Strimzi Kafka with Velero. First of all, what is Strmzi Kafka?\nhttps://strimzi.io\nStrimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\nBack in a while I worked on Strimzi Kafka deployment on Openshift, very easy to set up and manage production level Kafka cluster on kubernetes, I have to give credit to Strimzi project team, did a great job on documentation, support on Github discussions and active developments.\nOne of the important things in IT, when you bring new tech into a team, it should be disaster recovery proof. The team should able to recover data and bring back to normal state after a disaster. For strimzi, it is very easy once you set up backend PV as dynamic storage class provisioner as you can 11.6. Recovering a cluster from persistent volumes. In this guide, I use the velero tool to make things automated and can easily set up in GitOps.\nHeads-up ‚úã Before dive into this guide, I want to make you aware on my existing setup and scope\nStrimzi Kafka stateful app is up and running on Kubernetes cluster with backend PV storage class provisioner as AzureDisk. Velero server up and running. Check my previous post \u0026ldquo;Velero Deployment with Kustomize (Azure)\u0026rdquo; to know how to set up. This guide does not cover basics of Kafka or how to set up Strimzi Kafka on kubernetes, head over to Strimzi Github project page and browse resources. But for the sake of this guide I create a demo repo contains Strimzi Kafka deployment files üëâ https://github.com/veerendra2/strimzi-kafka-demo\nScenario This is the simple test scenario I picked to test recovery steps. The test is to produce logs/messages to Kafka brokers and read the produced logs/messages from Kafka brokers after recovery. If the recovery is successful, the consumer should able to fetch logs/messages after recovery as you can see in below diagram\nAssume below are events that happen over time.\nProducer produced some logs/messages to Kafka system Kafka brokers receives logs/messages and stores on disk Disaster happens(Refer \u0026ldquo;Disaster Simulation\u0026rdquo; to know how to simulate) Recovery (Refer \u0026ldquo;Recovery Plan\u0026rdquo; to know how to recover) Kafka recovered from disaster. Up and running Consumers now consume logs/messages which are produced in above point 1. If recovery is successful, the consumer should be able to fetch logs/messages there were stored in Kafka system before disaster. Disaster Simulation Before performing disaster simulation, Kafka cluster should be up and running, there should be some data generated on the cluster. In the prerequisites section, we will see how to prepare for disaster.\nCluster preparation Velero CLI tool should be installed on your local machine.\n$ git clone https://github.com/veerendra2/strimzi-kafka-demo $ cd strimzi-kafka-demo $ kubectl create -f base/namespace.yaml $ kubectl project kafka $ kubectl create -f base/cluster-operator.yaml $ kubectl create -f base/configmaps/ $ kubectl create -f stages/dev/deployment.yaml $ kubectl create -f stages/dev/topics.yaml $ kubectl create -f stages/dev/users.yaml Wait until kafka cluster bootstrap and verify everything is running\n$ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE carbon-dev-entity-operator-df5h6497-6xmqf 3/3 Running 0 4m carbon-dev-kafka-0 1/1 Running 0 6m carbon-dev-kafka-1 1/1 Running 0 6m carbon-dev-kafka-2 1/1 Running 0 6m carbon-dev-kafka-exporter-657956b4a-zpmdg 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-658y5cf364-tw2nh 1/1 Running 0 2h $ kubectl get pvc -n kafka NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-0-carbon-dev-kafka-0 Bound pvc-8092d619-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-0-carbon-dev-kafka-1 Bound pvc-8093210e-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-0-carbon-dev-kafka-2 Bound pvc-8093c74a-4883-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 6m data-carbon-dev-zookeeper-0 Bound pvc-4105d01a-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m data-carbon-dev-zookeeper-1 Bound pvc-4106f177-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m data-carbon-dev-zookeeper-2 Bound pvc-41072398-4883-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 7m $ kubectl get kafkatopic -n kafka NAME CLUSTER PARTITIONS REPLICATION FACTOR my-topic carbon-dev 1 1 $ kubectl get kafkauser -n kafka NAME CLUSTER AUTHENTICATION AUTHORIZATION my-user carbon-dev tls simple Deploy sample producer app to produce logs/messages to Kafka\n## Check deployment config. For example, broker bootstrap route, topic name and user name. Deploy consumer test app $ kubectl create -f kafka-producer.yaml deployment.apps/java-kafka-producer created $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE java-kafka-producer-bfd975945-cnmgg 0/1 Completed 1 42s carbon-dev-entity-operator-df5h6497-6xmqf 3/3 Running 0 4m carbon-dev-kafka-0 1/1 Running 0 6m carbon-dev-kafka-1 1/1 Running 0 6m carbon-dev-kafka-2 1/1 Running 0 6m carbon-dev-kafka-exporter-657956b4a-zpmdg 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-658y5cf364-tw2nh 1/1 Running 0 2h $ kubectl logs java-kafka-producer-bfd975945-cnmgg -n kafka ... 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 997\u0026#34; \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----------------THESE LOG LINES WE NEED TO LOOK TO KNOW PRODUCER SENT TO KAFKA 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 998\u0026#34; 2022-08-18 15:29:33 INFO KafkaProducerExample:69 - Sending messages \u0026#34;Hello world - 999\u0026#34; 2022-08-18 15:29:33 INFO KafkaProducerExample:91 - 1000 messages sent ... 2994 [main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter 3008 [main] INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed 3009 [main] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered [2022-08-18T15:29:33.852+0000] Heap [2022-08-18T15:29:33.852+0000] def new generation total 150016K, used 32262K [0x0000000619400000, 0x00000006236c0000, 0x00000006bb800000) [2022-08-18T15:29:33.852+0000] eden space 133376K, 24% used [0x0000000619400000, 0x000000061b3819d0, 0x0000000621640000) [2022-08-18T15:29:33.852+0000] from space 16640K, 0% used [0x0000000621640000, 0x0000000621640000, 0x0000000622680000) [2022-08-18T15:29:33.852+0000] to space 16640K, 0% used [0x0000000622680000, 0x0000000622680000, 0x00000006236c0000) [2022-08-18T15:29:33.852+0000] tenured generation total 333184K, used 7580K [0x00000006bb800000, 0x00000006cfd60000, 0x0000000800000000) [2022-08-18T15:29:33.852+0000] the space 333184K, 2% used [0x00000006bb800000, 0x00000006bbf672b0, 0x00000006bbf67400, 0x00000006cfd60000) [2022-08-18T15:29:33.852+0000] Metaspace used 23126K, capacity 23940K, committed 24192K, reserved 1071104K [2022-08-18T15:29:33.852+0000] class space used 2563K, capacity 2885K, committed 2944K, reserved 1048576K Configure backup Once Kafka is loaded with sample data, configure backup like below.\n# Velero binary uses a local kubeconfig file to manage velero deployment. So, before running velero, login into cluster # Run one time backup for the test scenario $ velero backup create kafka-backup --include-namespaces=kafka --include-resources persistentvolumeclaims, persistentvolumes Backup request \u0026#34;kafka-backup\u0026#34; submitted successfully. Run `velero backup describe kafka-backup` or `velero backup logs kafka-backup` for more details. ## Verify backup is \u0026#34;Completed\u0026#34; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup Completed 2021-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Destroy Delete kafka resources in kafka namespace\n## Note down below PVs $ kubectl get pvc -n kafka | awk \u0026#39;{print $3}\u0026#39; | tail -n+2 pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f pvc-9aadbaf8-4afa-11ec-9048-12rd2ab3g21f pvc-9aadd6b8-4afa-11ec-9048-12rd2ab3g21f pvc-6d3118cf-4afa-11ec-9048-12rd2ab3g21f pvc-6d3172c1-4afa-11ec-9048-12rd2ab3g21f pvc-6d3176b9-4afa-11ec-9048-12rd2ab3g21f ## Delete kafka cluster, PVC and namesapce $ kubectl delete kafka `kubectl get kafka -n kafka | awk \u0026#39;{print $1}\u0026#39; | tail -n+2` $ kubectl delete pv `kubectl get pvc -n kafka | awk \u0026#39;{print $3}\u0026#39; | tail -n+2` $ kubectl delete pvc `kubectl get pvc -n kafka | awk \u0026#39;{print $1}\u0026#39; | tail -n+2` $ kubectl delete -f base/cluster-operator.yaml $ kubectl delete namespace kafka Delete disks in cloud provider portal UI to make disaster more solid if required\nRecovery Steps Preparation Velero CLI should be installed on your local machine (Refere Basic Install). All strimzi deployments files should be exactly the same as before the disaster. Check which backups you want to restore\n# Login into cluster $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup Completed 2021-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Restore disk In below example uses \u0026ldquo;kafka-backup\u0026rdquo; backup to restore from it\n$ velero restore create --from-backup kafka-backup Restore request \u0026#34;kafka-backup\u0026#34; submitted successfully. Run `velero restore describe kafka-backup` or `velero restore logs kafka-backup` for more details. # Wait until the restore completed $ velero restore describe kafka-backup Name: kafka-backup Namespace: velero Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Phase: InProgress \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;---STATUS Backup: kafka-backup Namespaces: Included: all namespaces found in the backup Excluded: \u0026lt;none\u0026gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io Cluster-scoped: auto Namespace mappings: \u0026lt;none\u0026gt; Label selector: \u0026lt;none\u0026gt; Restore PVs: auto Once restore is completed, check PVCs created.\n‚ùó After restore completed, the PV \u0026ldquo;names\u0026rdquo; will same as during backup, but underneath the actual disk name is different in Azure cloud which you can see in below snippet\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-0-carbon-dev-kafka-0 Bound pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-0-carbon-dev-kafka-1 Bound pvc-9aadbaf8-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-0-carbon-dev-kafka-2 Bound pvc-9aadd6b8-4afa-11ec-9048-12rd2ab3g21f 256Gi RWO generic-retain 3m data-carbon-dev-zookeeper-0 Bound pvc-6d3118cf-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m data-carbon-dev-zookeeper-1 Bound pvc-6d3172c1-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m data-carbon-dev-zookeeper-2 Bound pvc-6d3176b9-4afa-11ec-9048-12rd2ab3g21f 64Gi RWO generic-retain 3m $ kubectl describe pv pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f Name: pvc-9aad8969-4afa-11ec-9048-12rd2ab3g21f Labels: velero.io/backup-name=kafka-backup velero.io/restore-name=kafka-backup Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: generic-retain Status: Bound Claim: kafka/data-0-carbon-dev-kafka-0 Reclaim Policy: Retain Access Modes: RWO Capacity: 256Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: restore-1b8257d7-2156-44ae-aa30-36f2gc6c64r6 ## \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----DISK NAME IS DIFFERENT AFTER RESTORE DiskURI: /subscriptions/fd667dc8-1e9e-4ab4-b428-9879d458e8ac/resourceGroups/carbondev-openshift/providers/Microsoft.Compute/disks/restore-1b8257d7-2156-44aeaa30-36f2gc6c64r6 Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u0026lt;none\u0026gt; Re-deploy Strimzi Kafka to recover From strimzi docs; 10.5.3. Recovering a deleted cluster from persistent volumes\nYou can recover a Kafka cluster from persistent volumes (PVs) if they are still present. You might want to do this, for example, after:\nA namespace was deleted unintentionally A whole Kubernetes cluster is lost, but the PVs remain in the infrastructure In our deployment, we use jbod config, the generate PVC name should be data-0-[CLUSTER-NAME]-kafka-0\n1. Bring up cluster-operator $ git clone https://github.com/veerendra2/strimzi-kafka-demo $ cd strimzi-kafka-demo $ kubectl create -f base/namepsace.yaml $ kubectl create -f base/cluster-operator.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-5c8d5cf966-dhhws 1/1 Running 0 2m 2. Deploy topics ‚ö†Ô∏è If you deploy topic-operator before deploying topics, the topic-operator deletes existing topics while bootstrapping. That\u0026rsquo;s why we need to deploy topics first\n$ kubectl create -f stages/dev/topics.yaml 3. Deploy Kafka cluster and users $ kubectl create -f stages/dev/deployment.yaml kafka.kafka.strimzi.io/carbon-dev created $ kubectl get pods NAME READY STATUS RESTARTS AGE carbon-dev-entity-operator-cd54f496-vmsfm 3/3 Running 0 3m carbon-dev-kafka-0 1/1 Running 0 5m carbon-dev-kafka-1 1/1 Running 0 5m carbon-dev-kafka-2 1/1 Running 0 5m carbon-dev-kafka-exporter-c67976b6b-vzdff 1/1 Running 0 3m carbon-dev-zookeeper-0 1/1 Running 0 7m carbon-dev-zookeeper-1 1/1 Running 0 7m carbon-dev-zookeeper-2 1/1 Running 0 7m strimzi-cluster-operator-5c8d5cf966-dfhwf 1/1 Running 0 10m $ kubectl create -f stages/dev/users.yaml # Verify all users and topics are created $ kubectl get kafkatopic NAME CLUSTER PARTITIONS REPLICATION FACTOR my-topic carbon-dev 1 1 $ kubectl get kafkauser NAME CLUSTER AUTHENTICATION AUTHORIZATION my-user carbon-dev tls simple If everything works well, the deployment should pick up existing PVCs and running!\nVerification üëâ This verification is based on the scenario we picked.\nIn the above section \u0026ldquo;Disaster Simulation\u0026rdquo;, we deployed a sample producer java app to produce logs/messages into Kafka. Now in this step(after recovery), we fetch those logs/messages to see recovery was successful\n# Check deployment config. For example, broker bootstrap route, topic name and user name. Deploy consumer test app $ kubectl create -f kafka-consumer.yaml deployment.apps/java-kafka-consumer created $ kubectl get pods NAME READY STATUS RESTARTS AGE java-kafka-consumer-7456748dbc-vvftf 1/1 Running 0 37s carbon-dev-entity-operator-cd54f496-vmsvm 3/3 Running 0 11m carbon-dev-kafka-0 1/1 Running 0 4m carbon-dev-kafka-1 1/1 Running 0 3m carbon-dev-kafka-2 1/1 Running 0 3m carbon-dev-kafka-exporter-c67976b6b-vz6f6 1/1 Running 0 10m carbon-dev-zookeeper-0 1/1 Running 0 15m carbon-dev-zookeeper-1 1/1 Running 0 15m carbon-dev-zookeeper-2 1/1 Running 0 15m strimzi-cluster-operator-5c8d5cf966-dhhws 1/1 Running 0 18m ## Check logs of the app, see it is fetching messages that were pushed before disaster $ kubectl logs java-kafka-consumer-7456748dbc-vvftf 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20891 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 891\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20892 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 892\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20893 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 893\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: 2021-11-23 02:59:28 INFO KafkaConsumerExample:48 - partition: 0 2021-11-23 02:59:28 INFO KafkaConsumerExample:49 - offset: 20894 2021-11-23 02:59:28 INFO KafkaConsumerExample:50 - value: \u0026#34;Hello world - 894\u0026#34; 2021-11-23 02:59:28 INFO KafkaConsumerExample:52 - headers: 2021-11-23 02:59:28 INFO KafkaConsumerExample:47 - Received message: ... If you able to see output like above(hello message), the Strimzi Kafka is restored properly\nConclusion In this guide, we have seen how to set up a disaster recovery plan for Strimzi Kafka by using a simple scenario and simulating disaster with Velero.\n","permalink":"https://veerendra2.github.io/strimzi-kafka-disaster-recovery/","summary":"Introduction Hello my dear fellow humans, hope you are having a great day. Today\u0026rsquo;s guide is on how to recover from a disaster for Strimzi Kafka with Velero. First of all, what is Strmzi Kafka?\nhttps://strimzi.io\nStrimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\nBack in a while I worked on Strimzi Kafka deployment on Openshift, very easy to set up and manage production level Kafka cluster on kubernetes, I have to give credit to Strimzi project team, did a great job on documentation, support on Github discussions and active developments.","title":"Strimzi Kafka Disaster Recovery with Velero"},{"content":" üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series.\nImportant Elasticsearch Configuration üìÑ Official Docs\nMainly 3 configuration files\nelasticsearch.yml - Elasticsearch config jvm.options - Elasticsearch JVM settings config log4j2.properties - Elasticsearch logging config Environment Variables\nexport the ES_PATH_CONF etc/default/elasticsearch (Sourced environment variables from. Recommended) Settings üìÑ Official Docs\nBefore going to production, it is recommended go through be below elasticsearch configs. Refer sample_config directory for configuration\nConfiguration Description Configuration Reference Path settings Log and data config Refer here Cluster name Cluster name Refer here Node name Node name Refer here Network host IP address that elasticsearch bind on Refer here Discovery settings Cluster discovery and initial master config Refer here Heap size JVM heap memory configuration Recommended heap size should be half of system memory. Make sure min and max heap memory same value. Refer here Heap dump path Heap dump location path config Default config is sufficient. Refer here GC logging Garbage collection logging configuration Default config is sufficient. Refer here Temp directory Configure private temporary directory that Elasticsearch uses is excluded from periodic cleanup Important System Configuration üìÑ Offical Docs\nBefore going to production, it is recommended go through be blow system configs Configuration Description Remark Disable swapping Disable swapping to prevent JVM heap or even its executable pages being swapped out to disk File descriptors Increase file descriptors for the user running Elasticsearch Virtual memory Increase mmap counts to prevent memory exceptions. DNS cache settings Overide JVM DNS positive/negetive cache settings (Leave default value) Temporary directory not mounted with noexec As the native library is mapped into the JVM virtual address space as executable, the underlying mount point of the location that this code is extracted to must not be mounted with noexec as this prevents the JVM process from being able to map this code as executable Bootstrap Checks üìÑ Offical Docs\nOnce you configured above configuration, elasticsearch performs some checks during bootstrap to verify configuration. If Elasticsearch is in development mode, any bootstrap checks that fail appear as warnings in the Elasticsearch log. If Elasticsearch is in production mode, any bootstrap checks(below) that fail will cause Elasticsearch to refuse to start.\nBelow are the boostrap checks.(In case, elasticsearch failed to start, below is the check list to verify)\nCheck Name Description Heap size check Enforces to start the JVM with the initial heap size equal to the maximum heap size to avoid these resize pauses File descriptor checks Enforces elasticsearch have good number of file descriptor Memory lock check Enforces JVM heap memory lock to avoid swapping pages to disk. Maximum number of theard pool checks Enforces Elasticsearch process has the rights to create enough threads under normal use. Max file size check Enforces that the Elasticsearch process can create max file size is unlimited Max size virtual memory check Enforces that the Elasticsearch process has unlimited address space Max map count check Enforces that the kernel allows a process to have at least 262,144 memory-mapped areas Client JVM check Enforces that the Elasticsearch start with the server JVM. Refer doc Use serial collector check Enforces that the Elasticsearch is not configured to run with the \u0026ldquo;serial collector\u0026rdquo; type JVM System call filter check Enforces system call filters are enabled which is an ability to execute system calls related to forking against arbitrary code execution attacks on Elasticsearch OnError and OnOutOfMemoryError check Enforces JVM has options related to OnError or OnOutOfMemoryError enabled Early-access check Enforces to start Elasticsearch on a release build of the JVM. Nor early-access snapshots of upcoming releases which are not suitable for production G1GC check Checks versions of the HotSpot JVM, Refer docs All permission check Enforces security policy used during bootstrap does not grant the java.security.AllPermission to Elasticsearch Discovery config check Enforces discovery is not running with the default configuration Docker Container Labeling Useful to filter logs, events, etc at logstash and also at kibana dashboard\ncom.yourdomain.container.type: \u0026#34;heartbeat\u0026#34; \u0026#34;metricbeat\u0026#34; \u0026#34;filebeat\u0026#34; \u0026#34;application\u0026#34; com.yourdomain.container.app.version: \u0026#34;1.2\u0026#34; com.yourdomain.container.environment: \u0026#34;stagging\u0026#34; \u0026#34;production\u0026#34; com.yourdomain.container.name: \u0026#34;auditlog-1\u0026#34; \u0026#34;odoo-1\u0026#34; Index Life Cycle Management Definitions Index Life Cycle Actions Index Roll Over Concept Index Aliasing - Stackoverflow Other elasticsearch terminologies - Stackoverflow ","permalink":"https://veerendra2.github.io/elasticsearch-deploy/config-overview/","summary":"üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series.\nImportant Elasticsearch Configuration üìÑ Official Docs\nMainly 3 configuration files\nelasticsearch.yml - Elasticsearch config jvm.options - Elasticsearch JVM settings config log4j2.properties - Elasticsearch logging config Environment Variables\nexport the ES_PATH_CONF etc/default/elasticsearch (Sourced environment variables from. Recommended) Settings üìÑ Official Docs\nBefore going to production, it is recommended go through be below elasticsearch configs. Refer sample_config directory for configuration\nConfiguration Description Configuration Reference Path settings Log and data config Refer here Cluster name Cluster name Refer here Node name Node name Refer here Network host IP address that elasticsearch bind on Refer here Discovery settings Cluster discovery and initial master config Refer here Heap size JVM heap memory configuration Recommended heap size should be half of system memory.","title":"Elasticsearch Configuration Overview"},{"content":" üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nInstall üìÑ Office Docs\nHardware Requirement üìÑ Offical Docs\nResource Minimum Recommended Memory 16 GB 64 GB CPU 8 Cores 16 Disk Depends Depends JDK Installation Pick JVM compatibility version with elasticsearch from here Install OpenJDK from here Download and install JDK 11 (Another guide here) $ apt-get install openjdk-11-jdk -y $ java -version openjdk version \u0026#34;11.0.6\u0026#34; 2020-01-14 OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1) OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing) Elasticsearch Installation Download latest elasticsearch from here (As of today the latest version is 7.6.2) Recommended to download/install package via .dep or PPA which postscripts creates user, groups and adds under systemd Install via apt-get from here\n$ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - $ sudo apt-get install apt-transport-https $ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install elasticsearch *DON\u0026rsquo;T START elasticsearch DAEMON YET!!! We need to configure\nSystem Configuration üìÑ Official Docs\nCreate data directory(This data directory you have to specify in elasticsearch config) $ mkdir /var/data/elasticsearch $ chown -R elasticsearch:elasticsearch /var/data/elasticsearch Disable swapping $ sudo swapoff -a ## *** Perminent Config *** ## Comment out any lines that contain the word \u0026#39;swap\u0026#39; in \u0026#39;/etc/fstab\u0026#39; Increase mmap count $ sudo sysctl -w vm.max_map_count=262144 ## *** Perminent Config *** ## update the vm.max_map_count setting in /etc/sysctl.conf Increase file descriptors for elasticseach user $ sudo su # ulimit -n 65535 ## *** Perminent Config *** ## echo \u0026#39;elasticsearch - nofile 65535\u0026#39;| sudo tee /etc/security/limits.conf ## Ubuntu ignores the limits.conf file for processes started by init.d. To enable the limits.conf file, edit /etc/pam.d/su and uncomment the following line ## # session required pam_limits.so Systems which uses systemd limits need to be specified via systemd\n$ sudo systemctl edit elasticsearch ## Above command opens editor to override the config. Add below setting in it [Service] LimitMEMLOCK=infinity Increase number of threads count for elasticsearch user $ sudo su # ulimit -u 4096 ## *** Perminent Config *** ## echo \u0026#39;elasticsearch - nproc 4096\u0026#39;| sudo tee /etc/security/limits.conf ## Ubuntu ignores the limits.conf file for processes started by init.d. To enable the limits.conf file, edit /etc/pam.d/su and uncomment the following line ## # session required pam_limits.so JVM Configuration Elasticsearch needs more heap memory. Change the JVM heap memory accordingly, please be noted that more heap means more garbage collection i.e more CPU utilization\nChange max and min heap in /etc/elasticsearch/jvm.options like below (Currently we are setting 15GB for heap)\n... -Xms15g -Xmx15g ... Go through other JVM config or logging config if required NOTE: Make sure min and max heap memories are same value to avoid bootstrap check failures\nElasticsearch Configuration üìÑ Official Docs\nIf you install elasticsearch via apt-get or .dep file, the config files are located in /etc/elasticsearch/ directory Refer elasticsearch.yml configuration in config_sample directory specify nodes and copy configurations to nodes accordingly Once everything is ready, start elasticsearch daemon in all nodes\n$ sudo systemctl enable elasticsearch $ sudo systemctl start elasticsearch If everything is ok, you should get responses like below\n$ curl -XGET 10.29.103.18:9200 { \u0026#34;name\u0026#34; : \u0026#34;carbon-1\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;carbon\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;zpoOdANFSXujgNMplMz1fQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.6.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;deb\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;ef48eb35cf30adf4db14086e8aabd07ef6fb113f\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2020-03-26T06:34:37.794943Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.4.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } $ curl -XGET 10.29.103.18:9200/_cat/nodes 10.29.103.18 7 74 5 0.07 0.91 1.95 dilm * carbon-1 10.29.103.10 2 87 25 0.08 1.08 1.68 dil - carbon-3 10.29.103.16 5 76 25 0.18 1.13 1.75 dil - carbon-2 Configure Security Starting from versions 6.8.0 and 7.1.0, security comes in basic. Read update blog here Watch Getting Started with Free Elasticsearch Security Features Configure transport layer SSL for node communication There is tool elasticsearch-certutil which comes with elasticsearch installation. Depends on requirement generate keys csr to get certificate signing request and sign it with CA cert to get self signed cerificates ca to get certificate authority http for certificates fot HTTP $ cd /usr/share/elasticsearch/bin $ ./elasticsearch-certutil --help Simplifies certificate creation for use with the Elastic Stack Commands -------- csr - generate certificate signing requests cert - generate X.509 certificates and keys ca - generate a new local certificate authority http - generate a new certificate (or certificate request) for the Elasticsearch HTTP interface Non-option arguments: command Option Description ------ ----------- -E \u0026lt;KeyValuePair\u0026gt; Configure a setting -h, --help Show help -s, --silent Show minimal output -v, --verbose Show verbose output Generating .p12 and copy the file to every node\n./elasticsearch-certutil cert ... Please enter the desired output file [elastic-certificates.p12]:/etc/elasticsearch/elastic-certificates.p12 ... Enable SSL and specify .p12 file location in config like below in every node in elasticsearch.yml xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/elastic-certificates.p12 Restart elasticsearch daemon in all nodes\n$ sudo systemctl restart elasticsearch Generate Passwords Login into master node and run below commands $ cd /usr/share/elasticsearch $ sudo bin/elasticsearch-setup-passwords auto Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user. The passwords will be randomly generated and printed to the console. Please confirm that you would like to continue [y/N]y Changed password for user apm_system PASSWORD apm_system = xxxxxxxxxxxxxx Changed password for user kibana PASSWORD kibana = xxxxxxxxxxxxxx Changed password for user logstash_system PASSWORD logstash_system = xxxxxxxxxxxxxx Changed password for user beats_system PASSWORD beats_system = xxxxxxxxxxxxxx Changed password for user remote_monitoring_user PASSWORD remote_monitoring_user = xxxxxxxxxxxxxx Changed password for user elastic PASSWORD elastic = xxxxxxxxxxxxxx Copy those credentials and store it safe.\nThe elastic user is cluster admin user The kibana user is for kibana to authenticate with elasticsearch Configure HTTPS for REST API Generate certificates(Below is the process to generate certificates) $ cd /usr/share/elasticsearch $ sudo bin/elasticsearch-certutil http ## Elasticsearch HTTP Certificate Utility The \u0026#39;http\u0026#39; command guides you through the process of generating certificates for use on the HTTP (Rest) interface for Elasticsearch. This tool will ask you a number of questions in order to generate the right set of files for your needs. ## Do you wish to generate a Certificate Signing Request (CSR)? A CSR is used when you want your certificate to be created by an existing Certificate Authority (CA) that you do not control (that is, you don\u0026#39;t have access to the keys for that CA). If you are in a corporate environment with a central security team, then you may have an existing Corporate CA that can generate your certificate for you. Infrastructure within your organisation may already be configured to trust this CA, so it may be easier for clients to connect to Elasticsearch if you use a CSR and send that request to the team that controls your CA. If you choose not to generate a CSR, this tool will generate a new certificate for you. That certificate will be signed by a CA under your control. This is a quick and easy way to secure your cluster with TLS, but you will need to configure all your clients to trust that custom CA. Generate a CSR? [y/N]n ## Do you have an existing Certificate Authority (CA) key-pair that you wish to use to sign your certificate? If you have an existing CA certificate and key, then you can use that CA to sign your new http certificate. This allows you to use the same CA across multiple Elasticsearch clusters which can make it easier to configure clients, and may be easier for you to manage. If you do not have an existing CA, one will be generated for you. Use an existing CA? [y/N]n A new Certificate Authority will be generated for you ## CA Generation Options The generated certificate authority will have the following configuration values. These values have been selected based on secure defaults. You should not need to change these values unless you have specific requirements. Subject DN: CN=Elasticsearch HTTP CA Validity: 5y Key Size: 2048 Do you wish to change any of these options? [y/N]n ## CA password We recommend that you protect your CA private key with a strong password. If your key does not have a password (or the password can be easily guessed) then anyone who gets a copy of the key file will be able to generate new certificates and impersonate your Elasticsearch cluster. IT IS IMPORTANT THAT YOU REMEMBER THIS PASSWORD AND KEEP IT SECURE CA password: [\u0026lt;ENTER\u0026gt; for none] ## How long should your certificates be valid? Every certificate has an expiry date. When the expiry date is reached clients will stop trusting your certificate and TLS connections will fail. Best practice suggests that you should either: (a) set this to a short duration (90 - 120 days) and have automatic processes to generate a new certificate before the old one expires, or (b) set it to a longer duration (3 - 5 years) and then perform a manual update a few months before it expires. You may enter the validity period in years (e.g. 3Y), months (e.g. 18M), or days (e.g. 90D) For how long should your certificate be valid? [5y] 5Y ## Do you wish to generate one certificate per node? If you have multiple nodes in your cluster, then you may choose to generate a separate certificate for each of these nodes. Each certificate will have its own private key, and will be issued for a specific hostname or IP address. Alternatively, you may wish to generate a single certificate that is valid You entered the following hostnames. - elk-staging.mydomain.com Is this correct [Y/n]y ## Which IP addresses will be used to connect to your nodes? If your clients will ever connect to your nodes by numeric IP address, then you can list these as valid IP \u0026#34;Subject Alternative Name\u0026#34; (SAN) fields in your certificate. If you do not have fixed IP addresses, or not wish to support direct IP access to your cluster then you can just press \u0026lt;ENTER\u0026gt; to skip this step. across all the hostnames or addresses in your cluster. If all of your nodes will be accessed through a single domain (e.g. node01.es.example.com, node02.es.example.com, etc) then you may find it simpler to generate one certificate with a wildcard hostname (*.es.example.com) and use that across all of your nodes. However, if you do not have a common domain name, and you expect to add additional nodes to your cluster in the future, then you should generate a certificate per node so that you can more easily generate new certificates when you provision new nodes. Generate a certificate per node? [y/N]n ## Which hostnames will be used to connect to your nodes? These hostnames will be added as \u0026#34;DNS\u0026#34; names in the \u0026#34;Subject Alternative Name\u0026#34; (SAN) field in your certificate. You should list every hostname and variant that people will use to connect to your cluster over http. Do not list IP addresses here, you will be asked to enter them later. If you wish to use a wildcard certificate (for example *.es.example.com) you can enter that here. Enter all the hostnames that you need, one per line. When you are done, press \u0026lt;ENTER\u0026gt; once more to move on to the next step. elk-staging.mydomain.com Enter all the IP addresses that you need, one per line. When you are done, press \u0026lt;ENTER\u0026gt; once more to move on to the next step. 85.xx.xx.xx 85.xx.xx.xx 85.xx.xx.xx You entered the following IP addresses. - 85.xx.xx.xx - 85.xx.xx.xx - 85.xx.xx.xx Is this correct [Y/n]y ## Other certificate options The generated certificate will have the following additional configuration values. These values have been selected based on a combination of the information you have provided above and secure defaults. You should not need to change these values unless you have specific requirements. Key Name: elk.mydomain.com Subject DN: CN=elk, DC=mydomain, DC=org Key Size: 2048 Do you wish to change any of these options? [y/N]n ## What password do you want for your private key(s)? Your private key(s) will be stored in a PKCS#12 keystore file named \u0026#34;http.p12\u0026#34;. This type of keystore is always password protected, but it is possible to use a blank password. If you wish to use a blank password, simply press \u0026lt;enter\u0026gt; at the prompt below. Provide a password for the \u0026#34;http.p12\u0026#34; file: [\u0026lt;ENTER\u0026gt; for none] ## Where should we save the generated files? A number of files will be generated including your private key(s), public certificate(s), and sample configuration options for Elastic Stack products. These files will be included in a single zip archive. What filename should be used for the output zip file? [/usr/share/elasticsearch/elasticsearch-ssl-http.zip] ./elasticsearch-certutil http 6.38s user 0.47s system 6% cpu 1:38.49 total The zip file /usr/share/elasticsearch/elasticsearch-ssl-http.zip contains below files\n$ unzip elasticsearch-ssl-http.zip $ tree . . ‚îú‚îÄ‚îÄ ca ‚îú‚îÄ‚îÄ README.txt ‚îî‚îÄ‚îÄ ca.p12 ‚îú‚îÄ‚îÄ elasticsearch ‚îú‚îÄ‚îÄ README.txt ‚îú‚îÄ‚îÄ http.p12 ‚îî‚îÄ‚îÄ sample-elasticsearch.yml ‚îî‚îÄ‚îÄ kibana ‚îú‚îÄ‚îÄ README.txt ‚îú‚îÄ‚îÄ elasticsearch-ca.pem ‚îî‚îÄ‚îÄ sample-kibana.yml Create new directory /etc/elasticsearch/http_ssl and copy http.p12 to all server\nAdd below config to /etc/elasticsearchelasticsearch.yml to enable http ssl like below\nxpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: \u0026#34;/etc/elasticsearch/http_ssl/http.p12\u0026#34; HTTPS configurations for HTTP clients For Kibana ‚ùó Refer Kibana Installation to know more\nCreate /etc/kibana/ca and copy elasticsearch-ca.pem to kibana server\nAdd below config to kibana.yml to enable HTTPS communication between elasticsearch and kibana\nelasticsearch.ssl.certificateAuthorities: [ \u0026#34;/etc/kibana/ca/elasticsearch-ca.pem\u0026#34; ] For Beats Refer elasticsearch-deploy-notes/beats for demo config\nUse same .pem file to enable HTTPS communication between beats and elasticsearch(sample snippet below) output.elasticsearch: hosts: [\u0026#39;https://elk-staging.mydomain.com:9201\u0026#39;] username: admin password: 123123123 ssl: certificate_authorities: [\u0026#34;/etc/elasticsearch-ca.pem\u0026#34;] setup.dashboards: enabled: true setup.kibana: host: \u0026#34;https://elk-staging.mydomain.com:5691\u0026#34; ssl: certificate_authorities: [\u0026#34;/etc/letsencryptauthorityx3.pem\u0026#34;] Alerting The official Elasticsearch \u0026ldquo;Basic\u0026rdquo; version doesn\u0026rsquo;t include alerting. Below are the 2 opensource plugin available for elasticsearch\n1. Opendistro Elasticsearch Alerting ‚ö†Ô∏è Opendistro Alerting IS NOT compatible with X-Pack Security, for more information refer this issue. If the x-pack is already enabled, DO NOT install this plugin, won\u0026rsquo;t work!. In order to install this plugin, disable x-pack security and install opendistro security\nPick Opendistro alerting standalone plugin version compatibility here\nLogin into master node and install necessary plugins and alerting plugin like below ## Go elasticseatch bin directory $ cd /usr/share/elasticsearch/ ## Install Job Scheduler plugin $ sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-job-scheduler/opendistro-job-scheduler-1.8.0.0.zip ## Install Alerting plugin $ sudo bin/elasticsearch-plugin install https://d3g5vo6xdbdb9a.cloudfront.net/downloads/elasticsearch-plugins/opendistro-alerting/opendistro_alerting-1.8.0.0.zip ## Restart Elasticsearch $ sudo systemctl restart elasticsearch 2. Elastalert Refer elastalert docs for installation instructions.\nStack Monitoring Offical Docs Kibana Docs Elasticsearch can monitor itself, enable stack monitoring using REST API like below\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;xpack.monitoring.collection.enabled\u0026#34;: true } } We can also monitor with metricbeat, this method is useful especially want to monitor multiple clusters. Please refer metricbeat elasticsearch-xpack module config\nElatic Beats Beats is a free and open platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash or Elasticsearch.\nRefer elasticsearch-deploy-notes/beats for example config\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/install/","summary":"üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nInstall üìÑ Office Docs\nHardware Requirement üìÑ Offical Docs\nResource Minimum Recommended Memory 16 GB 64 GB CPU 8 Cores 16 Disk Depends Depends JDK Installation Pick JVM compatibility version with elasticsearch from here Install OpenJDK from here Download and install JDK 11 (Another guide here) $ apt-get install openjdk-11-jdk -y $ java -version openjdk version \u0026#34;11.0.6\u0026#34; 2020-01-14 OpenJDK Runtime Environment (build 11.","title":"Elasticsearch Installation"},{"content":" üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nRolling Upgrade Elasticsearch üìÑ Official Docs\n‚ö†Ô∏è A rolling upgrade allows an Elasticsearch cluster to be upgraded one node at a time so upgrading does not interrupt service\nAs of now, the current latest version of elasticsearch is v7.7.1. Below procedure is for rolling upgrade from 7.6.2=\u0026gt;7.7.1.\n1. Divide the cluster into 2 groups Example node names carbon-x\na. Non master-eligible nodes carbon-2 carbon-3 b. Master-eligible nodes carbon-1 Upgrade order (Important!)\nNon master eligible nodes Master eligible nodes NOTE: Newer nodes can always join a cluster with an older master, BUT older nodes cannot always join a cluster with a newer master\n2. Prepare for upgrade Below points taken from official docs which are recomended\nCheck the deprecation log to see if you are using any deprecated features and update your code accordingly.(While checking this step, I found some histogram deprecation logs. But I performed upgrade anyways!) Review the breaking changes and make any necessary changes to your code and configuration for version 7.7.1. If you use any plugins, make sure there is a version of each plugin that is compatible with Elasticsearch version 7.7.1. Test the upgrade in an isolated environment before upgrading your production cluster. Back up your data by taking a snapshot 3. Disable shard allocation When the node is down, elasticsearch try to replicate the shards on that node to other nodes in the cluster which involves lot of I/O. Since the node will be down for short time, it is recommended disable allocation with below settings\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: \u0026#34;primaries\u0026#34; } } 4. Upgrade Elasticsearch NOTE: Follow the upgrade order like mentioned above. *Perform below operation on each node by node and group by group.\nSince in our cluster, the elasticsearch package installed from APT repository, we can upgrade package from APT repository like below\n$ sudo systemctl stop elasticsearch.service $ sudo apt-get update $ sudo apt-get install --only-upgrade elasticsearch $ sudo systemctl start elasticsearch $ sudo systemctl status elasticsearch Check logs if it required\nBefore upgrading to next node, wait for the cluster to finish shard allocation.You can check progress by below API(Important!)\nGET _cat/health?v NOTE: In case, the ip routes are disappeared which happened in one node while install package, follow below steps Take routing table reference from other nodes and add routes accordingly.\nip link set eth1 down ip addr add 10.29.103.10/24 dev eth1 ip route add 10.48.0.0/16 via 10.29.103.1 ip link set eth1 up 5. Upgrade any plugins *This step was not performed, since there were no plugins installed\nUse the elasticsearch-plugin script to install the upgraded version of each installed Elasticsearch plugi\n6. Re-enable shard allocation Verify all nodes are joined in the cluster\nGET _cat/nodes Reenable shard allocation\nPUT _cluster/settings { \u0026#34;persistent\u0026#34;: { \u0026#34;cluster.routing.allocation.enable\u0026#34;: null } } Once all nodes are upgraded, verify the version and health\nGET /_cat/health?v GET /_cat/nodes?h=ip,name,version\u0026amp;v ‚ùó Once cluster is upgraded and all nodes joined, it will take some for shard reallocation.\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/upgrade/","summary":"üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nRolling Upgrade Elasticsearch üìÑ Official Docs\n‚ö†Ô∏è A rolling upgrade allows an Elasticsearch cluster to be upgraded one node at a time so upgrading does not interrupt service\nAs of now, the current latest version of elasticsearch is v7.7.1. Below procedure is for rolling upgrade from 7.6.2=\u0026gt;7.7.1.\n1. Divide the cluster into 2 groups Example node names carbon-x\na. Non master-eligible nodes carbon-2 carbon-3 b.","title":"Elasticsearch Upgrade"},{"content":" üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nKibana Installation üìÑ Office docs\n‚ùó The elasticsearch should be up and running before you start kibana installation procedure\nInstall via apt-get from here\nAs of today the kibana version is 7.6.2 $ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - $ sudo apt-get install apt-transport-https $ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install kibana Configure Kibana Refer kibana.yml configuration file in current directory and modify accordingly\nSpecify kibana credentials in kibana.yml which were generated while configuration elasticsearch security Configure Security üìÑ Official doc\nHTTPS Configuration with Let\u0026rsquo;s Encrypt certificates\nDownload certbot and generate certificate $ wget https://dl.eff.org/certbot-auto $ chmod 755 certbot-auto $ ./certbot-auto certonly Copy certificates to kibana config directory and change permission $ mkdir /etc/kibana/ssl $ cp -pr /etc/letsencrypt/archive/data.example.com /etc/kibana/ssl/ $ chmod 750 /etc/kibana/ssl/data.example.com $ chmod 640 /etc/kibana/ssl/data.example.com/* $ chown -R root:kibana /etc/kibana/ssl/data.example.com Add below config to kibana.yml to enable HTTPS\nserver.ssl.enabled: true server.ssl.certificate: /etc/kibana/ssl/elk-staging.yourdomain.com/cert1.pem server.ssl.key: /etc/kibana/ssl/elk-staging.yourdomain.com/privkey1.pem SSL Config for Kibana to Communicate Elasticsearch\nCopy elasticsearch-ca.pem(This the pem file that you were generated while configuring elasticsearch security) to /etc/kibana/ca/ $ mkdir /etc/kibana/ca/ $ ls /etc/kibana/ca/elasticsearch-ca.pem Add below config to kibana.yml\nelasticsearch.ssl.certificateAuthorities: [ \u0026#34;/etc/kibana/ca/elasticsearch-ca.pem\u0026#34; ] Upgrade Kibana üìÑ Official docs\nAs of now, the current latest version of kibana is v7.7.1. Below is the procedure for upgrade from 7.6.2=\u0026gt;7.7.1.\nUpgrade $ sudo systemctl stop kibana $ sudo apt-get install --upgrade-only kibana $ sudo systemctl start kibana ‚ùó Kibana install will take some, especially while \u0026ldquo;Unpacking\u0026rdquo; package. So, please be patient\n","permalink":"https://veerendra2.github.io/elasticsearch-deploy/kabana-install/","summary":"üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nKibana Installation üìÑ Office docs\n‚ùó The elasticsearch should be up and running before you start kibana installation procedure\nInstall via apt-get from here\nAs of today the kibana version is 7.6.2 $ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - $ sudo apt-get install apt-transport-https $ echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install kibana Configure Kibana Refer kibana.","title":"Kibana Installation"},{"content":" üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nElastalert üìÑ Official docs üìÇ Official project Repo\nElastalert is developed by Yelp written in python, queries docs in elasticsearch and send alerts depends on the rules.\nSince Elastalert is not part of Elasticsearch plugin, we can install it where ever we want to.\nInstallation ‚ùó Refer elasticsearch-deploy-notes/elastalert for example config\n$ sudo apt-get install python3-pip $ sudo pip3 install elastalert $ sudo pip3 install -U PyYAML $ mkdir -p /opt/elastalert/rules ## Copy alert rules yaml files and config file to /opt/elastalert and /opt/elastalert/rules accordingly from this repo Recommended to create index in elasticsearch for elastalert to store metadata $ elastalert-create-index Elastic Version: 7.7.0 Reading Elastic 6 index mappings: Reading index mapping \u0026#39;es_mappings/6/silence.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert_status.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/past_elastalert.json\u0026#39; Reading index mapping \u0026#39;es_mappings/6/elastalert_error.json\u0026#39; New index elastalert_status created Done! Test rules in case if it is needed $ elastalert-test-rule --config /opt/elastalert/config.yaml /opt/elastalert/rules/heartbeat_checks.yml Postfix Gmail SMTP In oder to use Gmail as SMTP, you need to enable 2-Factor authentication and generate app password\nFirst configure 2-Factor Authentication at google accounts security Generate app passwords at app password generation Click Select app and choose Other (custom name) from the dropdown. Enter ‚ÄúPostfix‚Äù and click Generate. Copy App Password Install Postfix package sudo apt-get -y install postfix mailutils libsasl2-2 ca-certificates libsasl2-modules While postfix package install, it will prompt for configuration, default options/configs are sufficient i.e don\u0026rsquo;t have to change anything\nAdd below config in /etc/postfix/main.cf\nrelayhost = [smtp.gmail.com]:587 smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_sasl_security_options = noanonymous smtp_tls_CApath = /etc/ssl/certs smtpd_tls_CApath = /etc/ssl/certs smtp_use_tls = yes Add email and app password in /etc/postfix/sasl_passwd [smtp.gmail.com]:587 xxxxx@yourdomain.de:xxx xxxxx xxxxx xxxxx Start daemon sudo chmod 400 /etc/postfix/sasl_passwd sudo postmap /etc/postfix/sasl_passwd sudo systemctl restart postfix Test it echo \u0026#34;Testing\u0026#34; | mail -s \u0026#34;Test Email\u0026#34; soap@gmail.com sudo postqueue -p Daemonize the Elastalert We will use supervisord to run ElastAlert.\nInstall supervisord $ sudo pip3 install supervisord Review supervisor config and copy $ ls /etc/supervisordconf /etc/supervisord.conf Enable and start the deamon root@s-root-odoo03 ~ # supervisorctl supervisor\u0026gt; add elastalert supervisor\u0026gt; start elastalert supervisor\u0026gt; exit ","permalink":"https://veerendra2.github.io/elasticsearch-deploy/elastalert-demo/","summary":"üëâ This writeup is part of \u0026ldquo;Elasticsearch Deploy Docs\u0026rdquo; series\nElastalert üìÑ Official docs üìÇ Official project Repo\nElastalert is developed by Yelp written in python, queries docs in elasticsearch and send alerts depends on the rules.\nSince Elastalert is not part of Elasticsearch plugin, we can install it where ever we want to.\nInstallation ‚ùó Refer elasticsearch-deploy-notes/elastalert for example config\n$ sudo apt-get install python3-pip $ sudo pip3 install elastalert $ sudo pip3 install -U PyYAML $ mkdir -p /opt/elastalert/rules ## Copy alert rules yaml files and config file to /opt/elastalert and /opt/elastalert/rules accordingly from this repo Recommended to create index in elasticsearch for elastalert to store metadata $ elastalert-create-index Elastic Version: 7.","title":"Elastalert Demo Config"},{"content":"Introduction Hello guys, today I came up with an interesting write up, that is how to setup backup and restore with Velero on Kubernetes. A year back I worked on Strimzi Kafka, a deployment solution for deploying production level Kafka on Kubernetes. Strimzi Kafka uses persistance volume(PV) as a disk which is a managed disk from cloud provider(e.g. Azure, AWS, etc), but I couldn\u0026rsquo;t find proper backup solution in order to configure PV backup and restore. Sure, you can configure these managed disk backups from Terraform or manually in cloud provider portals. But tools like Velero, backups PV from kubernetes side which is more visible and easy to manage which is what you will see in a moment.\n‚ùó Velero supports multiple cloud provider, this write up covers only deployment of Velero on kubernetes with storage class provisioner as AzureDisk\nüëâ All deployment files are in my repo https://github.com/veerendra2/velero-demo\nVelero https://velero.io\nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes\nDisaster Recovery ‚Üí Reduces time to recovery in case of infrastructure loss, data corruption, and or service outage Data Migration ‚Üí Enables cluster portability by easily migrating Kubernetes resources from one cluster to another. Data Protection ‚Üí Offers key data protection features such as scheduled backups, retention schedules, and pre or post-backup hooks for custom actions. Features Backup Clusters Backup your Kubernetes resources and volumes for an entire cluster, or part of a cluster by using namespaces or label selectors. Schedule Backups Set schedules to automatically kickoff backups at recurring intervals. Backup Hooks Configure pre and post-backup hooks to perform custom operations before and after Velero backups. How it works ‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îå ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚î¨ ‚îÇ ‚îÇ ‚îî ‚îÄ ‚îÄ ‚îÄ V ‚îÄ a ‚îÄ ‚îÄ ‚îÄ ‚îÄ e ‚îÄ z ‚îÄ K ‚îÄ ‚îÄ ‚îÄ l ‚îÄ m u ‚îÄ u ‚îÄ ‚îÄ ‚îÄ e ‚îÄ i r ‚îÄ b ‚îÄ ‚îÄ ‚îÄ r ‚îÄ c e ‚îÄ e ‚îÄ ‚îÄ ‚îÄ o ‚îÄ r ‚îÄ r ‚îÄ ‚îÄ ‚îÄ ‚îÄ o p ‚îÄ n ‚îÄ ‚îÄ ‚îÄ S ‚îÄ s l ‚îÄ e ‚îÄ ‚îÄ ‚îÄ e ‚îÄ o u ‚îÄ t ‚îÄ ‚îÄ ‚îÄ r ‚îÄ f g ‚îÄ e ‚îÄ ‚îÄ ‚îÄ v ‚îÄ t i ‚îÄ s ‚îÄ ‚îÄ ‚îÄ e ‚îÄ n ‚îÄ ‚îÄ ‚îÄ ‚îÄ r ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚î¨ ‚îÇ ‚îÇ ‚îò ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îú ‚îÇ ‚îú ‚îò ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îº ‚îÇ ‚îº ‚îÇ | ‚îÇ ‚îÇ ‚îÇ ‚îò m ‚îÄ ‚îÄ e ‚îÄ ‚îÄ P b t ‚îÄ ‚îÄ e a a ‚îÄ ‚îÄ r c a d ‚îÄ ‚îÄ s k p a ‚îÄ ‚îÄ i u p t ‚îÄ ‚îÄ s p s a ‚îÄ ‚îÄ t ‚îÄ ‚îÄ a m d b ‚îÄ ‚îÄ n a e a ‚îÄ ‚îÄ c n p c ‚îÄ ‚îÄ e a l k ‚îÄ ‚îÄ - g o u ‚îÄ ‚îÄ v e y p ‚îÄ ‚îÄ o m m ‚îÄ ‚îÄ l e e m ‚îÄ ‚îÄ u n n a ‚îÄ ‚îÄ m t t n ‚îÄ ‚îÄ e a ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îî g ‚îÄ ‚îÄ e ‚îÄ ‚îÄ m ‚îÄ ‚îÄ e ‚îÄ ‚îÄ n ‚îÄ ‚îÄ t ‚ñ∫ ‚ñ∫ ‚îå ‚îÇ ‚îÇ ‚îî ‚îå ‚îÇ | ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ M ‚îÄ ‚îÄ ‚îÄ ‚îÄ a ‚îÄ ‚îÄ A ‚îÄ ‚îÄ n ‚îÄ ‚îÄ z ‚îÄ ‚îÄ A a ‚îÄ ‚îÄ u S ‚îÄ ‚îÄ z g ‚îÄ ‚îÄ r t ‚îÄ ‚îÄ u e ‚îÄ ‚îÄ e o ‚îÄ ‚îÄ r d ‚îÄ ‚îÄ r ‚îÄ ‚îÄ e ‚îÄ ‚îÄ B a ‚îÄ ‚îÄ D ‚îÄ ‚îÄ l g ‚îÄ ‚îÄ i ‚îÄ ‚îÄ o e ‚îÄ ‚îÄ s ‚îÄ ‚îÄ b ‚îÄ ‚îÄ k ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê ‚îÇ ‚îÇ ‚îò ‚îê ‚îÇ ‚îÇ ‚îò Velero uses service principle to access cloud resources for managing backups of deployments on K8s. It supports various plugins for cloud, while deploying Velero we have to specify plugin and provide credentials(/base/deployment.yaml#L67). By using Velero, we can backup deployments, PVs, namespaces and entire clusters.\nFor deployment related backups, it uses storage account to store all deployments metadata For PV backup, it uses a snapshot from the cloud provider. ‚ö†Ô∏è Backups may fails sometimes, if it doesn\u0026rsquo;t have necessary memory(#3234)\nBelow diagram gives bird eye view on how Velero works Deploy Velero provides binary to manage velero server deployment and scheduled backups. But, in our deployment, we do something different, that is; use kustomize and manage in git repo. But first we need to download velero binary from release page here\n$ curl -O https://github.com/vmware-tanzu/velero/releases/download/v1.9.1/velero-v1.9.1-linux-amd64.tar.gz $ tar -xf velero-v1.9.1-linux-amd64.tar.gz $ cd velero-v1.9.1-linux-amd64/ $ ls examples LICENSE velero $ sudo mv velero /usr/local/bin/ Prerequisites Before deploying Velero server on kubernetes, we need to customize the velero server according to our needs. Below is the configuration for Velero to manage Azure resources.\nBelow are the variables that will be used during installation\nEnvironmental Variable Name Description SUBSCRIPTION_ID Subscription ID TENANT_ID Tenant ID of service principle CLIENT_ID Client ID of service principle CLIENT_SECRET Service principal secret RESOURCE_GROUP Source resource groups where K8s PVs(Azure Managed Disks) are present to backup BACKUP_RESOURCE_GROUP Destination resource group where PV(Azure Managed Disks)snapshots are stored STORAGE_ACCOUNT_ID Storage account name BLOB_CONTAINER Container name in storage account where K8s deployment metadata stored BACKUP_SUBSCRIPTION_ID Subscription ID where backs are stored(In our case, this value SUBSCRIPTION_ID) STORAGE_ACCOUNT_ACCESS_KEY Storage account access key CLOUD_NAME Cloud name, the value is AzurePublicCloud Create service principal $ az account list --query \u0026#34;[].{name:name, id:id}\u0026#34; --output tsv $ export SUBSCRIPTION_ID=\u0026#34;[SUBSCRIPTION_ID_HERE]\u0026#34; $ az login $ CLIENT_SECRET=`az ad sp create-for-rbac --name \u0026#34;velero-sp\u0026#34; --role\u0026#34;Contributor\u0026#34; --query \u0026#39;password\u0026#39; -o tsv --scopes subscriptions/$SUBSCRIPTION_ID` $ CLIENT_ID=`az ad sp list --display-name \u0026#34;velero-sp\u0026#34; --query\u0026#39;[0].appId\u0026#39; -o tsv` $ TENANT_ID=`az ad sp list --display-name \u0026#34;velero-sp\u0026#34; --query\u0026#34;[].appOwnerTenantId\u0026#34; -o tsv Create Azure storage account $ az storage account create \\ --name $STORAGE_ACCOUNT_ID \\ --resource-group $RESOURCE_GROUP \\ --location \u0026lt;location\u0026gt; \\ --sku Standard_ZRS \\ --encryption-services blob $ az storage container create \\ --account-name STORAGE_ACCOUNT_ID \\ --name $BLOB_CONTAINER \\ --auth-mode login Set variables Below are dummy values of variables.\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; ./credentials-velero SUBSCRIPTION_ID=\u0026#34;my-sub-id\u0026#34; TENANT_ID=\u0026#34;my-tnt-id\u0026#34; CLIENT_ID=\u0026#34;my-client-id\u0026#34; CLIENT_SECRET=\u0026#34;secure-secret\u0026#34; RESOURCE_GROUP=\u0026#34;myresgrp\u0026#34; CLOUD_NAME=AzurePublicCloud EOF $ export BACKUP_RESOURCE_GROUP=myresgrp $ export STORAGE_ACCOUNT_ID=myaccount $ export BACKUP_SUBSCRIPTION_ID=my-sub-id $ export BLOB_CONTAINER=backup Generate Deployment Files Once variables are set and verified, generate velero deployment files using velero cli\n‚ùó You can also get the velero deployment here\n$ velero install \\ --provider azure \\ --plugins velero/velero-plugin-for-microsoft-azure:v1.1.0 \\ --bucket $BLOB_CONTAINER \\ --secret-file ./credentials-velero \\ --backup-location-config resourceGroup=$BACKUP_RESOURCE_GROUP,storageAccount=$STORAGE_ACCOUNT_ID,subscriptionId=$BACKUP_SUBSCRIPTION_ID \\ --snapshot-location-config apiTimeout=5m,resourceGroup=$BACKUP_RESOURCE_GROUP,subscriptionId=$BACKUP _SUBSCRIPTION_ID \\ --use-volume-snapshots=true --dry-run -o yaml ... Once you run the above command, it displays deployment files on stdout, because of the --dry-run -o yaml option. Copy the content to separate yaml files like below.\nüëâ You can check here how I seperated https://github.com/veerendra2/velero-demo/tree/main/base\n## https://github.com/veerendra2/velero-demo/tree/main/base $ tree . . ‚îú‚îÄ‚îÄ cluster-role-binding.yaml ‚îú‚îÄ‚îÄ deployment.yaml ‚îî‚îÄ‚îÄ velero-crds.yaml 0 directories, 3 files Once all files are arranged(and verify variables in below deployment files that we configured in above section), login into kubernetes and run the deployment one-by-one\n$ kubectl create -f velero-crds.yaml $ kubectl create -f cluster-role-binding.yaml $ kubectl create -f deployment.yaml Kubernetes deploys the velero server in the velero namespace as a pod. Verify velero server is installed with kubectl get pods -n velero\nConfigure Backups We can configure backups with velero cli\n‚ùó A handy tool to write cronjob -\u0026gt; https://crontab.guru/\n$ velero schedule create kafka-backup-schedule \\ --schedule=\u0026#34;@every 168h\u0026#34; --ttl 2160h0m0s \\ --include-namespaces=kafka $ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; The above example, backups all resources in namespace kafka(including PVs and K8s deployment files) for every 168 hours. But we want setup every thing in yaml files so that we store in git repo(e.g. /overlay/dev). So, we can ask velero cli to display yaml like below\n$ velero schedule create kafka-backup-schedule --schedule=\u0026#34;@every 168h\u0026#34; --ttl 2160h0m0s--include-namespaces=kafka -o yaml apiVersion: velero.io/v1 kind: Schedule metadata: creationTimestamp: null name: kafka-backup-schedule namespace: kafka spec: schedule: \u0026#39;@every 168h\u0026#39; template: hooks: {} includeClusterResources: true includedNamespaces: - \u0026#39;*\u0026#39; ttl: 2160h0m0s status: {} You can also look for help like below\n$ velero schedule create --help The --schedule flag is required, in cron notation, using UTC time: | Character Position | Character Period | Acceptable Values | | ------------------ | :--------------: | ----------------: | | 1 | Minute | 0-59,* | | 2 | Hour | 0-23,* | | 3 | Day of Month | 1-31,* | | 4 | Month | 1-12,* | | 5 | Day of Week | 0-7,* | The schedule can also be expressed using \u0026#34;@every \u0026lt;duration\u0026gt;\u0026#34; syntax. The duration can be specified using a combination of seconds (s), minutes (m), and hours (h), for example: \u0026#34;@every 2h30m\u0026#34;. Usage: velero schedule create NAME --schedule [flags] We can get backup status if the schedule that we configured above.\n$ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup-schedule-20201121202523 Completed 2020-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; kafka-backup-schedule-20201121201523 Completed 2020-11-21 21:15:23 +0100 STD 88d default \u0026lt;none\u0026gt; ‚ùó schedule consists of multiple backup as you can see above\nOnce everything is setup we can do magic with kustomize. So, below is finally show off with kustomize. This kustomize setup is very useful when you configure with GitOps tools like Argo CD.\nüëâ You can find this demo deployment files in my repo here https://github.com/veerendra2/velero-demo\n$ tree . . ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ cluster-role-binding.yaml ‚îÇ ‚îú‚îÄ‚îÄ deployment.yaml ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îî‚îÄ‚îÄ velero-crds.yaml ‚îú‚îÄ‚îÄ overlay ‚îÇ ‚îî‚îÄ‚îÄ dev ‚îÇ ‚îú‚îÄ‚îÄ backup-locations.yaml ‚îÇ ‚îú‚îÄ‚îÄ backup-schedules ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ pvc.yaml ‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml ‚îÇ ‚îî‚îÄ‚îÄ secrets ‚îÇ ‚îî‚îÄ‚îÄ cloud-credentials.yaml ‚îî‚îÄ‚îÄ README.md 5 directories, 9 files Terminology Before concluding this write up, we need to get familiar with 2 terminologies that helps us to understand how Velero works internally\nBackup Storage Location The BackupStorageLocation holds info about storage account, container in storage account. We can have multiple \u0026ldquo;Backup Storage Location\u0026rdquo; with difference containers and storage account(e.g. dev/backup-locations.yaml#L2-L15)\n## Since there velero CRDs are installed, we can query like below $ kubectl get backupstoragelocations -n velero NAME AGE default 1d kafka 1d Volume Snapshot Location The VolumeStorageLocation holds info about resource group where snapshots are stored(e.g. dev/backup-locations.yaml#L17-L21)\n## Since there velero CRDs are installed, we can query like below $ kubectl get volumesnapshotlocations -n velero NAME AGE default 1d Restore What good is the backup if you can\u0026rsquo;t restore?!\u0026hellip;\nGet schedule and backup like below\n## Login into kubernetes with kubectl cli $ velero schedule get NAME STATUS CREATED SCHEDULE BACKUP TTL LAST BACKUP SELECTOR kafka-backup-schedule Enabled 2020-11-21 20:35:04 +0100 STD 0 15 * * 5 2160h0m0s 1d ago \u0026lt;none\u0026gt; $ velero backup get NAME STATUS CREATED EXPIRES STORAGE LOCATION SELECTOR kafka-backup-schedule-20201121202523 Completed 2020-11-21 21:25:23 +0100 STD 88d default \u0026lt;none\u0026gt; Create restore\n$ velero restore create --from-backup kafka-backup-schedule-20201121202523 Restore request \u0026#34;kafka-backup-schedule-20201121202523-20201123033701\u0026#34; submitted successfully. Run `velero restore describe kafka-backup-schedule-20201121202523-20201123033701` or `velero restore logs kafka-backup-schedule-20201121202523-20201123033701` for more details. We can also describe to know the status\n$ velero restore describe kafka-backup-schedule-20201121202523-20201123033701 Name: kafka-backup-schedule-20201121202523-20201123033701 Namespace: velero Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Phase: InProgress \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;----STATUS-HERE Backup: kafka-backup-schedule-20201121202523 Namespaces: Included: all namespaces found in the backup Excluded: \u0026lt;none\u0026gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io Cluster-scoped: auto Namespace mappings: \u0026lt;none\u0026gt; Label selector: \u0026lt;none\u0026gt; Restore PVs: auto Once restore is completed, verify has it been restored\n$ kubectl get pods -n kafka ... Conclusion In this write up, I have covered\nSneak peek on Velero How Velero works How to deploy and configure to manage resources on Azure. Configure scheduled backups Restore BONUS; I have collected handfull of velero cli example in my Github gist here Stay tuned, my next write up will be published very soon which is related to backup and restore on Kubernetes üòÉ\n","permalink":"https://veerendra2.github.io/velero-deployment/","summary":"Introduction Hello guys, today I came up with an interesting write up, that is how to setup backup and restore with Velero on Kubernetes. A year back I worked on Strimzi Kafka, a deployment solution for deploying production level Kafka on Kubernetes. Strimzi Kafka uses persistance volume(PV) as a disk which is a managed disk from cloud provider(e.g. Azure, AWS, etc), but I couldn\u0026rsquo;t find proper backup solution in order to configure PV backup and restore.","title":"Velero Deployment with Kustomize (Azure)"},{"content":"It has been 2 year since I wrote a new post. Due to busy work, moving to a new city, new jobs and getting married, I wasn‚Äôt able to keep up posts. Finally I‚Äôm back now, I have been thinking of changing blog themes for a long time. I spent some time on exploring Jekyll themes and tried to modify them according to my requirements. As you can see here github issue.\nHugo A month ago I discovered Hugo, a static website builder like Jekyll, so I decided to take a look at it. And thus I decided to move to Hugo, because of its speed, simple and single binary\nJust like Hugo, I discovered Jekyll a few years back, as you can see from my old blog post. After using Jekyll for a while I see below annoying things while building and installing Jekyll everytime I change the OS in my laptop. #7\nJekyll is relatively slow I get confuse with gems versions and its dependencies while picking new themes Screw up my system ruby packages with other packages like Vagrant etc. because of that I had to use docker containers and these some permissions issues, etc. So, when I see Hugo, it solved my above problems\nIt is faster like they say in their website To install new theme, I can use git module or simply drop the theme in themes directly and configure Single binary! No BS! Simple directory structure Now my blog is powered by Hugo üéâ\nOther updates I removed disqus tool for commenting, moved to Github discussion with tool https://giscus.app From now onwards I will try to keep up the blogs regularly. I‚Äôm planning some ‚Äúnugget‚Äù blogs like small posts contains info from book I‚Äôm reading or may be some my troubleshooting on tools, etc. ü§û Here is my Hugo site Github Actions workflow https://gist.github.com/veerendra2/28d35b44c8340d5a801e5606edb32f45\n","permalink":"https://veerendra2.github.io/moving-to-hugo/","summary":"It has been 2 year since I wrote a new post. Due to busy work, moving to a new city, new jobs and getting married, I wasn‚Äôt able to keep up posts. Finally I‚Äôm back now, I have been thinking of changing blog themes for a long time. I spent some time on exploring Jekyll themes and tried to modify them according to my requirements. As you can see here github issue.","title":"Moving to Hugo and other updates!"},{"content":"Looks like my blog posts are like Sherlock TV Show episodes, posting once in a while..anyways I\u0026rsquo;m back now. As you might know, GitHub recently launched GitHub Actions where people can automate workflows like build, test, deploy code from GitHub.\nI have started reading docs a little bit, I have to admit, setting up workflows on GitHub Actions is not that hard. I can directly start creating workflow from available workflow templates in the \u0026ldquo;Actions\u0026rdquo; tab. Then I saw the workflow template for \u0026ldquo;Jekyll Site CI/CD\u0026rdquo;, builds Jekyll site. I got some feeling that I automate my workflow of building and deploying my blog (which currently you are reading) on GitHub Pages. So, Let\u0026rsquo;s see how did I setup CI/CD\nBut first let me explain how my blog site works and then manual steps every time I do to publish a blog.\nGitHub Pages This blog site is powered by GitHub Pages which you can publish static html site right from GitHub repository (For example my repo here). All you have to do is to create a repo name like \u0026lt;YourGitHubUsername\u0026gt;.github.io and drop static html pages in master branch. That\u0026rsquo;s it, GitHub does the magic, serves your website.\nFor this, I\u0026rsquo;m maintaining 2 branches, one for my markdown source files in source branch and another for static html site in master branch\nJekyll Everything\u0026rsquo;s good so far, but we know that we are too lazy to write html pages. So, that\u0026rsquo;s where this tool \u0026ldquo;Jekyll\u0026rdquo; comes into picture, converts markdown files to static html websites. Once you jekyll build, it will build a static website in the _site directory. For local testing you can run jekyll serve to see how site looks.(Checkout my other post to know how to install jekyll)\nMy Manual Workflow Write blog post in markdown files which I keep in source branch (Obviously I can not automate this step :-P) Run jekyll build to build static website Copy static website content from _site to master branch Push master branch changes to repo to publish website Push source branch changes to repo to store/version tracking GitHub Actions Below is the jekyll.yml to automate my workflow\nLet\u0026rsquo;s go thought the jekyll.yml line by line very briefly\nLine No. Description 3 Event subscription; I want to trigger the workflow only when there is any push in \u0026ldquo;source\u0026rdquo; branch 11 I want to run this build procedure on ubuntu box(GitHub action supports other box as well like windows, mac, etc) 16 In order to build, first I have to clone the repo, for this, there is ready made action called \u0026ldquo;actions/checkout@v2\u0026rdquo; which checkouts my repo with \u0026ldquo;source\u0026rdquo; branch 21 Since I can\u0026rsquo;t expect GitHub servers to have \u0026ldquo;jekyll\u0026rdquo; installed, I built my own docker image for jekyll with dependency gems to build a website. (You can find other jekyll docker image, I\u0026rsquo;m using older version of jekyll, that\u0026rsquo;s why I built my own) 28 I want to push static website to \u0026ldquo;master\u0026rdquo; branch, so I have to checkout \u0026ldquo;master\u0026rdquo; branch as well 33 Copy \u0026ldquo;_site\u0026rdquo; content to master branch 37 Normal shell commands, git add and git commit 44 Finally push changes to \u0026ldquo;master\u0026rdquo; to publish website with help of using ready made action \u0026ldquo;ad-m/GitHub-push-action@master\u0026rdquo; Now, all I have to do is drop jekyll.yml in the .github/workflows/ directory to GitHub to pick up my workflow. Below is the picture showing the pipeline for my website deployment.\n","permalink":"https://veerendra2.github.io/ci-cd-github-pages-with-github-actions/","summary":"Looks like my blog posts are like Sherlock TV Show episodes, posting once in a while..anyways I\u0026rsquo;m back now. As you might know, GitHub recently launched GitHub Actions where people can automate workflows like build, test, deploy code from GitHub.\nI have started reading docs a little bit, I have to admit, setting up workflows on GitHub Actions is not that hard. I can directly start creating workflow from available workflow templates in the \u0026ldquo;Actions\u0026rdquo; tab.","title":"CI/CD for GitHub Pages with GitHub Actions"},{"content":"Hallo alle zusammen, after a long time I\u0026rsquo;m writing this blog and I come with an interesting and long post\nI know what you are thinking, I steal Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial, but hey!, I did some research and try to fit K8s cluster(Multi-Master!) in a laptop with Docker as \u0026lsquo;CRI\u0026rsquo; and Flannel as \u0026lsquo;CNI\u0026rsquo;.\nThis blog post follows Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way, I highly recommend go through his repo. I\u0026rsquo;m writing this blog post to keep it as reference for me and share with other people whoever want to try it. So, feel free to correct me if there are any mistakes and ping me for any queries. This series is divided into 3 parts and all configuration/scripts are available in my github repo. Well that has been said, let\u0026rsquo;s start building the cluster.\nBelow is my laptop configuration. Make sure you have enough resources in your laptop.(or depends on resources, you can reduce nodes in the cluster, etc.)\nLaptop Acer Predator Helios 200 CPU Intel Core i5 8th Gen RAM 8 GB Host OS Ubuntu 18.04 Hostname ghost First let\u0026rsquo;s talk about the cluster in Kubernetes The Hard Way which has 3 controller nodes, 3 worker nodes and a load balancer on GCP. I want to deploy a cluster with multiple masters, but I was afraid it is too much for my laptop. So, I reduced it to 2 controller nodes, 2 worker nodes (or VMs in my case) and replaced GCP load balancer with nginx docker container as a load balancer, the clusters look like below.\n1. Prerequisites Installation of packages *NOTE: The following components will be installed on host machine(laptop)\nInstall KVM hypervisor. $ sudo apt-get install qemu-kvm quem-system \\ libvirt-bin bridge-utils \\ virt-manager -y Install Docker, because we want to run nginx load balancer container on host Install cfssl and cfssljson binaries $ wget -q --show-progress --https-only --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 $ chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 $ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl $ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson $ wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl $ chmod +x kubectl $ sudo mv kubectl /usr/local/bin/ Subnets In official \u0026ldquo;Kubernetes The Hard Way\u0026rdquo;, cluster network configuration done via gcloud and obviously we are not going to use it. We have to choose subnets manually for our cluster nodes,CIDR for pods and K8s services. So, here is what I come with\nNo. Name Subnet 1 Cluster Nodes 10.200.1.0/24 2 POD CIDR(cluster-cidr) 10.100.0.0/16 3 Service(service-cluster-ip) 10.32.0.0/24 Linux Bridge \u0026amp; NAT As you can see in the above diagram, we are going to use a linux bridge to connect our VMs and nginx docker container. Also we need to do NATing for our VMs in order to access the Internet.\n$ EXTERNAL_IFACE=\u0026#34;wlan0\u0026#34; ## Enable ip forwarding $ sudo sysctl net.ipv4.conf.all.forwarding=1 ## Create br0 bridge $ sudo ip link add name br0 type bridge $ sudo ip link set dev br0 up $ sudo ip addr add 10.200.1.1/24 dev br0 ## iptables NAT configuration $ sudo iptables -t nat -A POSTROUTING -o $EXTERNAL_IFACE -j MASQUERADE $ sudo iptables -A FORWARD -i $EXTERNAL_IFACE -o br0 -m state --state RELATED,ESTABLISHED -j ACCEPT $ sudo iptables -A FORWARD -i br0 -o $EXTERNAL_IFACE -j ACCEPT ## Bridge config. Read more @ https://tinyurl.com/yan5jnd4 $ sudo sysctl -w net.bridge.bridge-nf-call-arptables=0 $ sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=0 $ sudo sysctl -w net.bridge.bridge-nf-call-iptables=0 In order to launch docker container(nginx load balancer container) on different linux bridge(other than default docker0), we need to create a docker network and specify that network while launching the container. Below command creates docker network with br0 as bridge\n$ docker network create --driver=bridge \\ --ip-range=10.200.1.0/24 \\ --subnet=10.200.1.0/24 -o \u0026#34;com.docker.network.bridge.name=br0\u0026#34; br0 Create workspace directory We can save all configuration and generate certificates in this directory\n$ mkdir ~/kubernetes-the-hard-way $ cd ~/kubernetes-the-hard-way 2. Provisioning Compute Resources Specify cluster info(hostname, IP and user to login) in controllers.txt and workers.txt files respectively like in below. In the same way add those VM IPs in /etc/hosts file like below. These files are useful to automate things like copy files to nodes or generating certificates for these nodes, etc. You will see in a moment.\n$ cd ~/kubernetes-the-hard-way $ cat controllers.txt m1 10.200.1.10 veeru m2 10.200.1.11 veeru $ cat workers.txt n2 10.200.1.13 veeru n2 10.200.1.14 veeru $ cat nginx_proxy.txt proxy 10.200.1.15 $ cat /etc/hosts 127.0.0.1 localhost 127.0.1.1 ghost 10.200.1.10 m1 10.200.1.11 m2 10.200.1.12 n1 10.200.1.13 n2 10.200.1.15 nginx Below are the IPs, hostname and username for the nodes that I choose\nNode Role Node Hostname Node IP Node Login Username Controller m1 10.200.1.10 veeru Controller m2 10.200.1.11 veeru Worker n1 10.200.1.13 veeru Worker n2 10.200.1.14 veeru Load Balancer(nginx Container) proxy 10.200.1.15 N/A Download Ubuntu 18.04 server .iso from https://www.ubuntu.com/ In previous section, we installed kvm hypervisor and now lets spin up 4 VMs and specify bridge name under network section like in below screenshot.(I used \u0026ldquo;Virtual Machine Manager\u0026rdquo; GUI to launch VMs) *I\u0026rsquo;m not covering OS installation in VM. You can easily find it on the Internet.\n*NOTE: While installing OS, please select static IP and specify IPs according to their node names\n*TIP: Install OS in VM and clone VM 3 time\nOnce the OS installation is completed, check the connectivity between the host-VM and VM-VM and you should be able to ssh both host-to-VM and VM-to-VM. For convenience, you can copy ssh keys, so that you don\u0026rsquo;t have to enter a password every time.\n$ ssh-keygen $ ssh-copy-id guest-username@guest-ip 3. Provisioning a CA and Generating TLS Certificates It is a good practice to set up encrypted communication between the components of K8s. In this section we will create public key certificates and private keys for below components using CloudFlare\u0026rsquo;s PKI toolkit as we downloaded earlier.(Know more about PKI)\nadmin user kubelet kube-controller-manager kube-proxy kube-scheduler kube-api But first, we have to create Certificate Authority(CA) which e-signatures the certificates that we are going to generate.\n$ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/ca-config.json $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/ca-csr.json ## Generate CA $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca-key.pem ca.pem Admin User Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/admin-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin $ ls admin-key.pem admin.pem Kubelet Client Certificates As docs says\nK8s uses node authorization which is a special-purpose authorization mode that specifically authorizes API requests made by kubelets\nIn order to be authorized by the Node Authorizer, Kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;. Let\u0026rsquo;s create a certificate and private key for each worker nodes (In my case n1 and n2)\n$ cd ~/kubernetes-the-hard-way IFS=$\u0026#39;\\n\u0026#39; for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` INTERNAL_IP=`echo $line | awk \u0026#39;{print $2}\u0026#39;` cat \u0026gt; ${instance}-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;system:node:${instance}\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;Westeros\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;The North\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:nodes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes The Hard Way\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;Winterfell\u0026#34; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${INTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done $ ls n1-key.pem n1.pem n2-key.pem n2.pem Controller Manager Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-controller-manager-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager $ ls kube-controller-manager-key.pem kube-controller-manager.pem Kube Proxy Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-proxy-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy $ ls kube-proxy-key.pem kube-proxy.pem Scheduler Client Certificate $ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kube-scheduler-csr.json # Generate Certificate $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler $ ls kube-scheduler-key.pem kube-scheduler.pem Kubernetes API Server Certificate kube-api server certificate\u0026rsquo;s hostname should include following things\nAll controller\u0026rsquo;s hostname All controller\u0026rsquo;s IP Load balancer\u0026rsquo;s hostname Load balancer\u0026rsquo;s IP Kubernetes\u0026rsquo;s service(Both \u0026lsquo;service name\u0026rsquo; and IP which are 10.32.0.1 and kubernetes.default) localhost $ cd ~/kubernetes-the-hard-way ## CERT_HOSTNAME=10.32.0.1,\u0026lt;master node 1 Private IP\u0026gt;,\u0026lt;master node 1 hostname\u0026gt;,\u0026lt;master node 2 Private IP\u0026gt;,\u0026lt;master node 2 hostname\u0026gt;,\u0026lt;API load balancer Private IP\u0026gt;,\u0026lt;API load balancer hostname\u0026gt;,127.0.0.1,localhost,kubernetes.default $ CERT_HOSTNAME=10.32.0.1,m1,10.200.1.10,m2,10.200.1.11,proxy,10.200.1.15,127.0.0.1,localhost,kubernetes.default $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/kubernetes-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${CERT_HOSTNAME} \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes $ ls Service Account Key Pair Service account key pair certificate is used to sign service account tokens\n$ cd ~/kubernetes-the-hard-way $ wget -q https://raw.githubusercontent.com/veerendra2/k8s-the-hard-way-blog/master/certificate_configs/service-account-csr.json $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account $ ls service-account-key.pem service-account.pem Copy Certificates to Nodes $ cd ~/kubernetes-the-hard-way # Minion $ IFS=$\u0026#39;\\n\u0026#39; $ for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ca.pem ${instance}-key.pem ${instance}.pem ${user}@${instance}:~/ done # Master $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat controllers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem ${user}@${instance}:~/ done 4. Generating kubeconfig Files for Authentication kubeconfig are used for authentication between the kubernetes components and users-to-kubernetes. kubeconfig consists of mainly 3 things\nNo. Entity Description 1 Cluster api-server\u0026rsquo;s IP and its certificate which encodes in base64 2 Users User related info like who are authenticating ,their certificate and key or service account token 3 Context Holds Cluster\u0026rsquo;s and User\u0026rsquo;s reference. If you have multiple clusters and users, this context becomes handy In this section, we are going to generate kubeconfig for below components\nGenerating kubelet kubeconfig The user in kubeconfig should be system:node:\u0026lt;Worker_name\u0026gt; which should match the Kubelet hostname that we specified while generating the kubelet client certificate. This will ensure Kubelets are properly authorized by the Kubernetes Node Authorizer.\n$ cd ~/kubernetes-the-hard-way $ KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\ --client-certificate=${instance}.pem \\ --client-key=${instance}-key.pem \\ --embed-certs=true \\ --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:node:${instance} \\ --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done $ ls n1.kubeconfig n2.kubeconfig Generate kube-proxy kubeconfig $ cd ~/kubernetes-the-hard-way $ KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig } $ ls kube-proxy.kubeconfig Generate kube-controller-manager kubeconfig $ cd ~/kubernetes-the-hard-way { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig } $ ls kube-controller-manager.kubeconfig Generate kube-scheduler kubeconfig $ cd ~/kubernetes-the-hard-way $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig } $ ls kube-scheduler.kubeconfig Generate admin kubeconfig $ cd ~/kubernetes-the-hard-way $ { kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig } $ ls admin.kubeconfig Copy kubeconfig files to nodes $ cd ~/kubernetes-the-hard-way $ IFS=$\u0026#39;\\n\u0026#39; $ for line in `cat workers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh ${instance}.kubeconfig kube-proxy.kubeconfig ${user}@${instance}:~/ done $ for instance in `cat controllers.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${user}@${instance}:~/ done 5. Generating the Data Encryption Config and Key Kubernetes stores different types of data including cluster state, application configurations, and secrets. Kubernetes supports the ability to encrypt cluster data at rest.In this section we will generate an encryption key and an encryption config suitable for encrypting Kubernetes Secrets.\nThe Encrypted Key ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) The Encryption Config File cat \u0026gt; encryption-config.yaml \u0026lt;\u0026lt;EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF Copy to Controller Nodes $ IFS=$\u0026#39;\\n\u0026#39; $ for instance in `cat controller.txt`; do instance=`echo $line | awk \u0026#39;{print $1}\u0026#39;` user=`echo $line | awk \u0026#39;{print $3}\u0026#39;` rsync -zvhe ssh encryption-config.yaml ${user}@${instance}:~/ done Till now we have done following things\nProvisioned compute resources Generated certificates Generated kubeconfig files Copied certificate files and kubeconfigs to nodes In the next post, we will bootstrap controller nodes\n","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-1/","summary":"Hallo alle zusammen, after a long time I\u0026rsquo;m writing this blog and I come with an interesting and long post\nI know what you are thinking, I steal Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial, but hey!, I did some research and try to fit K8s cluster(Multi-Master!) in a laptop with Docker as \u0026lsquo;CRI\u0026rsquo; and Flannel as \u0026lsquo;CNI\u0026rsquo;.\nThis blog post follows Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way, I highly recommend go through his repo.","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 1)"},{"content":"Welcome back to \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series part 2. In previous post we have provisioned compute resources, generated certificates and kubeconfig files. In this post, we will install and configure controller nodes\n6. Bootstrapping the etcd Cluster etcd is a consistent and highly-available key value storage DB. Kubernetes stores all cluster data in etcd via api-server. In this section we will install and configure etcd on all controller nodes.\n*NOTE: The below commands must run on all controller nodes\n*TIP: You can use tumx to run command on multiple nodes at same time\n## On controller nodes $ wget -q --show-progress --https-only --timestamping \\ \u0026#34;https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz\u0026#34; $ tar -xvf etcd-v3.3.9-linux-amd64.tar.gz $ sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/ $ sudo mkdir -p /etc/etcd /var/lib/etcd $ sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ Set up the following environment variables which are usefull generate etcd systemd unit file\n## On controller nodes $ ETCD_NAME=`hostname` $ INTERNAL_IP=`hostname -i` # IP of the current node #INITIAL_CLUSTER=\u0026lt;controller 1 hostname\u0026gt;=https://\u0026lt;controller 1 private ip\u0026gt;:2380,\u0026lt;controller 2 hostname\u0026gt;=https://\u0026lt;controller 2 private ip\u0026gt;:2380 $ INITIAL_CLUSTER=m1=https://10.200.1.10:2380,m2=https://10.200.1.11:2380 Create systemd unit file\n## On controller nodes $ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster ${INITIAL_CLUSTER} \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start the etcd service\n## On controller nodes $ { sudo systemctl daemon-reload sudo systemctl enable etcd sudo systemctl start etcd } Once etcd installation and configuration done in all controller nodes, verify that etcd cluster is working properly\n## On controller nodes $ sudo ETCDCTL_API=3 etcdctl member list \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem You should see output like below\n7. Bootstrapping the Kubernetes Control Plane The control plane binaries are\nkube-apiserver kube-controller-manager kube-scheduler Download control plane binaries\n*NOTE: The below commands must run on all controller nodes\n## On controller nodes $ sudo mkdir -p /etc/kubernetes/config $ KUBERNETES_VERSION=v1.10.13 $ wget -q --show-progress --https-only --timestamping \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-apiserver\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-controller-manager\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kube-scheduler\u0026#34; \\ \u0026#34;https://dl.k8s.io/${KUBERNETES_VERSION}/bin/linux/amd64/kubectl\u0026#34; *TIP: You can get version number from kubernetes releases page\nMove the binaries to /usr/local/bin/\n## On controller nodes $ chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl $ sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ Kubernetes API Server Configuration Move certificates to kubernetes directory\n## On controller nodes $ sudo mkdir -p /var/lib/kubernetes/ $ sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem \\ encryption-config.yaml /var/lib/kubernetes/ Create a kube-api server systemd unit file.\n## On controller nodes $ CONTROLLER0_IP=10.200.1.10 $ CONTROLLER1_IP=10.200.1.11 $ INTERNAL_IP=`hostname -i` # Current node\u0026#39;s IP $ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://$CONTROLLER0_IP:2379,https://$CONTROLLER1_IP:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 \\\\ --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kubernetes Controller Manager Configuration Move kubeconfig files to kubernetes directory\n## On controller nodes $ sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/ Create kube-controller-manager systemd unit file\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kubernetes Scheduler Configuration Move kube-scheduler kubeconfig to kubernetes directory\n# On controller nodes $ sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/ Create kube-scheduler configuration file\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: componentconfig/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \u0026#34;/var/lib/kubernetes/kube-scheduler.kubeconfig\u0026#34; leaderElection: leaderElect: true EOF {% endhighlight %} Create kube-scheduler systemd unit file {% highlight shell %} # On controller nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start the controller services ## On controller nodes $ sudo systemctl daemon-reload $ sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler $ sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler Enable HTTP Health Checks In the original \u0026ldquo;Kubernetes The Hard Way\u0026rdquo;, Kelsey used GCP load balancer to load balance the requests among controllers. Since it is difficult to set up HTTPS health checks on GCP network load balancer and kube-apiserver supports only HTTPS health checks. He created HTTP nginx proxy for kube-api server, GCP network load balancer performs health check via HTTP nginx proxy. But in our case, we can skip this step since we are not using GCP network load balancer\nVerification Check the components status using below commands.\n## On controller nodes $ kubectl get componentstatuses --kubeconfig admin.kubeconfig Run above command on all controller nodes and verify statuses which should like below\nRBAC for Kubelet Authorization In this section we will configure RBAC permissions to allow the kube-api server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs, and executing commands in pods.\nCreate the system:kube-apiserver-to-kubelet ClusterRole with permissions to access the Kubelet.\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \u0026#34;*\u0026#34; EOF The kube-api server authenticates to the Kubelet as the \u0026ldquo;kubernetes\u0026rdquo; user using the client certificate as defined by the --kubelet-client-certificate flag which has been defined in the kube-apiserver systemd unit file above.\nBind the system:kube-apiserver-to-kubelet ClusterRole to the kubernetes user:\n## On controller nodes $ cat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \u0026#34;\u0026#34; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF The Kubernetes Frontend Load Balancer As I said earlier, we are not going using GCP load network load balancer, but we are going using nginx docker container on host(Laptop) to load balance the requests.\nIn this section, we will build nginx docker image with appropriate configuration to load balance requests among controller nodes(m1 and m2)\nnginx configuration Specify controllers IPs with kube-api server\u0026rsquo;s port in nginx configuration like below\n## On host cd ~/kubernetes-the-hard-way $ cat \u0026lt;\u0026lt;EOF | tee kubernetes.conf stream { upstream kubernetes { server 10.200.1.10:6443; server 10.200.1.11:6443; } server { listen 6443; listen 443; proxy_pass kubernetes; } } EOF Dockerfile Create Dockerfile to build nginx load balancer docker image\n# On host $ cd ~/kubernetes-the-hard-way $ cat \u0026lt;\u0026lt;EOF | tee Dockerfile FROM nginx:latest MAINTAINER Veerendra Kakumanu RUN mkdir -p /etc/nginx/tcpconf.d \u0026amp;\u0026amp; echo \u0026#34;include /etc/nginx/tcpconf.d/*;\u0026#34; \u0026gt;\u0026gt; /etc/nginx/nginx.conf COPY kubernetes.conf /etc/nginx/tcpconf.d/kubernetes.conf EOF Build and launch the container\n# On host $ cd ~/kubernetes-the-hard-way $ sudo docker build -t nginx_proxy . $ sudo docker run -it -d -h proxy --net br0 --ip 10.200.1.15 nginx-proxy Verification curl the HTTPS endpoint of load balancer(nginx docker container) which forwards the requests to the controller node with certificate.\n## On host $ KUBERNETES_PUBLIC_ADDRESS=10.200.1.15 $ curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version If everything is good, you should see output like below.\nIn this post, we have successfully provisioned controller nodes and load balancers. We will bootstrap the worker nodes in next post\n","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-2/","summary":"Welcome back to \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series part 2. In previous post we have provisioned compute resources, generated certificates and kubeconfig files. In this post, we will install and configure controller nodes\n6. Bootstrapping the etcd Cluster etcd is a consistent and highly-available key value storage DB. Kubernetes stores all cluster data in etcd via api-server. In this section we will install and configure etcd on all controller nodes.","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 2)"},{"content":"Welcome to the final part of \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series. In part-1, we discussed our cluster architecture, provisioned compute resources, generated certificates and kubeconfig. In part-2, we have bootstrapped controller nodes.\nIn this post, we will bootstrap worker nodes and at the end, perform smoke test on the cluster\n9. Bootstrapping the Kubernetes Worker Nodes As the title of this post \u0026ldquo;Kubernetes The Hard Way With Docker \u0026amp; Flannel\u0026rdquo;, what we are going to do now is different from Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial i.e. container runtime interface is docker instead of containerd\n*NOTE: The below commands must run on all worker nodes\nInstall below packages. conntack is required for iptables, since it tracks the connections for K8s services\n## On worker nodes $ { sudo apt-get update sudo apt-get -y install socat conntrack ipset } Install docker You can follow official docs to install docker on ubuntu\nKubelet Configuration Move certificate files to kubernetes directory\n## On worker nodes $ { sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/ sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig sudo mv ca.pem /var/lib/kubernetes/ } Create kubelet configuration file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \u0026#34;/var/lib/kubernetes/ca.pem\u0026#34; authorization: mode: Webhook clusterDomain: \u0026#34;cluster.local\u0026#34; clusterDNS: - \u0026#34;10.32.0.10\u0026#34; podCIDR: \u0026#34;10.100.0.0/16\u0026#34; #resolvConf: \u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; runtimeRequestTimeout: \u0026#34;15m\u0026#34; tlsCertFile: \u0026#34;/var/lib/kubelet/n1.pem\u0026#34; tlsPrivateKeyFile: \u0026#34;/var/lib/kubelet/n1-key.pem\u0026#34; EOF Create a kubelet systemd unit file. Below you can notice I have specified --docker* flag which indicates that kubelet intracts with docker daemon\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --docker=unix:///var/run/docker.sock \\ --docker-endpoint=unix:///var/run/docker.sock \\ --image-pull-progress-deadline=2m \\ --network-plugin=cni \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --register-node=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Kube Proxy Configuration Move kubeconfig to kubernetes directory\n## On worker nodes $ sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig Create kube-proxy configuration file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \u0026#34;/var/lib/kube-proxy/kubeconfig\u0026#34; mode: \u0026#34;iptables\u0026#34; clusterCIDR: \u0026#34;10.100.0.0/16\u0026#34; EOF Create kube-proxy systemd unit file\n## On worker nodes $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start Worker services ## On worker nodes $ { sudo systemctl daemon-reload sudo systemctl enable kubelet kube-proxy sudo systemctl start kubelet kube-proxy } Verification Once worker services configuration is done on all worker nodes, get nodes list like below command in any controller node\n10. Configuring kubectl for Remote Access In this section, we will generate a kubeconfig file for admin user. The kubeconfig file requires Kubernetes API server IP which is nginx load balancer docker container‚Äôs IP\n## On host $ { KUBERNETES_PUBLIC_ADDRESS=`cat nginx_proxy.txt | awk \u0026#39;{print $2}\u0026#39;` kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem kubectl config set-context kubernetes-the-hard-way \\ --cluster=kubernetes-the-hard-way \\ --user=admin kubectl config use-context kubernetes-the-hard-way } Verification Check health of the remote Kubernetes cluster List the nodes in the remote Kubernetes cluster Provisioning CNI In this section, we will set up CNI i.e Flannel as the title of this blog post says.\n**If you want to know other CNIs and there performances, check Alexis Ducastel\u0026rsquo;s post here\nFirst login into worker nodes and enable ip forwarding\n## On worker nodes $ sudo sysctl net.ipv4.conf.all.forwarding=1 Get kube-flannel.yml from coreos\u0026rsquo;s flannel github repo\n## On host $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Wait for few seconds and verify flannel daemonset status\n$ kubectl get daemonsets -n kube-system Once pods are up, we have to test pod networking that they can connect each other\nFor that, we will deploy nginx deployment with 2 replicas and a busybox pod. Then we will try to curl nginx home page from busybox via nginx\u0026rsquo;s POD IP\nCreate nginx deployment with 2 replicas\n$ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: selector: matchLabels: run: nginx replicas: 2 template: metadata: labels: run: nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 EOF Create service for the deployment\n$ kubectl expose deployment/nginx Get nginx pods IP\n$ kubectl get ep nginx Now let curl nginx home of nginx pods\n$ kubectl run busybox --image=odise/busybox-curl --command -- sleep 3600 $ POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ kubectl exec $POD_NAME -- curl \u0026lt;first nginx pod IP address\u0026gt; $ kubectl exec $POD_NAME -- curl \u0026lt;second nginx pod IP address\u0026gt; $ kubectl get svc 11. Deploying the DNS Cluster Add-on In this section we will deploy DNS add-on which provides DNS based service discovery. We will use coreDNS as DNS add-on in our K8s\nDeploy core DNS\n$ kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns.yaml Verification Verify core DNS pods are up\n$ kubectl get pods -l k8s-app=kube-dns -n kube-system In order to verify DNS resolution in K8s, we need to create a busybox pod and try nslookup the kubernetes service\nCreate a busybox deployment\n$ kubectl run busybox --image=odise/busybox-curl --command -- sleep 3600 Retrieve the full name of the busybox pod and execute a DNS lookup for the kubernetes service inside the busybox pod\n$ POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ kubectl exec -ti $POD_NAME -- nslookup kubernetes If everything is good, you should see \u0026ldquo;kubernetes\u0026rdquo; name resolution like above\nThat completes our objectives, we have installed necessary components to bring up the kubernetes.You can perform some other smoke test from official Kubernetes The Hard Way\nConclusion It has been a long post for readers. I have modified the official Kubernetes The Hard Way to set up Docker as CRI and Flannel as CNI. So, let\u0026rsquo;s conclude what we have done so far\nProvisioning compute resources in Laptop with kvm hypervisor 2 controllers, 2 computers and nginx docker containers which serves as load balancer. Generated certificates to setup TLS communication between the kubernetes components kubeconfig files generations Provisioning controller and worker nodes with docker and Flannel You can go even further to set up K8s dashboard,K8s logging and Prometheus monitoring, etc. (For starters, you can refer my prometheus-k8s-monitoring)\nReferences https://github.com/kelseyhightower/kubernetes-the-hard-way https://developer.ibm.com/recipes/tutorials/bridge-the-docker-containers-to-external-network/ https://docs.docker.com/config/containers/container-networking/ https://coreos.com/flannel/docs/latest/kubernetes.html https://unix.stackexchange.com/questions/490893/not-able-to-ssh-from-vm-to-vm-via-linux-bridge ","permalink":"https://veerendra2.github.io/kubernetes-the-hard-way-3/","summary":"Welcome to the final part of \u0026ldquo;Kubernetes-The Hard Way With Docker \u0026amp; Flannel\u0026rdquo; series. In part-1, we discussed our cluster architecture, provisioned compute resources, generated certificates and kubeconfig. In part-2, we have bootstrapped controller nodes.\nIn this post, we will bootstrap worker nodes and at the end, perform smoke test on the cluster\n9. Bootstrapping the Kubernetes Worker Nodes As the title of this post \u0026ldquo;Kubernetes The Hard Way With Docker \u0026amp; Flannel\u0026rdquo;, what we are going to do now is different from Kelsey Hightower\u0026rsquo;s Kubernetes The Hard Way tutorial i.","title":"Kubernetes-The Hard Way With Docker \u0026 Flannel (Part 3)"},{"content":"As we all know, enabling HTTPS to endpoints/websites is essential now-a-days. When it comes to Kubernetes, when we expose a service as LoadBalancer, the cloud provider doesn\u0026rsquo;t provide an HTTPS mechanism for the endpoint by default.\nIf we look at the K8s setup that is deployed on AWS(For example kops), there is an actual ELB(Elastic Load Balancer) sits in front of K8s service and load balance the traffic. AWS\u0026rsquo;s ELB is not TLS enabled by default. With help of aws-cli, we can deploy certificates(self-signed) on the load balancer and make the endpoint secure.\nNote that the K8s cluster is deployed on AWS and configured \u0026ldquo;type: LoadBalancer\u0026rdquo; for service which applications can access from outside of the cluster.\nPrerequisites Get cfssl and cfssljson binary files from https://pkg.cfssl.org/ Get aws-cli. Check installation docs Configure aws-cli with aws configure. It should create files like below veeru@ultron:~$ cat ~/.aws/credentials [default] aws_access_key_id = ATIA2HTxxxV5Cqwe aws_secret_access_key = ATIA2HTxxxV5Cqwexxxxxx veeru@ultron:~$ cat ~/.aws/config [default] region = us-east-2 output = text Create certificate $ cat \u0026lt;\u0026lt;EOF \u0026gt;csr_ca.json { \u0026#34;CN\u0026#34;: \u0026#34;My Awesome CA\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;Westeros\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Winterfell\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;House Stark\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;CA Secsr_ca.jsonrvices\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;The North\u0026#34; } ] } EOF Generate the CA certificate and private key:\n$ cfssl gencert -initca csr_ca.json | cfssljson -bare ca $ ls ca-key.pem ca.pem Upload your self signed certificate to aws $ aws iam upload-server-certificate --server-certificate-name your-name --certificate-body file://ca.pem --private-key file://ca-key.pem List certificates\n$ aws iam list-server-certificates SERVERCERTIFICATEMETADATALIST arn:aws:iam::xxxxx:server-certificate/your-name 2023-04-30T07:52:00Z / ASCAIxxxxxCHES3FxxIQO cf 2018-05-01T08:17:30Z Specify annotation in Kuberenetes service Edit service with \u0026ldquo;kubectl edit svc {svc-name}\u0026rdquo; or you can also edit with the help of K8s dashboard like me.\n\u0026#34;service.beta.kubernetes.io/aws-load-balancer-ssl-cert\u0026#34;: \u0026#34;arn:aws:iam::xxxxx:server-certificate/your-name\u0026#34; Now you should be able to access the endpoint on https.\nFor example: https://xxxx-xxxx.us-east-2.elb.amazonaws.com:9090/graph Check out other AWS service annotations\n","permalink":"https://veerendra2.github.io/ssl-config-k8s-service-aws/","summary":"As we all know, enabling HTTPS to endpoints/websites is essential now-a-days. When it comes to Kubernetes, when we expose a service as LoadBalancer, the cloud provider doesn\u0026rsquo;t provide an HTTPS mechanism for the endpoint by default.\nIf we look at the K8s setup that is deployed on AWS(For example kops), there is an actual ELB(Elastic Load Balancer) sits in front of K8s service and load balance the traffic. AWS\u0026rsquo;s ELB is not TLS enabled by default.","title":"SSL Configuration for Kubernetes External LoadBalancer - [AWS ELB]"},{"content":"*A blog post that I‚Äôm actively collecting ‚ÄúLinux pseudo files info, cheat sheets and tips‚Äù\nTips \u0026amp; Tricks How to force a command to return exit code 0 even if the command exited non-zero?\nHow to install dependencies of .deb automatically which failed to install previously?\nExample Solution:\n$ dpkg -i r-base-core_3.3.3-1trusty0_amd64.deb || : \\ \u0026amp;\u0026amp; apt-get --yes --force-yes -o Dpkg::Options::=\u0026#34;--force-confdef\u0026#34; -o Dpkg::Options::=\u0026#34;--force-confold\u0026#34; -f install -y \\ How to traverse directories in shell script?\ncd command should not be used to traverse directories. Remember that each commands in shell script will spawn as individual process unlink programming language, entire script as single process i.e. The scope of cd command is only for the child process, not the parent. By using pushd and popd we can achieve traversing directories.\nExample Solution:\n$ pushd Downloads $ cat download.txt $ popd $ pushd Downloads/movies $ ls $ popd Files: /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq - Real time speed of the CPU(ability to adjust their speed to help in saving on battery/power usage)\n/proc Directory\n/proc/cpuinfo | grep MHz - The absolute (max) CPU speed /proc/sys/net/ipv4/* - Get more info under this directory from kernel.org docs /proc/net/tcp and /proc/net/tcp6 - Get complete info of variables for these files from kernel.org.docs /proc/sysctl https://www.kernel.org/doc/Documentation/sysctl/ https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Reference_Guide/s3-proc-sys-net.html\nNetwork related in Linux - Refer kernel.org.doc\nSpecial Device Files /dev/null - Discards all data written to it but reports that the write operation succeeded Read man pages\n/dev/full - Returns the error code ENOSPC (meaning ‚ÄúNo space left on device‚Äù) on writing Read man pages\n/dev/random - Special file that serves as a blocking pseudorandom number generator. It allows access to environmental noise collected from device drivers and other sources.(Block until additional environmental noise is gathered)Read man\n/dev/urandom - Without block Read man pages\n/dev/zero - Provides as many null characters as are read from it Read More\nudev - Linux dynamic device management Read man pages\nudevadm - command to query the udev database and sysfs Read More Commands/Tools lscpu - Display CPU architecture information\ncat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1 - Generates 40 characters long random string\nmtr - mtr combines the functionality of the traceroute and ping programs in a single network diagnostic tool.\nlsblk - Lists block devices\nDirectories /var/lock/ - Store lock files, which are simply files used to indicate that a certain resource (a database, a file, a device) is in use and should not be accessed by another process. Aptitude, for example, locks its database when a package manager is running.\n/var/run - Used to store .pid files, which contain the process id of a running program. This is commonly used in services or programs that need to make their process id available to other processes.\n","permalink":"https://veerendra2.github.io/linux-cheatseets/","summary":"*A blog post that I‚Äôm actively collecting ‚ÄúLinux pseudo files info, cheat sheets and tips‚Äù\nTips \u0026amp; Tricks How to force a command to return exit code 0 even if the command exited non-zero?\nHow to install dependencies of .deb automatically which failed to install previously?\nExample Solution:\n$ dpkg -i r-base-core_3.3.3-1trusty0_amd64.deb || : \\ \u0026amp;\u0026amp; apt-get --yes --force-yes -o Dpkg::Options::=\u0026#34;--force-confdef\u0026#34; -o Dpkg::Options::=\u0026#34;--force-confold\u0026#34; -f install -y \\ How to traverse directories in shell script?","title":"Linux pseudo files \u0026 cheat sheet"},{"content":"Wireshark is a really great tool for analyzing traffic, whether it could be live traffic on interface or .cap file. The tool enables different types filtering on packets like follow stream, filter by protocol and IP, etc\nIn order to install the latest version of wireshark on Linux, one should build and install from source. Sometimes, building from source is difficult because we have to hunt down the dependencies. That\u0026rsquo;s what I did for this software.\nDepending on your OS and package availability, you may need to install other dependencies. I\u0026rsquo;m using Ubuntu Mate 16 and I found the below are sufficient for me.\nInstall Dependencies $ apt-get install -y \\ qtbase5-dev qtbase5-dev-tools \\ qttools5-dev qttools5-dev-tools \\ qtmultimedia5-dev libqt5svg5-dev \\ libpcap-dev libgcrypt11-dev \\ glib2.0 libgcrypt20-dev \\ libglib2.0-dev ibglib2.0-dev Get the latest tarball from wireshark $ wget https://2.na.dl.wireshark.org/src/wireshark-2.4.5.tar.xz $ tar -xf wireshark-2.4.5.tar.xz $ cd wireshark-2.4.5 Start building $ ./configure $ sudo make install -j2 $ sudo ldconfig $ sudo wireshark ./configure checks dependencies for wireshark in your machines. That\u0026rsquo;s why while running ./configure you may get dependency missing errors. If that is the case, it will show missing dependency packages name i.e. you can google it and install it.\nmake install -j2 will take some time, you can have coffee.(Specify jobs that equals to your number of CPU cores. Ex.-j4 for quad core)\n","permalink":"https://veerendra2.github.io/wireshark-install/","summary":"Wireshark is a really great tool for analyzing traffic, whether it could be live traffic on interface or .cap file. The tool enables different types filtering on packets like follow stream, filter by protocol and IP, etc\nIn order to install the latest version of wireshark on Linux, one should build and install from source. Sometimes, building from source is difficult because we have to hunt down the dependencies. That\u0026rsquo;s what I did for this software.","title":"Build and Install Wireshark"},{"content":"Long back before I worked on Openshift which is really a great container platform tool from Redhat. But installation is not as simple as Kubernetes(relatively). One of the prerequisites for the cluster deployment is Open vSwitch.\nNow let\u0026rsquo;s see how to install Open vSwitch v2.6.1 in RedHat7 step by step\nInstall dependencies\n$ sudo yum install gcc make python-devel openssl-devel \\ kernel-devel graphviz kernel-debug-devel \\ autoconf automake rpm-build redhat-rpm-config \\ libtool Grab OpenvSwitch source from http://www.openvswitch.org/download/\n$ wget http://openvswitch.org/releases/openvswitch-2.6.1.tar.gz $ tar -xf openvswitch-2.6.1.tar.gz $ cd openvswitch-2.6.1 Create a distribution tarball\n$ ./boot.sh $ ./configure $ make dist Now you have distribution tarball(openvswitch-2.6.1.tar.gz) in current directory. Copy this file into the RPM sources directory, e.g.:\n$ cp openvswitch-2.6.1.tar.gz $HOME/rpmbuild/SOURCES Extract distribution tarball openvswitch-2.6.1.tar.gz\n$ tar -xf openvswitch-2.6.1.tar.gz $ cd openvswitch-2.6.1 $ pwd /home/ec2-user/openvswitch-2.6.1/openvswitch-2.6.1 Build Open vSwitch\n$ rpmbuild -bb --without check rhel/openvswitch.spec ... Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/openvswitch-2.6.1-1.x86_64 Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-2.6.1-1.x86_64.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-devel-2.6.1-1.x86_64.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/noarch/openvswitch-selinux-policy-2.6.1-1.noarch.rpm Wrote: /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-debuginfo-2.6.1-1.x86_64.rpm ... At the end of building, it will generate openvswitch RPM files.\nInstall the openvswitch RPM files\n$ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-2.6.1-1.x86_64.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-devel-2.6.1-1.x86_64.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/noarch/openvswitch-selinux-policy-2.6.1-1.noarch.rpm $ sudo rpm -i /home/ec2-user/rpmbuild/RPMS/x86_64/openvswitch-debuginfo-2.6.1-1.x86_64.rpm Start the openvswitch daemon\n$ sudo service openvswitch start $ sudo service openvswitch status Then you should able to run ovs-appctl --help\nSource http://www.openvswitch.org//support/dist-docs-2.5/INSTALL.RHEL.md.html\n","permalink":"https://veerendra2.github.io/openvswitch-redhat/","summary":"Long back before I worked on Openshift which is really a great container platform tool from Redhat. But installation is not as simple as Kubernetes(relatively). One of the prerequisites for the cluster deployment is Open vSwitch.\nNow let\u0026rsquo;s see how to install Open vSwitch v2.6.1 in RedHat7 step by step\nInstall dependencies\n$ sudo yum install gcc make python-devel openssl-devel \\ kernel-devel graphviz kernel-debug-devel \\ autoconf automake rpm-build redhat-rpm-config \\ libtool Grab OpenvSwitch source from http://www.","title":"Open vSwitch installation on Redhat7 OS"},{"content":"Ok, getting metrics(CPU, Memory \u0026amp; Network) from Windows OS is completely different from Linux. In Linux, people can easily develop scripts to get system metrics by simply reading /proc pesudo files. In fact there are so many open source tools to do this in Linux, like tcollector which is my favourite.\nNow, Let\u0026rsquo;s look at this Telegraf tool and what it does. I found Telegraf tool is really simple, elegant way to collect Windows OS metrics and light weight too, unlike others which some are paid and crappy. This tools doesn\u0026rsquo;t provide any wizard installation to setup, but one has to run a command in Windows powershell to install it as Windows service. It supports multiple TSDB backend storage like Graphite, OpenTSDB, etc but I have tested only with OpenTSDB.\nAs they said in Github repo and I quote\nTelegraf is an agent written in Go for collecting, processing, aggregating, and writing metrics.\nDesign goals are to have a minimal memory footprint with a plugin system so that developers in the community can easily add support for collecting metrics from local or remote services.\nGoto influxdata download portal and download Telegraf zip file\nCreate a folder and name it as Telegraf in C:\\Program Files and extract the .zip content to Telegraf folder (C:\\Program Files\\Telegraf)\nDownload telegraf configuration from here (telegraf.conf) and place it in C:\\Program Files\\Telegraf\nSpecify OpenTSDB server IP in outputs.opentsdb section in the configuration Open \u0026ldquo;Windows PowerShell\u0026rdquo; with administrator rights(Run as administrator) and paste below command to create \u0026ldquo;windows service\u0026rdquo;\nC:\\\u0026#34;Program Files\u0026#34;\\Telegraf\\telegraf.exe --config C:\\\u0026#34;Program Files\u0026#34;\\Telegraf\\telegraf.conf ‚Äì-service install In Windows Services, you should see Telegraf service. Right-click on the Telegraf service, open \u0026quot;Properties\u0026quot;-\u0026gt; Select \u0026quot;Automatic\u0026quot; for \u0026ldquo;Startup Type\u0026rdquo; and click \u0026ldquo;Start\u0026rdquo; button to start the Telegraf service.\nYou should able to see these metrics in your OpenTSDB\nDocs - https://docs.influxdata.com/telegraf/v1.5/introduction/getting_started/ ","permalink":"https://veerendra2.github.io/windows-metrics-collection/","summary":"Ok, getting metrics(CPU, Memory \u0026amp; Network) from Windows OS is completely different from Linux. In Linux, people can easily develop scripts to get system metrics by simply reading /proc pesudo files. In fact there are so many open source tools to do this in Linux, like tcollector which is my favourite.\nNow, Let\u0026rsquo;s look at this Telegraf tool and what it does. I found Telegraf tool is really simple, elegant way to collect Windows OS metrics and light weight too, unlike others which some are paid and crappy.","title":"Windows OS metrics collection with Telegraf"},{"content":"1. Install Packages Check system is capable of running KVM by running kvm-ok\n$ apt-get install qemu-kvm qemu-system libvirt-bin bridge-utils virt-manager -y Create KVM/Qemu Hard Disk File $ qemu-img create -f raw \u0026lt;name\u0026gt;.img \u0026lt;Size\u0026gt; ## Example $ qemu-img create -f raw ubuntu14-HD.img 10G Then copy the HD file to /var/lib/libvirt/images/ Launch VM with virt-install virt-install --name spinnaker \\ --ram 11096 \\ --vcpus=4 \\ --os-type linux \\ --os-variant=ubuntutrusty \\ --accelerate \\ --nographics -v \\ --disk path=/var/lib/libvirt/images/ubuntu14-HD.img,size=8 \\ --extra-args \u0026#34;console=ttyS0\u0026#34; \\ --location /opt/ubuntu14.iso --force \\ --network bridge:virbr0 Explanation\nCreate bridge virbr0 if it is necessary To know what are --os-variant available, run virt-install --os-variant list Specify --location and --disk locations Specify --ram (By default in MBs) Other Options --boot hd Boot from HD file --force Force to use existing HD that is used by another VM --debug verbose --description Description of VM Connect to console virsh list --all - : List VMs virsh console \u0026lt;name\u0026gt; - : Connect to tty of the VM Note down the IP of the VM once you connect to tty. we can ssh\nNOTE: If console/tty is already being used or active, you can reconnect to that tty by using --extra-args='console=ttyS0' option\nExport VM as .qcow2 $ qemu-img convert -f raw -O qcow2 \u0026lt;source .img file\u0026gt; \u0026lt;destination .qcow2 file\u0026gt; ## Example $ qemu-img convert -f raw -O qcow2 /var/lib/libvirt/images/ubuntu14-HD.img /home/opsmx/spinnaker.qcow2 Commands CheatSheet Command Description virsh list --all Shows all VMs virsh console \u0026lt;VM name\u0026gt; Connect to tty of the VM (If tty is enables) virsh shutdown \u0026lt;VM name\u0026gt; Shutdown the VM vish destroy \u0026lt;VM name\u0026gt; Destroys VM (Won\u0026rsquo;t deletes the VM/ Similar to shutdown) vish undefine \u0026lt;VM name\u0026gt; Deletes the VM (Run after destroy) virsh net-list List the available networks virsh net-edit \u0026lt;net name\u0026gt; Edit the network virt-install --os-variant list Lists OS variants Resource Links https://www.wavether.com/2016/11/import-qcow2-images-into-aws https://docs.openstack.org/image-guide/convert-images.html https://serverfault.com/questions/604862/any-way-to-convert-qcow2-to-ovf https://docs.openstack.org/image-guide/convert-images.html ","permalink":"https://veerendra2.github.io/kvm-hyperviour-cheatsheets/","summary":"1. Install Packages Check system is capable of running KVM by running kvm-ok\n$ apt-get install qemu-kvm qemu-system libvirt-bin bridge-utils virt-manager -y Create KVM/Qemu Hard Disk File $ qemu-img create -f raw \u0026lt;name\u0026gt;.img \u0026lt;Size\u0026gt; ## Example $ qemu-img create -f raw ubuntu14-HD.img 10G Then copy the HD file to /var/lib/libvirt/images/ Launch VM with virt-install virt-install --name spinnaker \\ --ram 11096 \\ --vcpus=4 \\ --os-type linux \\ --os-variant=ubuntutrusty \\ --accelerate \\ --nographics -v \\ --disk path=/var/lib/libvirt/images/ubuntu14-HD.","title":"KVM Hypervisor Cheat Sheets"},{"content":"We think that connecting to a website over HTTPS is secure, which is true(not true sometimes!), but what about DNS queries that you(browser) send?\nSure if we use HTTPS, all your (POST or GET) data is encrypted end-to-end which prevents eavesdropping, MITM attack and have Confidentiality, but again what about DNS queries?\nI got this question back in a while, so after a quick Internet search, I found DNSCrypt protocol which is really cool that I can encrypt DNS queries.\nFirst of all what the heck is DNS? in simple, DNS or Domain Name System is a service that resolves/translates domain \u0026ldquo;name\u0026rdquo; to \u0026ldquo;IP\u0026rdquo; or vice versa. So once you hit google.com in your browser, a DNS query fired to DNS host(for example 8.8.8.8) like asking \u0026ldquo;what is the IP of google.com\u0026rdquo; and gets DNS responses which contains IP of google.com. Now we got the IP of google.com, the browser initiates connection and establishes HTTPS.\nSo, you see these DNS queries are not part of \u0026ldquo;HTTPS\u0026rdquo;. So let\u0026rsquo;s encrypt DNS queries with DNCrypt.\nWhy should we care about \u0026ldquo;DNS queries encryption\u0026rdquo;? Well, sometimes the eavesdroppers are interested in the metadata of communication rather than actual communication.\nWhat is DNSCrypt? DNSCrypt is a protocol that authenticates communications between a DNS client and a DNS resolver. It prevents DNS spoofing. It uses cryptographic signatures to verify that responses originate from the chosen DNS resolver and haven\u0026rsquo;t been tampered with.\nIt is an open specification, with free and open source reference implementations, and it is not affiliated with any company nor organization.\nThere are some points to be noted\nIn order to use this protocol, we should install a package called dnscrypt-proxy Normal name servers(like 8.8.8.8) won\u0026rsquo;t support this protocol. We should use these DNS resolvers dnscrypt-proxy by default binds on loopback interface (127.0.0.1) 53 port. So, I have to change the title configuration. Install dnscrypt-proxy From Ubuntu 16/ Linux Mint 18.x, dnscrypt-proxy is available in the official repo.\nsudo apt-get install dnscrypt-proxy I found a PPA for Ubuntu 14.04 and Linux Mint 17.x\nsudo add-apt-repository ppa:anton+/dnscrypt sudo apt-get update sudo apt-get install dnscrypt-proxy Start dnscrypt-proxy After installation, with --help argument get options and run accordingly. But luckily I created a python script which will do it for you.\nwget -qO dnscrypt.py https://goo.gl/zjZYVR sudo python dnscrypt.py After you run the script, it will list the DNS resolver details like below.(The script downloads resolvers csv and passes this file as argument to `dnscrypt-proxy``)\nSelect one name server. You can see these name servers have options DNSSec \u0026amp; No Logging which provider can logs your queries, choose one accordingly (These options/table header you can\u0026rsquo;t see in above screenshot. You have to scroll up)\nNext, configure your network settings like below\nRestart network (disconnect and connect wifi) and your done!\nTo verify run tcpdump -i any -n port 2053 (Why 2053 port? because in above screenshot I selected 66 option which has 178.216.201.222:2053)\nWhat\u0026rsquo;s happening? Go beyond this script! I created an init script which runs at system boot. So that there is no need to run above script again and again.\nDownload resolvers csv file with \u0026ndash;\u0026gt; python dsncrypt.py -d Specify resolver_name(By default it has soltysiak which has No Logging policy and DNSSec) in the script. sudo wget -O /etc/init.d/encryptdns https://goo.gl/opZ78J sudo chmod +x /etc/init.d/encryptdns sudo update-rc.d encryptdns defaults sudo service encryptdns start Github Repository Link\nhttps://github.com/veerendra2/useless-scripts\nDNSCrypt in Windows Simple DNSCrypt Other resources you can try\nhttps://github.com/jedisct1/dnscrypt-proxy ","permalink":"https://veerendra2.github.io/dns-encrypt/","summary":"We think that connecting to a website over HTTPS is secure, which is true(not true sometimes!), but what about DNS queries that you(browser) send?\nSure if we use HTTPS, all your (POST or GET) data is encrypted end-to-end which prevents eavesdropping, MITM attack and have Confidentiality, but again what about DNS queries?\nI got this question back in a while, so after a quick Internet search, I found DNSCrypt protocol which is really cool that I can encrypt DNS queries.","title":"Encrypt your DNS queries, stay anonymous"},{"content":" A Wi-Fi deauthentication attack is a type of denial-of-service attack that targets communication between a user and a Wi-Fi wireless access point.\n-Wikipedia\nAs you can see, this type of attack is pretty powerful and difficult to detect who is attacking. There are some tools(like ‚Äúaircrack-ng‚Äù) for this attack(You can check the commands here).\nSo, basically the concept is the attacker broadcasts a wifi management ‚ÄúDeauthentication‚Äù frame to the victim\u0026rsquo;s devices/PC to tell them to deauthenticate. It is like, ‚ÄúHey client! Can you please deauthenticate‚Äù. Once deauthenticated, then the client will reconnect to AP (Access Point). These types of frames are supposed to send by valid ‚ÄúAP‚Äù to its clients, but the attacker can mimic these frames and broadcast in the network.\nInterestingly, the victim‚Äôs device/PC could not differentiate between the attacker and valid AP. Here, the attacker creates a ‚ÄúDeauthentication‚Äù packet/frame with the source MAC address of valid AP‚Äôs MAC address. So, every device thinks, the management frame came from valid AP.\nThe attacker not just sends the frame once, but sends continuously. Things get pretty bad, now the clients are continuously trying to reconnect. In this way, the clients never connect to its valid AP until the attacker stops sending the ‚Äúdeauth‚Äù frames.\nSo, how to avoid this attack? Simple, use 802.1w supported routers. Know more about 802.1w and read cisco document here.\nCheck if your wifi network is vulnerable to this attack or not\u0026hellip; I have created a Python script which sends deauth packets using the scapy python module. You can use this script to check if your wifi network is vulnerable or not. Just run the script, select the wifi network that you want to test and if you see a network outage, your wifi is vulnerable!\nDependencies wireless Install aircrack-ng and scapy\n$ sudo apt-get install aircrack-ng -y $ sudo apt-get install python-scapy -y Download and run the script $ wget -O deauth.py https://raw.githubusercontent.com/veerendra2/wifi-deauth-attack/master/deauth.py $ python deauth.py When you run the command, you should see it like below.\nWhen you start the script, it will create a ‚Äúmon0‚Äù interface(A monitoring virtual interface used to send our deauth frames) and observe wifi signals. After a few seconds, it will display near APs and its MAC addresses. Choose one to broadcast the ‚Äúdeauth‚Äù frames to that network which results network outage for connected clients to that AP.\nNOTE: Inorder to work a deauthentication attack successfully, you should be near the target network. The deauth packets should reach the connected devices of the target network\nUse my docker image to kick the environment quickly. Github Repository Link - https://github.com/veerendra2/wifi-deauth-attack ","permalink":"https://veerendra2.github.io/wifi-deathentication-attack/","summary":"A Wi-Fi deauthentication attack is a type of denial-of-service attack that targets communication between a user and a Wi-Fi wireless access point.\n-Wikipedia\nAs you can see, this type of attack is pretty powerful and difficult to detect who is attacking. There are some tools(like ‚Äúaircrack-ng‚Äù) for this attack(You can check the commands here).\nSo, basically the concept is the attacker broadcasts a wifi management ‚ÄúDeauthentication‚Äù frame to the victim\u0026rsquo;s devices/PC to tell them to deauthenticate.","title":"Wifi Deauthentication Attack"},{"content":" GNU Screen is a terminal multiplexer, a software application that can be used to multiplex several virtual consoles, allowing a user to access multiple separate login sessions inside a single terminal window, or detach and reattach sessions from a terminal. It is useful for dealing with multiple programs from a command line interface, and for separating programs from the session of the Unix shell that started the program, particularly so a remote process continues running even when the user is disconnected. more\n-Wikipedia\nInstall screen $ sudo apt-get install screen Keys/Commands Description screen Enables Screen Ctrl+a and then c Create new screen Ctrl+a and then n Go to next screen Ctrl+a and then p Go to previous screen Ctrl+a and then Shift+s Split screen horizontally Ctrl+a and then Shift+\\ Split screen vertically Ctrl+a and then Tab Traverse between splited screens Ctrl+a and then Shift+x Unsplit screens Ctrl+a and then Esc (Hit Esc, once you are done) Scroll screen Ctrl+a and then d Detach screens screen -r \u0026lt;PID\u0026gt; Reattach screen screen -ls List screens screen -S \u0026lt;session\u0026gt; Attach screen screen -XS \u0026lt;session\u0026gt; quit Kills screen Few more in my Github Gist ","permalink":"https://veerendra2.github.io/gnu-screen-commands/","summary":"GNU Screen is a terminal multiplexer, a software application that can be used to multiplex several virtual consoles, allowing a user to access multiple separate login sessions inside a single terminal window, or detach and reattach sessions from a terminal. It is useful for dealing with multiple programs from a command line interface, and for separating programs from the session of the Unix shell that started the program, particularly so a remote process continues running even when the user is disconnected.","title":"GNU screen commands(Cheat Sheet)"},{"content":"üëâ Update on 27-08-2022 Moving to Hugo and other updates! I was very excited to try Jekyll and Github Pages when I heard about it. When I try to install jekyll, I got below error\nroot@veeru:/home/veeru# gem install jekyll bundler Fetching: public_suffix-3.0.1.gem (100%) ERROR: Error installing jekyll: public_suffix requires Ruby version \u0026gt;= 2.1. Fetching: bundler-1.16.1.gem (100%) Successfully installed bundler-1.16.1 1 gem installed Installing ri documentation for bundler-1.16.1... Installing RDoc documentation for bundler-1.16.1... I don\u0026rsquo;t even know what that means(I\u0026rsquo;m not a Ruby guy, so..). Clearly jekyll needs more than Ruby version 2.1, but in Ubuntu 14.04 if you type apt-get install ruby -y you will end up having Ruby 1.9. So let\u0026rsquo;s install Ruby 2.4 like below\nsudo apt-add-repository ppa:brightbox/ruby-ng sudo apt-get update sudo apt-get install ruby2.4 ruby2.4-dev make g++ -y Then install jekyll\n$ gem install jekyll bundler What\u0026rsquo;s next? I found a blog which exactly I was looking for to deploy a website on Github Pages. As Drew Silcock said in the blog, it is better to maintain website source code and compiled websites on the same repository. Just head over to his blog and check the stuff\n","permalink":"https://veerendra2.github.io/jeklly-website/","summary":"üëâ Update on 27-08-2022 Moving to Hugo and other updates! I was very excited to try Jekyll and Github Pages when I heard about it. When I try to install jekyll, I got below error\nroot@veeru:/home/veeru# gem install jekyll bundler Fetching: public_suffix-3.0.1.gem (100%) ERROR: Error installing jekyll: public_suffix requires Ruby version \u0026gt;= 2.1. Fetching: bundler-1.16.1.gem (100%) Successfully installed bundler-1.16.1 1 gem installed Installing ri documentation for bundler-1.16.1... Installing RDoc documentation for bundler-1.","title":"Install jekyll in Ubuntu 14.04"},{"content":"‚ÄúMAC Address Scrambling‚Äú- By name itself we can understand, instead of using a burned-in address, the machine uses a random MAC address. The machine/device changes MAC addresses regularly to improve security. MAC address is a 48 bit hexadecimal digit which is burned in every electronic device that has capability of ‚Äúconnectivity‚Äù such as mobile devices, smart TV, PC, etc. ‚ÄúApple‚Äù added this feature to iPhones from iOS8 to protect user‚Äôs privacy.\nSo, how does a static MAC address cause some security issues? First thing caught in my mind is this\nAccording to Edward Snowden, the National Security Agency has a system that tracks the movements of everyone in a city by monitoring the MAC addresses of their electronic devices. As a result of users being trackable by their devices‚Äô MAC addresses, Apple has started using random MAC addresses in their iOS line of devices while scanning for networks.If random MAC addresses are not used, researchers have confirmed that it is possible to link a real identity to a particular wireless MAC address.\n-wikipedia (https://en.wikipedia.org/wiki/MAC_address)\nAs I said it is ‚ÄúBurned-in‚Äù, means it never changes which network you connect unlike IP address. Another possible attack is ‚ÄúMan-in-Middle‚Äù with ARP poisoning. I highly recommend you to read wikipedia article: ARP spoofing for better understanding of ARP poisoning. IEEE group also recommends random MAC address for Wifi security. Read this article for more info\nFor Linux, soon will get this feature. But now, I made a script(init script: I know init scripts are not meant for this, but I made it anyway!) which changes MAC address on every time machine boots. Not only on boot, we can change whenever we want with simple commands and can restore to the original or we can go one step further with cron job to schedule the script that changes MAC address for every 1 hour or 30 minutes (Depends on your need).\nIt is a shell script that uses macchanger, which executes every time machine boots thus the interface gets random MAC address every time.\nNOTE: The \u0026ldquo;macchanger\u0026rdquo; or any other script never changes the device‚Äôs actual MAC address which is burned on the interface, but macchanger create a proxy which machines uses this proxy MAC address for network communication\nHow to install? Install macchanger\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install macchanger -y Download and place changer script in /etc/init.d/\n$ wget -q -O /etc/init.d/changer https://goo.gl/tRfoJo Give executable permission\n$ sudo chmod +x /etc/init.d/changer Run update-rc.d\n$ sudo update-rc.d changer defaults Commands $ service changer restore # To restores original MAC MAC Address Restored 0X:XX:XX:27:d8:XX $ sudo service changer new # To assign a new MAC. Note that, interface will go down and up MAC Address Changed Successfully $ service changer show # To shows current MAC Current MAC: 08:00:0c:27:d8:39 NOTE:\nChange the interface in changer after you download, by default the interface is wlan0 There will be network restart when you run service changer new or service changer restore Kali Linux‚Äôs latest version(kali-rolling) has this feature. While upgrading(apt-get install upgrade), there is a macchanger prompt asking to enable this feature. ","permalink":"https://veerendra2.github.io/mac-scrambling/","summary":"‚ÄúMAC Address Scrambling‚Äú- By name itself we can understand, instead of using a burned-in address, the machine uses a random MAC address. The machine/device changes MAC addresses regularly to improve security. MAC address is a 48 bit hexadecimal digit which is burned in every electronic device that has capability of ‚Äúconnectivity‚Äù such as mobile devices, smart TV, PC, etc. ‚ÄúApple‚Äù added this feature to iPhones from iOS8 to protect user‚Äôs privacy.","title":"MAC Address Scrambling in Linux"},{"content":"","permalink":"https://veerendra2.github.io/about/","summary":"","title":"About"},{"content":" Network Blogs Capturing Wireless LAN Packets on Ubuntu with tcpdump and Kismet Linux Bridging Phishing With a Rogue Wi-Fi Access Point Fast DDoS analyzer with sflow/netflow/mirror support China\u0026rsquo;s Man-on-the-Side Attack on GitHub SSH testing tool checks the configuration of given server accessible over internet Infinite possibilities with the Scapy Module An Illustrated Guide to the Kaminsky DNS Vulnerability A penetration tester‚Äôs guide to sub-domain enumeration A source for pcap files and malware samples Explanation of how https works Daniels Networking Blog Python for Network Engineers Fuzzing proprietary protocols with Scapy, radamsa and a handful of PCAPs How to Decrypt 802.11 Tutorials for Network Simulator \u0026ldquo;ns\u0026rdquo; The First Few Milliseconds of an HTTPS Connection HTTPS explained with carrier pigeons How To Run Your Own Mail Server Bettercap - MITM attack tool Tcpdump Examples Bluetooth BLEAH-A BLE scanner for \u0026ldquo;smart\u0026rdquo; devices hacking Proxying Bluetooth devices for security analysis using btproxy Nike+ FuelBand SE BLE Protocol Reversed Ubertooth - Can sniff some data from Basic Rate (BR) Bluetooth Classic connections. A Bluetooth LE interface for Python Github Projects Wireless MITM WPA attacks Framework for Rogue Wi-Fi Access Point Attack FruityWiFi is a wireless network auditing tool. Tool for sniffing unencrypted wireless probe requests from devices [Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing](Rogue Access Point framework for Wi-Fi automatic association attacks and victim-customized phishing) This script creates a NATed or Bridged WiFi Access Point Kick devices off your network by performing an ARP Spoof attack A framework for wireless pentesting. Crack WPA/WPA2 Wi-Fi Routers with Airodump-ng and Aircrack-ng/Hashcat Bluetooth Proxy tool DNS DNS over HTTPS Analyze the security of any domain by finding all the information possible. Made in python Domain name permutation engine for detecting typo squatting, phishing and corporate espionage Selective DNS proxy forwarding based on DNS threat blocking providers intelligence DNS Enumeration Script Fast subdomains enumeration tool for penetration testers Open redirect subdomains scanner Analyze the security of any domain by finding all the information possible. Made in python A set of tools for performing reconnaissance on domain names Scan Scrapy, a fast high-level web crawling \u0026amp; scraping framework for Python TCP port scanner, spews SYN packets asynchronously, scanning entire Internet in under 5 minutes. Nikto web server scanner Scan .onion hidden services with nmap using Tor, proxychains and dnsmasq A small TOR Onion Address harvester for checking if the address is available or not. Tool for capturing and replaying live HTTP traffic into a test environment netdiscover Layer 2 network neighbourhood discovery tool that uses scapy SSL/TLS layers for scapy the interactive packet manipulation tool The python-based interactive packet manipulation program \u0026amp; library Python wrapper for tshark, allowing python packet parsing using wireshark dissectors Stores your data in ICMP ping packets TCP/IP packet demultiplexer Python binding of libnetfilter_conntrack Nipe is a script to make Tor Network your default gateway A reverse TCP tunnel let you access target behind NAT or firewall A Tunnel which Turns UDP Traffic into Encrypted UDP/FakeTCP/ICMP Traffic by using Raw Socket Search MAC Address Linux Blogs Boot Run Levels \u0026amp; How to make init scripts Realmode Assembly ‚Äì Writing bootable stuff Making scripts run at boot time with Debian Writing a Bootloader init script template systemd, Beyond init-Youtube Talk Analyzing the Linux boot process [Write Simple OS from scratch [PDF]]({{ site.url }}/assets/os-dev.pdf) OS Development The little book about OS development How to Make a Computer Operating System Let\u0026rsquo;s Write a Kernel Information about the creation of operating systems Writing OS in Rust Commands htop explained visually with screenshot Explain Shell Learn VIM 30 interesting commands for the Linux shell An Introduction to Linux Permissions 3 Ways to Permanently and Securely Delete Two great uses for the cp command: Bash shortcuts Files Understanding and generating the hash stored in /etc/shadow What is setiud, setgid and sticky bit in Linux? /proc/sys/net/ipv4 /proc/net/tcp and /proc/net/tcp6 /dev/null /dev/full /dev/random /dev/urandom /dev/zero Warden.NET is an easy to use process management library for keeping track of processes on Windows. [Android Internals[PDF]]({{ \u0026ldquo;/assets/android_internals.pdf \u0026quot; | absolute_url }}) A list of reading materials for BPF How Linux CPU Usage Time and Percentage is calculated Removing Your PDF Metadata \u0026amp; Protecting PDF Files Linux Process Hunter Linux Memory Managment Frequently Asked Questions Attack Infrastructure Logging Virtualization Internals Github Projects A toolkit for creating efficient kernel tracing and manipulation programs Tool for in-depth analysis of USB HID devices communication From finding text to search and replace, from sorting to beautifying text and more Programmable completion functions for bash CheatSheets Bash Python Blog How to recover lost Python source code if it\u0026rsquo;s still resident in-memory Cpython Internals: Codewalk through the Python interpreter source codes [Youtube Playlist] Problem Solving with Algorithms and Data Structures using Python Natural Language Processing with Python Python Anti-Patterns Python Plays: Grand Theft Auto V https://pythonprogramming.net/ Pythonic Data Structures and Algorithms An automation tool that models a user‚Äôs actions on a terminal. Regx in easy way You Should Learn Regx Let‚Äôs Build A Simple Interpreter Python Excel Tutorial: The Definitive Guide C++ Data Structures [Scapy Docs [PDF]]({{ \u0026ldquo;/assets/scapydoc.pdf\u0026rdquo; | absolute_url }}) python-course.eu Github Projects Android Package Inspector - dynamic analysis with api hooks, start unexported activities and more. (Xposed Module) A public list of APIs from round the web. A Python toolbox for building complex digital hardware A collection of (mostly) technical things every software developer should know What happens when\u0026hellip; Command and Rule over your Shell Shutit-Automation framework for programmers A general-purpose fuzzer Minimal examples of data structures and algorithms in Python Python By Examples Security/Privacy Blogs Privacy What Is Intelligent Tracking Prevention and How Does It Work? The Truth About Online Privacy: How Your Data is Collected, Shared, and Sold What is Cookie Syncing and How Does it Work? Webpage tracking only using CSS (and no JS) How to Monitor Mobile App Traffic With Sniffers Python for PenTesters Intro to basic Disassembly \u0026amp; Reverse Engineering Python for Pentesters-pentesteracademy Start Your Own ISP Details of the implementation of Spectre, Attacking secure USB keys, behind the scene How to Install Tripwire IDS (Intrusion Detection System) on Linux Hacker101! Four Ways to Bypass Android SSL Verification and Certificate Pinning Tracing API calls in Burp with Frida Open Source CyberSecurity - n0where.net [Command Injection [PDF]]({{ \u0026ldquo;/assets/Command_Injection_Shell_Injection.pdf\u0026rdquo; | absolute_url }}) [Recon-ng Guid [PDF]]({{ \u0026ldquo;/assets/recon-ng-guide.pdf\u0026rdquo; | absolute_url }}) Android Applications Reversing 101 moveax.me The New zANTI: Mobile Penetration \u0026amp; Security Analysis Toolkit Reverse Engineering Reverse Engineering Basics A Primer Guide to Reverse Engineering Binary patching and intro to assembly with r2 Github Projects WhatsApp Discover Mobile App Pentest cheat sheet ","permalink":"https://veerendra2.github.io/bookmarks/","summary":"Network Blogs Capturing Wireless LAN Packets on Ubuntu with tcpdump and Kismet Linux Bridging Phishing With a Rogue Wi-Fi Access Point Fast DDoS analyzer with sflow/netflow/mirror support China\u0026rsquo;s Man-on-the-Side Attack on GitHub SSH testing tool checks the configuration of given server accessible over internet Infinite possibilities with the Scapy Module An Illustrated Guide to the Kaminsky DNS Vulnerability A penetration tester‚Äôs guide to sub-domain enumeration A source for pcap files and malware samples Explanation of how https works Daniels Networking Blog Python for Network Engineers Fuzzing proprietary protocols with Scapy, radamsa and a handful of PCAPs How to Decrypt 802.","title":"My Bookmarks"}]