<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes-The Hard Way With Docker & Flannel (Part 2) – Veerendra's Blog – #docker #kubernetes #linux #python #networking</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Welcome back to “Kubernetes-The Hard Way With Docker &amp; Flannel” series part 2. In previous post we have provised compute resource, generated certificates and kubeconfig files. In this post, we will install/configure controller nodes

" />
    <meta property="og:description" content="Welcome back to “Kubernetes-The Hard Way With Docker &amp; Flannel” series part 2. In previous post we have provised compute resource, generated certificates and kubeconfig files. In this post, we will install/configure controller nodes

" />
    
    <meta name="author" content="Veerendra's Blog" />

    
    <meta property="og:title" content="Kubernetes-The Hard Way With Docker & Flannel (Part 2)" />
    <meta property="twitter:title" content="Kubernetes-The Hard Way With Docker & Flannel (Part 2)" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Veerendra's Blog - #docker #kubernetes #linux #python #networking" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://avatars0.githubusercontent.com/u/8393701?s=460&v=4" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Veerendra's Blog</a></h1>
            <p class="site-description">#docker #kubernetes #linux #python #networking</p>
          </div>

          <nav>
            <a href="/bookmarks">My Bookmarks</a>
            <a href="/archive">Archives</a>
            <a href="/tags">Tags</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Kubernetes-The Hard Way With Docker & Flannel (Part 2)</h1>

  <div class="entry">
    <p>Welcome back to “Kubernetes-The Hard Way With Docker &amp; Flannel” series part 2. In <a href="/kubernetes-the-hard-way-1/">previous post</a> we have provised compute resource, generated certificates and kubeconfig files. In this post, we will install/configure controller nodes</p>

<h1 id="6-bootstrapping-the-etcd-cluster">6. Bootstrapping the etcd Cluster</h1>
<p><a href="https://coreos.com/etcd/"><code class="highlighter-rouge">etcd</code></a> is a consistent and highly-available key value storage DB. Kubernetes stores all cluster data in <code class="highlighter-rouge">etcd</code> via api-server. In this section we will install and configure <code class="highlighter-rouge">etcd</code> on all controller nodes.</p>

<p><strong><em>*NOTE: The below commands must run on all controller nodes</em></strong></p>

<p><em>*TIP: You can use <a href="https://github.com/tmux/tmux/wiki">tumx</a> to run command on multiple nodes at same time</em></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>
<span class="nv">$ </span>wget <span class="nt">-q</span> <span class="nt">--show-progress</span> <span class="nt">--https-only</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  <span class="s2">"https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"</span>
<span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-xvf</span> etcd-v3.3.9-linux-amd64.tar.gz
<span class="nv">$ </span><span class="nb">sudo mv </span>etcd-v3.3.9-linux-amd64/etcd<span class="k">*</span> /usr/local/bin/

<span class="nv">$ </span><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/etcd /var/lib/etcd
<span class="nv">$ </span><span class="nb">sudo cp </span>ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/</code></pre></figure>

<p>Set up the following environment variables which are usefull generate etcd systemd unit file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ ETCD_NAME</span><span class="o">=</span><span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span>
<span class="nv">$ INTERNAL_IP</span><span class="o">=</span><span class="sb">`</span><span class="nb">hostname</span> <span class="nt">-i</span><span class="sb">`</span> <span class="c"># IP of the current node</span>
<span class="c">#INITIAL_CLUSTER=&lt;controller 1 hostname&gt;=https://&lt;controller 1 private ip&gt;:2380,&lt;controller 2 hostname&gt;=https://&lt;controller 2 private ip&gt;:2380</span>
<span class="nv">$ INITIAL_CLUSTER</span><span class="o">=</span><span class="nv">m1</span><span class="o">=</span>https://10.200.1.10:2380,m2<span class="o">=</span>https://10.200.1.11:2380</code></pre></figure>

<p>Create systemd unit file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \\
  --name </span><span class="k">${</span><span class="nv">ETCD_NAME</span><span class="k">}</span><span class="sh"> \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 \\
  --listen-peer-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2380 \\
  --listen-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh">:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster </span><span class="k">${</span><span class="nv">INITIAL_CLUSTER</span><span class="k">}</span><span class="sh"> \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</span></code></pre></figure>

<p>Start the etcd service</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="o">{</span>
  <span class="nb">sudo </span>systemctl daemon-reload
  <span class="nb">sudo </span>systemctl <span class="nb">enable </span>etcd
  <span class="nb">sudo </span>systemctl start etcd
<span class="o">}</span></code></pre></figure>

<p>Once <code class="highlighter-rouge">etcd</code> installation and configuration done in all controller nodes, verify that etcd cluster is working properly</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nb">sudo </span><span class="nv">ETCDCTL_API</span><span class="o">=</span>3 etcdctl member list <span class="se">\</span>
  <span class="nt">--endpoints</span><span class="o">=</span>https://127.0.0.1:2379 <span class="se">\</span>
  <span class="nt">--cacert</span><span class="o">=</span>/etc/etcd/ca.pem <span class="se">\</span>
  <span class="nt">--cert</span><span class="o">=</span>/etc/etcd/kubernetes.pem <span class="se">\</span>
  <span class="nt">--key</span><span class="o">=</span>/etc/etcd/kubernetes-key.pem</code></pre></figure>

<p>You should see output like below</p>

<p><img src="https://veerendra2.github.io/assets/etcd_verify_output.jpg" alt="etcd verify image" class="center-image" /></p>

<h1 id="7-bootstrapping-the-kubernetes-control-plane">7. Bootstrapping the Kubernetes Control Plane</h1>
<p>The control plane binaries are</p>
<ul>
  <li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a></li>
  <li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a></li>
  <li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a></li>
</ul>

<p>Download control plane binaries</p>

<p><strong><em>*NOTE: The below commands must run on all controller nodes</em></strong></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/kubernetes/config
<span class="nv">$ KUBERNETES_VERSION</span><span class="o">=</span>v1.10.13
<span class="nv">$ </span>wget <span class="nt">-q</span> <span class="nt">--show-progress</span> <span class="nt">--https-only</span> <span class="nt">--timestamping</span> <span class="se">\</span>
  <span class="s2">"https://dl.k8s.io/</span><span class="k">${</span><span class="nv">KUBERNETES_VERSION</span><span class="k">}</span><span class="s2">/bin/linux/amd64/kube-apiserver"</span> <span class="se">\</span>
  <span class="s2">"https://dl.k8s.io/</span><span class="k">${</span><span class="nv">KUBERNETES_VERSION</span><span class="k">}</span><span class="s2">/bin/linux/amd64/kube-controller-manager"</span> <span class="se">\</span>
  <span class="s2">"https://dl.k8s.io/</span><span class="k">${</span><span class="nv">KUBERNETES_VERSION</span><span class="k">}</span><span class="s2">/bin/linux/amd64/kube-scheduler"</span> <span class="se">\</span>
  <span class="s2">"https://dl.k8s.io/</span><span class="k">${</span><span class="nv">KUBERNETES_VERSION</span><span class="k">}</span><span class="s2">/bin/linux/amd64/kubectl"</span></code></pre></figure>

<p><em>*TIP: You can get version number from <a href="https://github.com/kubernetes/kubernetes/releases">kuberntes releases page</a></em></p>

<p>Move the binaries to <code class="highlighter-rouge">/usr/local/bin/</code></p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">chmod</span> +x kube-apiserver kube-controller-manager kube-scheduler kubectl
<span class="nv">$ </span><span class="nb">sudo mv </span>kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/</code></pre></figure>

<h3 id="kubernetes-api-server-configuration">Kubernetes API Server Configuration</h3>
<p>Move certificates to kubernetes directory</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /var/lib/kubernetes/

<span class="nv">$ </span><span class="nb">sudo mv </span>ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem <span class="se">\</span>
    service-account-key.pem service-account.pem <span class="se">\</span>
    encryption-config.yaml /var/lib/kubernetes/</code></pre></figure>

<p>Create kube-api server systemd unit file.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ CONTROLLER0_IP</span><span class="o">=</span>10.200.1.10
<span class="nv">$ CONTROLLER1_IP</span><span class="o">=</span>10.200.1.11
<span class="nv">$ INTERNAL_IP</span><span class="o">=</span><span class="sb">`</span><span class="nb">hostname</span> <span class="nt">-i</span><span class="sb">`</span>  <span class="c"># Current node's IP</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=</span><span class="k">${</span><span class="nv">INTERNAL_IP</span><span class="k">}</span><span class="sh"> \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --enable-swagger-ui=true \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://</span><span class="nv">$CONTROLLER0_IP</span><span class="sh">:2379,https://</span><span class="nv">$CONTROLLER1_IP</span><span class="sh">:2379 \\
  --event-ttl=1h \\
  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2 \\
  --kubelet-preferred-address-types=InternalIP,InternalDNS,Hostname,ExternalIP,ExternalDNS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</span></code></pre></figure>

<h3 id="kubernetes-controller-manager-configuration">Kubernetes Controller Manager Configuration</h3>
<p>Move kubeconfig files to kubernetes directory</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">sudo mv </span>kube-controller-manager.kubeconfig /var/lib/kubernetes/</code></pre></figure>

<p>Create kube-controller-manager systemd unit file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</span></code></pre></figure>

<h3 id="kubernetes-scheduler-configuration">Kubernetes Scheduler Configuration</h3>
<p>Move kube-scheduler kubeconfig to kubernetes directory</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">sudo mv </span>kube-scheduler.kubeconfig /var/lib/kubernetes/</code></pre></figure>

<p>Create kube-scheduler configuration file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF</span></code></pre></figure>

<p>Create kube-scheduler systemd unit file</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF</span></code></pre></figure>

<h3 id="start-the-controller-services">Start the controller services</h3>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">sudo </span>systemctl daemon-reload
<span class="nv">$ </span><span class="nb">sudo </span>systemctl <span class="nb">enable </span>kube-apiserver kube-controller-manager kube-scheduler
<span class="nv">$ </span><span class="nb">sudo </span>systemctl start kube-apiserver kube-controller-manager kube-scheduler</code></pre></figure>

<h3 id="enable-http-health-checks">Enable HTTP Health Checks</h3>
<p>In original “Kubernetes The Hard Way”, Kelsey used GCP load balancer to load balance the requests among controllers. Since it is difficult to setup HTTPS health checks on GCP network load balancer and kube-apiserver supports only HTTPS health check. He created HTTP nginx proxy for kube-api server, GCP network load balancer perform health check via HTTP nginx proxy. But in our case, we can skip this step since we are not using GCP network load balancer</p>

<h3 id="verification">Verification</h3>
<p>Check the components status using below commands.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span>kubectl get componentstatuses <span class="nt">--kubeconfig</span> admin.kubeconfig</code></pre></figure>

<p>Run above command on all controller nodes and verify statuses which should like below</p>

<p><img src="https://veerendra2.github.io/assets/components_status.jpg" alt="components status image" class="center-image" /></p>
<h3 id="rbac-for-kubelet-authorization">RBAC for Kubelet Authorization</h3>
<p>In this section we will configure <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC permissions</a> to allow the kube-api server to access the Kubelet API on each worker node. Access to the Kubelet API is required for retrieving metrics, logs, and executing commands in pods.</p>

<p>Create the <code class="highlighter-rouge">system:kube-apiserver-to-kubelet</code> ClusterRole with permissions to access the Kubelet.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF</span></code></pre></figure>

<p>The kube-api server authenticates to the Kubelet as the “kubernetes” user using the client certificate as defined by the <code class="highlighter-rouge">--kubelet-client-certificate</code> flag which have defined in kube-apiserver systemd unit file above.</p>

<p>Bind the <code class="highlighter-rouge">system:kube-apiserver-to-kubelet</code> ClusterRole to the kubernetes user:</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On controller nodes</span>

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF</span></code></pre></figure>

<h3 id="the-kubernetes-frontend-load-balancer">The Kubernetes Frontend Load Balancer</h3>
<p>As I said earlier, we are not going using GCP load network load balancer, but we are going using nginx docker container on host(Laptop) to load balance the requests.</p>

<p>In this section, we will build nginx docker image with appropriate configuration to load balance requests among controller nodes(<code class="highlighter-rouge">m1</code> and <code class="highlighter-rouge">m2</code>)</p>

<h5 id="nginx-configuration">nginx configuration</h5>
<p>Specify controllers IPs with kube-api server’s port in nginx configuration like below</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On host</span>
<span class="nb">cd</span> ~/kubernetes-the-hard-way

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | tee kubernetes.conf 
stream {
    upstream kubernetes {
        server 10.200.1.10:6443;
        server 10.200.1.11:6443;
    }

    server {
        listen 6443;
        listen 443;
        proxy_pass kubernetes;
    }
}
EOF</span></code></pre></figure>

<h5 id="dockerfile">Dockerfile</h5>
<p>Create <code class="highlighter-rouge">Dockerfile</code> to build nginx load balancer docker image</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On host</span>
<span class="nv">$ </span><span class="nb">cd</span> ~/kubernetes-the-hard-way

<span class="nv">$ </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | tee Dockerfile
FROM nginx:latest
MAINTAINER Veerendra Kakumanu

RUN mkdir -p /etc/nginx/tcpconf.d &amp;&amp; echo "include /etc/nginx/tcpconf.d/*;" &gt;&gt; /etc/nginx/nginx.conf
COPY kubernetes.conf /etc/nginx/tcpconf.d/kubernetes.conf
EOF</span></code></pre></figure>

<p>Build and launch the container</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On host</span>
<span class="nv">$ </span><span class="nb">cd</span> ~/kubernetes-the-hard-way

<span class="nv">$ </span><span class="nb">sudo </span>docker build <span class="nt">-t</span> nginx_proxy <span class="nb">.</span>
<span class="nv">$ </span><span class="nb">sudo </span>docker run <span class="nt">-it</span> <span class="nt">-d</span> <span class="nt">-h</span> proxy <span class="nt">--net</span> br0 <span class="nt">--ip</span> 10.200.1.15 nginx-proxy</code></pre></figure>

<h3 id="verification-1">Verification</h3>
<p><code class="highlighter-rouge">curl</code> the HTTPS endpoint of load balancer(nginx docker container) which forwards the requests to controller node with certificate.</p>

<figure class="highlight"><pre><code class="language-shell" data-lang="shell"><span class="c"># On host</span>

<span class="nv">$ KUBERNETES_PUBLIC_ADDRESS</span><span class="o">=</span>10.200.1.15
<span class="nv">$ </span>curl <span class="nt">--cacert</span> ca.pem https://<span class="k">${</span><span class="nv">KUBERNETES_PUBLIC_ADDRESS</span><span class="k">}</span>:6443/version</code></pre></figure>

<p>If everything is good, you should see output like below.</p>

<p><img src="https://veerendra2.github.io/assets/curl_version.jpg" alt="curl for version image" class="center-image" /></p>

<p>In this post, we have successfully provisioned controller nodes and load balancer. We will bootstrap the worker nodes in next post</p>

<div class="PageNavigation">
  
    <a class="prev" href="/kubernetes-the-hard-way-1/">&laquo; Kubernetes-The Hard Way With Docker &amp; Flannel (Part 1)</a>
  
  
    <a class="next" href="/kubernetes-the-hard-way-3/">Kubernetes-The Hard Way With Docker &amp; Flannel (Part 3) &raquo;</a>
  
</div>

<style>
.tablelines table, .tablelines td, .tablelines th {
        border: 2px solid black;
        padding: 5px;
        }
.tablelines th{
 text-align:center;
 font-weight:bold
}

.PageNavigation {
  font-size: 16px;
  display: block;
  width: auto;
  overflow: hidden;
}

.PageNavigation a {
  display: block;
  width: 50%;
  float: left;
  margin: 1em 0;
}

.PageNavigation .next {
  text-align: left;
}

</style>


  </div>

  <div class="date">
    Written on January 17, 2019
  </div>
  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112970252-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112970252-1');
</script>



  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'veerendra';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          



<a href="https://github.com/veerendra2"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/veerendrav2"><i class="svg-icon linkedin"></i></a>


<a href="https://www.twitter.com/veerendrav2"><i class="svg-icon twitter"></i></a>
<a href="http://stackoverflow.com/users/2200798/veerendra"><i class="svg-icon stackoverflow"></i></a>


        </footer>
      </div>
    </div>

    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112970252-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112970252-1');
</script>



  </body>
</html>
